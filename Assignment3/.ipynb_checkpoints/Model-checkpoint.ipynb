{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e53d2ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType, LongType, TimestampType\n",
    "import os\n",
    "\n",
    "# Initialize the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FrontpagePrediction\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.driver.cores\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Define the data schema from the json\n",
    "schema = StructType([\n",
    "    StructField(\"aid\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    StructField(\"domain\", StringType(), True),\n",
    "    StructField(\"votes\", LongType(), True),\n",
    "    StructField(\"user\", StringType(), True),\n",
    "    StructField(\"posted_at\", StringType(), True),\n",
    "    StructField(\"comments\", LongType(), True),\n",
    "    StructField(\"source_title\", StringType(), True),\n",
    "    StructField(\"source_text\", StringType(), True),\n",
    "    StructField(\"frontpage\", BooleanType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5cfba52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples: 650, Negative samples: 3403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 20:33:43 WARN DAGScheduler: Broadcasting large task binary with size 1315.3 KiB\n",
      "24/05/25 20:33:45 WARN DAGScheduler: Broadcasting large task binary with size 1315.4 KiB\n",
      "24/05/25 20:33:57 WARN DAGScheduler: Broadcasting large task binary with size 1419.8 KiB\n",
      "24/05/25 20:34:31 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:34:48 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:34:52 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:34:57 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:35:02 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:35:08 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:35:13 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:35:17 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:35:22 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:35:28 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:35:38 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:35:42 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:35:46 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:35:51 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:35:58 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:36:07 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:36:11 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:36:16 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:36:20 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:36:28 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:36:38 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:36:42 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:36:47 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:36:54 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:37:04 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:37:15 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:37:19 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:37:24 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:37:31 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:37:38 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:37:49 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:37:53 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:37:57 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:38:02 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:38:09 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:38:20 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:38:24 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:38:28 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:38:33 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:38:40 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:38:52 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:38:56 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:39:00 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:39:05 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:39:12 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:39:19 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:39:24 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:39:28 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:39:33 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:39:40 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:39:51 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:39:55 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:39:59 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:40:04 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:40:11 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:40:19 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:40:24 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:40:29 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:40:35 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:40:44 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:40:54 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:40:59 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:41:03 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:41:08 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:41:15 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:41:24 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:41:29 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:41:34 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:41:40 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:41:49 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:42:00 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:42:04 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:42:09 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:42:14 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:42:21 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:42:30 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:42:35 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:42:39 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:42:44 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:42:51 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:43:00 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:43:04 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:43:09 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:43:16 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:43:25 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:43:33 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:43:38 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:43:43 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:43:50 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:43:59 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:44:07 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:44:12 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 20:44:17 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:44:24 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:44:35 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/25 20:44:48 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "24/05/25 20:44:53 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "24/05/25 20:44:58 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "24/05/25 20:45:05 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "24/05/25 20:45:14 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "24/05/25 20:45:25 WARN DAGScheduler: Broadcasting large task binary with size 1402.2 KiB\n",
      "24/05/25 20:45:27 WARN DAGScheduler: Broadcasting large task binary with size 1402.2 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+-----+\n",
      "|         source_text|         probability|prediction|label|\n",
      "+--------------------+--------------------+----------+-----+\n",
      "|Android 15 can te...|[0.84810569857494...|       0.0|  0.0|\n",
      "|Federal Appeals C...|[0.93147330635496...|       0.0|  0.0|\n",
      "|Federal Appeals C...|[0.93147330635496...|       0.0|  0.0|\n",
      "|New Video of Stro...|[0.94334250077501...|       0.0|  0.0|\n",
      "|New Video of Stro...|[0.94334250077501...|       0.0|  0.0|\n",
      "|Google Common Lis...|[0.94221426536922...|       0.0|  0.0|\n",
      "|Google Common Lis...|[0.94221426536922...|       0.0|  0.0|\n",
      "|Russian TOR-M2U A...|[0.94221426536922...|       0.0|  0.0|\n",
      "|Russian TOR-M2U A...|[0.94221426536922...|       0.0|  0.0|\n",
      "|The invisible sea...|[0.94334250077501...|       0.0|  0.0|\n",
      "|Inside the disinf...|[0.89022944107950...|       0.0|  0.0|\n",
      "|Amazon ebooks: Ar...|[0.94334250077501...|       0.0|  0.0|\n",
      "|Amazon ebooks: Ar...|[0.94334250077501...|       0.0|  0.0|\n",
      "|NetBSD turns 30 a...|[0.88146434129386...|       0.0|  0.0|\n",
      "|NetBSD turns 30 a...|[0.88146434129386...|       0.0|  0.0|\n",
      "|Electronics | Fre...|[0.86148977655627...|       0.0|  0.0|\n",
      "|What can a techno...|[0.94334250077501...|       0.0|  0.0|\n",
      "|What can a techno...|[0.94334250077501...|       0.0|  0.0|\n",
      "|What can a techno...|[0.94334250077501...|       0.0|  0.0|\n",
      "|What can a techno...|[0.94334250077501...|       0.0|  0.0|\n",
      "+--------------------+--------------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 20:45:29 WARN DAGScheduler: Broadcasting large task binary with size 1397.9 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC:  0.9882362779223767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 20:45:44 WARN DAGScheduler: Broadcasting large task binary with size 1403.0 KiB\n",
      "24/05/25 20:45:59 WARN DAGScheduler: Broadcasting large task binary with size 1403.0 KiB\n",
      "24/05/25 20:46:14 WARN DAGScheduler: Broadcasting large task binary with size 1403.0 KiB\n",
      "24/05/25 20:46:28 WARN DAGScheduler: Broadcasting large task binary with size 1403.0 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  1.0\n",
      "Recall:  0.9252615844544095\n",
      "F1 Score:  0.9611801242236024\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.sql.functions import length\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Read data from local\n",
    "data = spark.read.schema(schema).json(\"/Users/xiaodi/anaconda3/spark/notebooks/A data/*/part*\")\n",
    "\n",
    "# Drop the Nan value\n",
    "data = data.na.drop()\n",
    "\n",
    "# Convert frontpage from boolean to string\n",
    "data = data.withColumn(\"frontpage\", col(\"frontpage\").cast(\"string\"))\n",
    "\n",
    "# Add a new feature 'text_length'\n",
    "data = data.withColumn(\"text_length\", length(col(\"source_text\")))\n",
    "\n",
    "# Tokenize the 'source_text' column into words\n",
    "tokenizer = Tokenizer(inputCol=\"source_text\", outputCol=\"words\")\n",
    "\n",
    "# Remove stopwords from the tokenized words\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "\n",
    "# Compute term frequencies using HashingTF\n",
    "hashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"tf_features\", numFeatures=10000)\n",
    "\n",
    "# Compute inverse document frequencies to get TF-IDF features\n",
    "idf = IDF(inputCol=\"tf_features\", outputCol=\"tfidf_features\")\n",
    "\n",
    "# Convert 'domain' into numerical feature\n",
    "domain_indexer = StringIndexer(inputCol=\"domain\", outputCol=\"domain_index\", handleInvalid=\"keep\")\n",
    "\n",
    "# One-hot encode 'domain' column\n",
    "domain_encoder = OneHotEncoder(inputCol=\"domain_index\", outputCol=\"domain_vec\")\n",
    "\n",
    "# Convert 'frontpage' into numerical labels\n",
    "label_indexer = StringIndexer(inputCol=\"frontpage\", outputCol=\"label\", handleInvalid=\"keep\")\n",
    "\n",
    "# Assemble all the feature columns into a single feature vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"tfidf_features\", \"text_length\", \"votes\", \"comments\", \"domain_vec\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Create a pipeline for future data preprocessing\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwords_remover, hashing_tf, idf, domain_indexer, domain_encoder, assembler, label_indexer])\n",
    "\n",
    "# Check the ratio of positive and negative samples\n",
    "positive_samples = data.filter(col(\"frontpage\") == \"true\").count()\n",
    "negative_samples = data.filter(col(\"frontpage\") == \"false\").count()\n",
    "\n",
    "print(f\"Positive samples: {positive_samples}, Negative samples: {negative_samples}\")\n",
    "\n",
    "# If the samples are imbalanced, perform upsampling or downsampling\n",
    "if positive_samples < negative_samples:\n",
    "    # Upsample positive samples\n",
    "    ratio = negative_samples / positive_samples\n",
    "    sampled_positive_data = data.filter(col(\"frontpage\") == \"true\").sample(withReplacement=True, fraction=ratio)\n",
    "    balanced_data = sampled_positive_data.union(data.filter(col(\"frontpage\") == \"false\"))\n",
    "else:\n",
    "    # Downsample negative samples\n",
    "    ratio = positive_samples / negative_samples\n",
    "    sampled_negative_data = data.filter(col(\"frontpage\") == \"false\").sample(withReplacement=True, fraction=ratio)\n",
    "    balanced_data = sampled_negative_data.union(data.filter(col(\"frontpage\") == \"true\"))\n",
    "\n",
    "# Fit the pipeline model\n",
    "pipeline_model = pipeline.fit(balanced_data)\n",
    "balanced_data = pipeline_model.transform(balanced_data)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "train_data, test_data = balanced_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Initialize the Gradient Boosted Trees classifier\n",
    "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Train the model and make predictions\n",
    "gbt_model = gbt.fit(train_data)\n",
    "predictions = gbt_model.transform(test_data)\n",
    "predictions.select(\"source_text\", \"probability\", \"prediction\", \"label\").show(n=20)\n",
    "\n",
    "# Initialize the evaluator for binary classification\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\")\n",
    "\n",
    "# Calculate the ROC\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "print(\"Test Area Under ROC: \", accuracy)\n",
    "\n",
    "# Calculate other evaluation metrics\n",
    "tp = predictions[(predictions.label == 1) & (predictions.prediction == 1)].count()  # True Positives\n",
    "tn = predictions[(predictions.label == 0) & (predictions.prediction == 0)].count()  # True Negatives\n",
    "fp = predictions[(predictions.label == 0) & (predictions.prediction == 1)].count()  # False Positives\n",
    "fn = predictions[(predictions.label == 1) & (predictions.prediction == 0)].count()  # False Negatives\n",
    "\n",
    "precision = tp / (tp + fp) if tp + fp != 0 else 0  # Precision\n",
    "recall = tp / (tp + fn) if tp + fn != 0 else 0  # Recall\n",
    "f1 = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0  # F1 Score\n",
    "\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1)\n",
    "\n",
    "# Save Pipeline and Model\n",
    "pipeline_model.write().overwrite().save(\"/Users/xiaodi/anaconda3/spark/notebooks/saved_pipeline\")\n",
    "gbt_model.write().overwrite().save(\"/Users/xiaodi/anaconda3/spark/notebooks/saved_gbt_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ef972b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
