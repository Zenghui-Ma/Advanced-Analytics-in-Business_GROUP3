{"aid": "39975330", "title": "The Effects of Rank, Epochs, and Learning Rate on Training Textual LoRAs", "url": "https://blog.runpod.io/the-effects-of-rank-epochs-and-learning-rate-on-training-textual-loras/", "domain": "runpod.io", "votes": 2, "user": "aadillpickle", "posted_at": "2024-04-09 00:59:55", "comments": 0, "source_title": "The Effects Of Rank, Epochs, and Learning Rate on Training Textual LoRAs", "source_text": "The Effects Of Rank, Epochs, and Learning Rate on Training Textual LoRAs\n\nRunPod Blog\n\nSign in Subscribe\n\nText Generation\n\n# The Effects Of Rank, Epochs, and Learning Rate on Training Textual LoRAs\n\n#### Brendan McKeag\n\nAug 10, 2023 \u2022 9 min read\n\nHave you ever wanted to have a large language model tell you stories in the\nvoice and style of your favorite author? Well, through training a LoRA on\ntheir work, you can totally do that! There are so many different levers to\nflip when training a LoRA, though, and it can be a little intimidating to\ndetermine how to even start, especially because training on a large number of\nepochs can take several hours even on high-spec pods. Although creating LoRAs\nwill always involve a lot of testing and experimentation, here's how to get\nyourself started so you can get to the tweaking and fine-tuning steps.\n\n## Step 1: Accumulate a Corpus of Text\n\nIf you'd like to train a LoRA on a specific author, you'll need to get as much\ntext as is feasible from them. To train a LoRA on a 7b model, you'll want to\nshoot for at least 1MB of raw text if possible (approximately 700 pages.) For\nlarger models, even more text is better. Project Gutenberg and The Internet\nArchive have tons of public domain books, downloadable in plain text, to get\nyourself started. Alternatively, if you have .pdfs of books, you can convert\nthose to plain text as well. (If you are intent on distributing LoRAs trained\non authors, though, I do implore you to source your data responsibly!)\n\nFor this example, I've accumulated approximately 800 pages of Vladimir\nNabokov's work over three books and saved them to a plain .txt file.\n\n## Step 2: Start up an LLM Pod on RunPod\n\nStart up a pod in our Secure or Community Cloud with the TheBloke LLM\ntemplate. You won't need a lot of disk space (about 30-40 GB should do for a\n7b model) but you will want at least 48GB of VRAM.\n\n## Step 3: Transfer a Model and Your Corpus\n\nYou'll need to first download your model. When starting out, you'll definitely\nwant to stick to unquantized models (so no GPTQ, etc.) You'll also need to be\nsure that you can load the model in 8-bit, which is why you'll need the extra\nVRAM. First, download the model by cd'ing into the text-generation-webui\nfolder in Terminal and running the following:\n\npython3 download-model.py NousResearch/Llama-2-7b-hf\n\nThen, you'll need to transfer your text to the /training/datasets folder. If\nyou can host your .txt somewhere on the Internet, you can simply do a wget to\nthat URL from that folder and it will download straight to that folder. Else,\nif it's on your local PC, you can transfer it through runpodctl instead.\n\n## Step 4: Start Training!\n\nNow, it's time to start training your LoRAs. First, make sure the model is\nloaded in 8 bit by ensuring the load-in-8bit checkbox is checked, and then\nclick Load.\n\nThen, click on the Training tab. Wow, there's a lot of options there! However,\nI've highlighted the truly important ones:\n\nName: The name of the LoRA. You'll want to follow a certain naming convention\nwhile you train them to keep them organized (shown below.)\n\nEpochs: The number of times your dataset is run against the model.\n\nLearning Rate: How much the LoRA learns from each training run. Think of this\nas how slowly the dataset is being read during the training process. You know\nhow when you read a book more slowly in real life, you remember more of it?\nIt's basically that. Think of an epoch as reading a book cover to cover, and\nthe learning rate is how quickly you do that.\n\nLoRA Rank: Think of this as similar to the Temperature control when generating\ntext. This is how deeply the LoRA applies what it's taught. A lower rank value\nmeans that the LoRA will only take stylistic cues, while a higher rank value\nmeans it also learns things like facts and vocabulary. As a rule of thumb, set\nLoRA Alpha to be twice the rank.\n\nAs you go through and train LoRAs, be sure to create them with descriptive\nnames so you can keep them separated from each other. I recommend using a\nconvention like this:\n\nSo the first one would be 3 epochs, a learning rate of 3e-4, and a rank of 16.\n\n### Evaluating your LoRAs\n\nWith textual LoRAs, there's a lot of trial and error due to so many x-y\nfactors involved with the training data as well as the model you are training\nit on. So there are two criteria you'll want to keep in mind:\n\n1.) How well does it adhere to the spirit of the dataset text?\n\n2.) How coherent is it while applying that spirit?\n\nThe only way to find that out is to start training.\n\nBy way of example, I loaded Llama-2-7b and prompted the example assistant in\ntext-generation-webui (Chiharu) with the following query: Tell me a story\nabout a young girl in simple, easy to understand English. The intention is to\nget simple, easy to understand text that contrasts with the complicated prose\nin the Nabokov training text.\n\nBaseline:\n\n> When I was little, my mother told me stories about princesses who lived far\n> away from their homes, fighting against monsters and evil people. They were\n> strong women, brave enough to face danger head-on without fear or\n> hesitation. Those stories inspired me to become someone like them. Someone\n> who could stand up for herself no matter what happened around her, even if\n> it meant putting herself at risk.\n\nSimple, easy to read text, like you might see in a grade school text. For\nreference, here's a snippet from the training text to give you an idea of the\ndifference in style:\n\n> Then, still ignoring the raincoated phantasm that had stopped in midstairs,\n> master walked into a cozy boudoir across the hall from the drawing room,\n> through which--taking it easy, knowing he was safe--I now went away from\n> him, and in a bar-adorned kitchen gingerly unwrapped dirty Chum, talking\n> care not to leave any oil stains on the chrome--I think I got the wrong\n> product, it was black and awfully messy. In my usual meticulous way, I\n> transferred naked Chum to a clean recess about me and made for the little\n> boudoir. My step, as I say, was springy--too springy perhaps for success.\n> But my heart pounded with tiger joy, and I crunched a cocktail glass\n> underfoot.\n\nSo what do we get when we train a LoRA on this text and ask it for a simple\nstory?\n\n3 epochs, learning rate 3e-4, rank 16:\n\n> Once upon a time there lived an adorably cute little girl who loved\n> everything pink and purple. She would spend hours daydreaming of all the\n> wonderful things she wanted to buy someday--beautiful dresses, dolls, teddy\n> bears, shoes...the list went on and on forever! One day while walking\n> through town she met some very special people who had a huge store filled\n> with every single thing she could possibly dream of owning. They offered to\n> give her anything she liked if only she would come live with them inside\n> their magical shop full of treasures. The little girl agreed without\n> hesitation because nothing mattered more than being surrounded by beautiful\n> objects everywhere she turned. And thus began life inside the shop where she\n> could play at will until late into the night.\n\nNot much has changed in the vocabulary or style. We are, however, starting to\nsee the double hyphen (and later, parenthetical asides) that Nabokov was very\nfond of.\n\n3 epochs, learning rate 3e-4, rank 128:\n\n> Once upon a time there lived a sweet and kind little girl. The sun shone\n> brightly down as she danced across the meadow barefoot. Her golden brown\n> hair whirled all around her and tiny birds flitted among them, twittering\n> happily. As she ran her hands lovingly through their feathers they sang\n> louder and brighter than ever before, filling her heart with joy! Oh if only\n> those dear ones might remain in this state eternally. \"Oh, Wynne,\" moaned\n> her grandfather who sat next door smoking his pipe; \"such a pity.\"\n\nThe language is becoming a bit more verbose. But looking at the next..\n\n3 epochs, learning rate 3e-4, rank 256:\n\n> It was a cold winter evening, but 4 year old Lilly still refused to wear\n> shoes even though there were snow flurries blown across Main Street. Her\n> mother kept insisting; her father, an aspiring actor, had gone somewhere to\n> perform at night. She walked barefoot toward their neighbor's house, whose\n> porch light seemed more inviting than those of other houses on that lonely\n> block because it reminded Lily of candied peels. But before she got closer\n> (and perhaps just to tease herself), she looked down again--at her little\n> naked feet--which were getting colder with each passing minute. Then,\n> without saying a word, she began running towards another home which had a\n> brightly lit chimney, hoping to hear someone else singing Christmas carols.\n\nWe are seeing a huge jump in Nabakov-ness with the parentheticals, hyphens,\nand character descriptiveness. With this data set, it appears that Rank 256\nprovides the best balance between coherence and spirit. The language is\ncomplicated, owing to the thick prose in the training text, but it tells a\nstory that follows smoothly from one sentence to the next.\n\n3 epochs, learning rate 3e-4, rank 512:\n\n> Okay. It was in the spring. This tiny fellow (I am small) and I (she is\n> small too) sat side by side on some damp sand near a rocky seaport clad in\n> an old shabby raincoat and a pair of ancient rubbers. The waves broke lazily\n> against stones at his feet and beyond their brown crest the town glittered\n> greenly on its hill, between the dark harbor basin and the azure ocean\n> stretching away forever towards China. A red balloon floated high overhead\n> while the shadow under our little feet grew longer each minute, slowly\n> enveloping and blotting out the whole world; and neither of us moved despite\n> its increasing warmth nor made even the slightest gesture or sound, though a\n> million words were burning to rush forth.\n\nAgain, it's clear that the training text is really having a huge effect on how\nthe text is being output. There are parentheticals included, but they are\nconfusingly laid out and nonsensical. The story also isn't coherent at all,\nand doesn't seem to follow any kind of plot. However, what if we train for\nmore epochs at this rank?\n\n10 epochs, learning rate 3e-4, rank 512:\n\n> It was in the spring. We all know that time period has its charm; the cherry\n> blossoms are blooming. It's nice because they are also pretty and it's not\n> too hot yet. You can enjoy them from your room window, but it's also nice to\n> go to Ueno Park. The weather is just right for that kind of thing--nice\n> enough for jackets but warm enough to sit outside. In the evening it gets\n> colder and you want something warm to drink. So I'll tell you about an old\n> fashioned coffee house I went to. They still have these places. I think\n> they're more popular in Europe than here, actually. But in Japan, they are\n> everywhere! Anyway, it's very retro and very expensive but also quite cozy.\n> I remember ordering this delicious iced coffee, the flavor is caramel or\n> chocolate--I don't know exactly what it's called.\n\nAt this point, we can see that it still does follow the training text.\nHowever, both the impact that the style has, as well as the coherence problems\nare attenuated. You'll notice that when the epochs increased, it lost a lot of\nthe \"literary\" feel that the higher ranks had, but it also turned out to be\neasier to follow.\n\nLeaving the number of epochs the same, but lowering the learning rate had a\nsimilar effect:\n\n3 epochs, learning rate 3e-6, rank 512:\n\n> One day while I was playing my Nintendo, a beautiful little girl came into\n> the room. She had dark hair and glowing green eyes. \"What are you doing?\"\n> She asked me with a wide smile on her face. I couldn't help but stare at\n> her, captivated by her innocent and charming demeanor. As she played along\n> with me, I became more intrigued by her unique beauty.\n\n> After an hour or two passed by, I felt myself becoming enchanted by her\n> presence even more. Our game eventually turned into something more intimate\n> than I expected it would, yet I didn't care because all that mattered was\n> being in her presence. After our playtime ended, we said goodbye as if\n> nothing happened between us\u2014but it certainly left a deep impression upon\n> both our minds forevermore. And thus, my life changed completely ever since\n> meeting her that night.\n\nAs you can see, leaving the epochs the same, but lowering the learning rate\nhas a similar attenuation on the language. It's much less literary and more\nconversational again in the same manner.\n\nConclusion\n\nAs you can see, changing the rank is the largest adjustment you can make on\nhow strong a LoRA is at altering the text. Think of the rank as volume, and\nthe learning rate and epoch count as gain and equalizers to adjust the impact\nof the rank. Having too low of a rank, or too high of an epoch count or learn\nrate will mean that you may not notice much, if any, affect on the output. So\nfirst, crank up the rank to get yourself in the ballpark, and then adjust the\nepoch and learning rate as finer adjustments to get closer to where you need\nto be. Every model and dataset combination is different, so there's going to\nbe a huge case of YMMV - but using this technique will get to where you want\nto be in the most efficient manner.\n\n## Subscribe to our newsletter.\n\nEnter your email\n\nSubscribe\n\n## Unleash Your Imagination: Generate Uncensored & Unrestricted NSFW Images on\nRunPod\n\nToday, we're not just pushing the envelope\u2014we're tearing it open. Discover how\nto generate uncensored NSFW images with RunPod, embracing the limitless\npotential of open-source innovation.\n\nApr 1, 2024 1 min read\n\n## Generate Images with Stable Diffusion on RunPod\n\n\ud83d\udca1RunPod is hosting an AI art contest, find out more on our Discord in the\n#art-contest channel. In this tutorial, you will learn how to generate images\nusing Stable Diffusion, a powerful text-to-image model, on the RunPod\nplatform. By following the step-by-step instructions, you'll set up the\nprerequisites,\n\nMar 27, 2024 3 min read\n\n## Announcing RunPod\u2019s Integration with SkyPilot\n\nRunPod is excited to announce its latest integration with SkyPilot, an open-\nsource framework for running LLMs, AI, and batch jobs on any cloud. This\ncollaboration is designed to significantly enhance the efficiency and cost-\neffectiveness of your development process, particularly for training, fine-\ntuning, and deploying models. What is SkyPilot? SkyPilot is\n\nMar 13, 2024 3 min read\n\nRunPod Blog \u00a9 2024\n\nPowered by Ghost\n\n", "frontpage": false}
