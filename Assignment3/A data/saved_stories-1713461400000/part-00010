{"aid": "40073565", "title": "Open Source in the Age of LLMs", "url": "https://blog.mozilla.ai/open-source-in-the-age-of-llms/", "domain": "blog.mozilla.ai", "votes": 2, "user": "rrampage", "posted_at": "2024-04-18 06:39:32", "comments": 0, "source_title": "Open Source in the Age of LLMs", "source_text": "Open Source in the Age of LLMs\n\nSign in Subscribe\n\n# Open Source in the Age of LLMs\n\n#### Vicki Boykis\n\nApr 10, 2024 \u2014 5 min read\n\nPhoto by Wes Hicks / Unsplash\n\nLike our parent company, Mozilla.ai\u2019s founding story is rooted in open-source\nprinciples and community collaboration. Since our start last year, our key\nfocus has been exploring state-of-the-art methods for evaluating and fine-\ntuning large-language models (LLMs), including continued open-source LLM\nevaluation experiments and establishing our GPU cluster\u2019s infrastructure.\n\nThis initial work paved the way to build lm-buddy, our reference framework for\nevaluation and fine-tuning. Building on top of this, in early 2024 we started\ndeveloping our product, a platform designed to empower developers and\norganizations to independently develop, deploy, and evaluate large language\nmodels within an open-source ecosystem.\n\nThroughout this process, we\u2019ve been diving into the open-source ecosystem\naround LLMs. What we\u2019ve found is an electric environment where everyone is\nbuilding. As Nathan Lambert writes in his post, \u201cIt\u2019s 2024 and they just want\nto learn.\u201d\n\n> \u201cWhile everything is on track across multiple communities, that also unlocks\n> the ability for people to tap into excitement and energy that they\u2019ve never\n> experienced in their career (and maybe lives).\u201d\n\nThe energy in the space, with new model releases every day, is made even more\nexciting by the promise of open source where, as I\u2019ve observed before, anyone\ncan make a contribution and have it be meaningful regardless of credentials,\nand there are plenty of contributions to be made. If the fundamental question\nof the web is, \u201cWhy wasn\u2019t I consulted,\u201d open-source in machine learning today\noffers the answer, \u201cYou are as long as you can productively contribute PRs,\ncome have a seat at the table.\u201d\n\nWithin MzAI, we\u2019ve been actively contributing to the ecosystem across several\ndifferent projects focused on open-source large language model development,\nevaluation, and serving. Our recent contributions have included:\n\n  * Enabling OpenAI-style chat completions in EleutherAI\u2019s lm-evaluation-harness to be able to run self-hosted inference servers with local fine-tuned models for evaluation (versus pulling from HuggingFace Hub or hitting APIs).\n  * Surfacing constructor logic in HuggingFace\u2019s PEFT library to load the configuration of a given model adapter from a set of Python kwargs.\n  * Improving KubeRay\u2019s Helm configuration chart to reduce redundancy by adding a shared \u2018common\u2019 section to share config values between head and worker nodes.\n  * Enabling the setting of trust_remote_code=True in lm-evaluation-harness to anticipate upstream changes in HuggingFace\u2019s datasets module of setting that value to false, improving open source dataset ecosystem security.\n  * Improving the robustness of the library in lm-evaluation-harness through tests for command-line input args.\n\nEven though some of us have been active in open-source work for some time,\nbuilding and contributing to it at a team and company level is a qualitatively\ndifferent and rewarding feeling. And it's been especially fun watching\nupstream make its way into both the communities and our own projects.\n\nAt a high level, here\u2019s what we\u2019ve learned about the process of successful\nopen-source contributions:\n\n  1. Start small when you\u2019re starting with a new project. If you\u2019re contributing to a new project for the first time, it takes time to understand the project\u2019s norms: how fast they review, who the key people are, their preferences for communication, code review style, build systems, and more. It\u2019s like starting a new job entirely from scratch. Be gentle with both yourself and the reviewers and pick something like a documentation task, or a \u201cgood first issue\u201d label just to get a feel for how things work.\n\n  2. Be easy to work with. There are specific norms around working with open source, and they closely follow this fantastic post of understanding how to be an effective developer - \u201cAs a developer you have two jobs: to write code, and be easy to work with.\u201dIn open source, being easy to work with means different things to different people, but I generally see it as:\n\n    1. Submitting clean PRs with working code that passes tests or gets as close as possible. No one wants to fix your build.\n    2. Making small code changes by yourself, and proposing larger architecture changes in a group before getting them down in code for approval. Asking \u201cWhat do you think about this?\u201d Always try to also propose a solution instead of posing more problems to maintainers: they are busy!\n    3. Write unit tests if you\u2019re adding a significant feature, where significant is anything more than a single line of code.\n    4. Remembering Chesterton\u2019s fence: that code is there for a reason, study it before you suggest removing it.\n\n  3. Assume good intent, but make intent explicit. When you\u2019re working with people in writing, asynchronously, potentially in other countries or timezones, it\u2019s extremely easy for context, tone, and intent to get lost in translation. Implicit knowledge becomes rife. Assume people are doing the best they can with what they have, and if you don\u2019t understand something, ask about it first.\n\n  4. The AI ecosystem moves quickly. Extremely quickly. New models come out every day and are implemented in downstream modules by tomorrow. Make sure you\u2019re ok with this speed and match the pace. Something you can do before you do PRs is to follow issues on the repo and follow the repo itself so you get a sense of how quickly things move/are approved. If you\u2019re into fast-moving projects, jump in. Otherwise, pick one that moves at a slower cadence.\n\n  5. The LLM ecosystem is currently bifurcated between HuggingFace and OpenAI compatibility: An interesting pattern has developed in my development work on open-source in LLMs. It\u2019s become clear to me that, in this new space of developer tooling around transformer-style language models at an industrial scale, you are generally conforming to be downstream of one of two interfaces:\n\n    1. models that are trained and hosted using HuggingFace libraries and particularly the HuggingFace hub as infrastructure - in practicality, this means dependence on PyTorch\u2019s programming paradigm, which HuggingFace tools wrap (although they now also provide interop between Tensorflow and JAX)\n    2. Models that are available via API endpoints, particularly as hosted by OpenAI. Given that OpenAI was a first mover in the product LLM space, they currently have the API advantage, and many tools that have developed have developed OpenAI-compatible endpoints which don\u2019t always mean using OpenAI, but conform to the same set of patterns that the Chat Completions API v1/chat/completionsoffers.\n\nFor example, adding OpenAI-style interop chat completions allowed us to stand\nup our own vLLM OpenAI-compatible server that works against models we\u2019ve\nstarted with on HuggingFace and fine-tuned locally.\n\nIf you want to be successful in this space today, you as a library or service\nprovider have to be able to interface with both of these.\n\n  6. Sunshine is the best disinfectant. As the recent xz issue proved, open code is better code, and issues get fixed more quickly. This means don\u2019t be afraid to work out in the open. All code has bugs, even yours and mine, and discovering those bugs is a natural process of learning and developing better code rather than a personal failing.\n\nWe\u2019re looking forward to continuing our contributions, upstreaming them, and\nlearning from them as we continue our product development work.\n\n## Read more\n\n### The cost of cutting-edge: Scaling compute and limiting access\n\nIn a year marked by extraordinary advancements in artificial intelligence,\nlargely driven by the evolution of large language models (LLMs), one factor\nstands out as a universal accelerator: the exponential growth in computational\npower. Over the last few years, researchers have continuously pushed the\nboundaries of what a \u2018large\u2019 language\n\nBy Stefan French Mar 20, 2024\n\n### LLM evaluation at scale with the NeurIPS Large Language Model Efficiency\nChallenge\n\nAfter a year of breakneck innovation and hype in the AI space, we have now\nmoved sufficiently beyond the peak of the hype cycle to start asking a\ncritical question: are LLMs good enough yet to solve all of the business and\nsocietal challenges we are setting them up for?\n\nBy Vicki Boykis Feb 21, 2024\n\n### Introducing Mozilla.ai: Investing in trustworthy AI\n\nAnnouncing Mozilla.ai, a startup building a trustworthy and open-source AI\necosystem with agency, accountability, and openness at its core. Mozilla will\nmake an initial $30M investment in the company to fund the vision for making\nit easy to develop trustworthy AI products.\n\nBy Mark Surman Jan 23, 2024\n\nPowered by Ghost\n\n## Mozilla.ai Blog\n\nSubscribe to receive our publications\n\n", "frontpage": false}
