{"aid": "40133558", "title": "Stable Diffusion Benchmarks: 45 Nvidia, AMD, and Intel GPUs Compared (2023)", "url": "https://www.tomshardware.com/pc-components/gpus/stable-diffusion-benchmarks", "domain": "tomshardware.com", "votes": 2, "user": "pantalaimon", "posted_at": "2024-04-23 16:07:11", "comments": 0, "source_title": "Stable Diffusion Benchmarks: 45 Nvidia, AMD, and Intel GPUs Compared", "source_text": "Stable Diffusion Benchmarks: 45 Nvidia, AMD, and Intel GPUs Compared | Tom's Hardware\n\nSkip to main content\n\nWhen you purchase through links on our site, we may earn an affiliate\ncommission. Here\u2019s how it works.\n\n# Stable Diffusion Benchmarks: 45 Nvidia, AMD, and Intel GPUs Compared\n\nFeatures\n\nBy Jarred Walton\n\npublished December 15, 2023\n\nWhich graphics card offers the fastest AI performance?\n\n(Image credit: Tom's Hardware)\n\nJump to:\n\n  * Introduction\n  * 512x512 Benchmarks\n  * 768x768 Benchmarks\n  * Picking an SD Model\n  * Batch Sizes\n  * Test Setup\n  * Theoretical GPU Performance\n\n### Stable Diffusion Introduction\n\nStable Diffusion and other AI-based image generation tools like Dall-E and\nMidjourney are some of the most popular uses of deep learning right now. Using\ntrained networks to create images, videos, and text has become not just a\ntheoretical possibility but is now a reality. While more advanced tools like\nChatGPT can require large server installations with lots of hardware for\ntraining, running an already-trained network for inference can be done on your\nPC, using its graphics card. How fast are consumer GPUs for doing AI inference\nusing Stable Diffusion? That's what we're here to investigate.\n\nWe've benchmarked Stable Diffusion, a popular AI image generator, on the 45 of\nthe latest Nvidia, AMD, and Intel GPUs to see how they stack up. We've been\npoking at Stable Diffusion for over a year now, and while earlier iterations\nwere more difficult to get running \u2014 never mind running well \u2014 things have\nimproved substantially. Not all AI projects have received the same level of\neffort as Stable Diffusion, but this should at least provide a fairly\ninsightful look at what the various GPU architectures can manage with AI\nworkloads given proper tuning and effort.\n\nThe easiest way to get Stable Diffusion running is via the Automatic1111 webui\nproject. Except, that's not the full story. Getting things to run on Nvidia\nGPUs is as simple as downloading, extracting, and running the contents of a\nsingle Zip file. But there are still additional steps required to extract\nimproved performance, using the latest TensorRT extensions. Instructions are\nat that link, and we've previous tested Stable Diffusion TensorRT performance\nagainst the base model without tuning if you want to see how things have\nimproved over time. Now we're adding results from all the RTX GPUs, from the\nRTX 2060 all the way up to the RTX 4090, using the TensorRT optimizations.\n\nFor AMD and Intel GPUs, there are forks of the A1111 webui available that\nfocus on DirectML and OpenVINO, respectively. We used these webui OpenVINO\ninstructions to get Arc GPUs running, and these webui DirectML instructions\nfor AMD GPUs. Our understanding, incidentally, is that all three companies\nhave worked with the community in order to tune and improve performance and\nfeatures.\n\nWhether you're using an AMD, Intel, or Nvidia GPU, there will be a few hurdles\nto jump in order to get things running optimally. If you have issues with the\ninstructions in any of the linked repositories, drop us a note in the comments\nand we'll do our best to help out. Once you have the basic steps down,\nhowever, it's not too difficult to fire up the webui and start generating\nimages. Note that extra functionality (i.e. upscaling) is separate from the\nbase text to image code and would require additional modifications and tuning\nto extract better performance, so that wasn't part of our testing.\n\nAdditional details are lower down the page, for those that want them. But if\nyou're just here for the benchmarks, let's get started.\n\n### Stable Diffusion 512x512 Performance\n\n(Image credit: Tom's Hardware)\n\nThis shouldn't be a particularly shocking result. Nvidia has been pushing AI\ntechnology via Tensor cores since the Volta V100 back in late 2017. The RTX\nseries added the feature in 2018, with refinements and performance\nimprovements each generation (see below for more details on the theoretical\nperformance). With the latest tuning in place, the RTX 4090 ripped through\n512x512 Stable Diffusion image generation at a rate of more than one image per\nsecond \u2014 75 per minute.\n\nAMD's fastest GPU, the RX 7900 XTX, only managed about a third of that\nperformance level with 26 images per minute. Even more alarming, perhaps, is\nhow poorly the RX 6000-series GPUs performed. The RX 6950 XT output 6.6 images\nper minute, well behind even the RX 7600. Clearly, AMD's AI Matrix\naccelerators in RDNA 3 have helped improve throughput in this particular\nworkload.\n\nIntel's current fastest GPU, the Arc A770 16GB, managed 15.4 images per\nminute. Keep in mind that the hardware has theoretical performance that's\nquite a bit higher than the RTX 2080 Ti (if we're looking at XMX FP16\nthroughput compared to Tensor FP16 throughput): 157.3 TFLOPS versus 107.6\nTFLOPS. It looks like the Arc GPUs are thus only managing less than half of\ntheir theoretical performance, which is why benchmarks are the most important\ngauge of real-world performance.\n\nWhile there are differences between the various GPUs and architecture,\nperformance largely scales proportionally with theoretical compute. The RTX\n4090 was 46% faster than the RTX 4080 in our testing, while in theory it\noffers 69% more compute performance. Likewise, the 4080 beat the 4070 Ti by\n24%, and it has 22% more compute.\n\nThe newer architectures aren't necessarily performing substantially faster.\nThe 4080 beat the 3090 Ti by 10%, while offering potentially 20% more compute.\nBut the 3090 Ti also has more raw memory bandwidth (1008 GB/s compared to the\n4080's 717 GB/s), and that's certainly a factor. The old Turing generation\nheld up as well, with the newer RTX 4070 beating the RTX 2080 Ti by just 12%,\nwith theoretically 8% more compute.\n\n### Stable Diffusion 768x768 Performance\n\n(Image credit: Tom's Hardware)\n\nKicking the resolution up to 768x768, Stable Diffusion likes to have quite a\nbit more VRAM in order to run well. Memory bandwidth also becomes more\nimportant, at least at the lower end of the spectrum.\n\nThe relative positioning of the various Nvidia GPUs doesn't shift too much,\nand AMD's RX 7000-series gains some ground with the RX 7800 XT and above,\nwhile the RX 7600 dropped a bit. The 7600 was 36% slower than the 7700 XT at\n512x512, but dropped to being 44% slower at 768x768.\n\nThe previous generation AMD GPUs had an even tougher time. The RX 6950 XT\ndidn't even manage two images per minute, and the 8GB RX 6650 XT, 6600 XT, and\n6600 all failed to render even a single image. That's a bit odd, as the RX\n7600 still worked okay with only 8GB of memory, but some other architectural\ndifference was at play.\n\nIntel's Arc GPUs also lost ground at the higher resolution, or if you prefer,\nthe Nvidia GPUs \u2014 particularly the fastest models \u2014 put some additional\ndistance between themselves and the competition. The 4090 for example was 4.9X\nfaster than the Arc A770 16GB at 512x512 images, and that increased to a 6.4X\nlead with 768x768 images.\n\nWe haven't tested SDXL, yet, mostly because the memory demands and getting it\nrunning properly tend to be even higher than 768x768 image generation.\nTensorRT support is also missing for Nvidia GPUs, and most likely we'd see\nquite a few GPUs struggle with SDXL. It's something we plan to investigate in\nthe future, however, as the results are generally preferable to SD1.5 and\nSD2.1 for higher resolution outputs.\n\nFor now, we know that performance will be lower than our 768x768 results. As\nan example of what to expect, the RTX 4090 doing 1024x1024 images (still using\nSD1.5), managed just 13.4 images per minute. That's less than half the speed\nof 768x768 image generation, which makes sense as the 1024x1024 images have\n78% more pixels and the time required seems to scale somewhat faster than the\nresolution increase.\n\n### Picking a Stable Diffusion Model\n\nImage 1 of 3\n\nDirectly trying for 1920x1080 generation(Image credit: Tom's Hardware)\n\nAnother attempt at 1920x1080 generation(Image credit: Tom's Hardware)\n\nUpscaling via SwinIR_4x from 768x768 to 1920x1080(Image credit: Tom's\nHardware)\n\nDeciding which version of Stable Generation to run is a factor in testing.\nCurrently, you can find v1.4, v1.5, v2.0, and v2.1 models from Hugging Face,\nalong with the newer SDXL. The earlier 1.x versions were mostly trained on\n512x512 images, while 2.x included more training data for up to 768x768\nimages. SDXL targets 768x768 to 1024x1024 images. As noted above, higher\nresolutions also require more VRAM. Different versions of Stable Diffusion can\nalso generate radically different results from the same prompt, due to\ndifferences in the training data.\n\nIf you try to generate a higher resolution image than the training data, you\ncan end up with \"fun\" results like the multi-headed, multi-limbed, multi-eyed,\nor multi-whatever examples shown above. You can try to work around these via\nvarious upscaling tools, but if you're thinking about just generating a bunch\nof 4K images to use as your Windows desktop wallpaper, be aware that it's not\nas straightforward as you'd probably want it to be. (Our prompt for the above\nwas \"Keanu Reeves portrait photo of old warrior chief, tribal panther make up,\nblue on red, side profile, looking away, serious eyes, 50mm portrait\nphotography, hard rim lighting photography\" \u2014 taken from this page if you're\nwondering.)\n\nIt's also important to note that not every GPU has received equal treatment\nfrom the various projects, but the core architectures are also a big factor.\nNvidia has had Tensor cores in all of its RTX GPUs, and our understanding is\nthat the current TensorRT code only uses FP16 calculations, without sparsity.\nThat explains why the scaling from 20-series to 30-series to 40-series GPUs\n(Turing, Ampere, and Ada Lovelace architectures) mostly correlates with the\nbaseline Tensor FP16 rates.\n\nAs shown above, performance on AMD GPUs using the latest webui software has\nimproved throughput quite a bit on RX 7000-series GPUs, while for RX\n6000-series GPUs you may have better luck with using Nod.ai's Shark version \u2014\nand note that AMD has recently acquired Nod.ai. Throughput with SD2.1 in\nparticular was faster with the RDNA 2 GPUs, but then the results were also\ndifferent from SD1.5 and thus can't be directly compared. Nod.ai doesn't have\n\"sharkify\" tuning if you use SD1.5 models either, which resulted in lower\nperformance with our apples to apples testing.\n\nLATEST VIDEOS FROM tomshardware\n\n### Test Setup: Batch Sizes\n\nImage 1 of 14\n\n(Image credit: Tom's Hardware)\n\n(Image credit: Tom's Hardware)\n\n(Image credit: Tom's Hardware)\n\n(Image credit: Tom's Hardware)\n\n(Image credit: Tom's Hardware)\n\n(Image credit: Tom's Hardware)\n\n(Image credit: Tom's Hardware)\n\n(Image credit: Tom's Hardware)\n\n(Image credit: Tom's Hardware)\n\n(Image credit: Tom's Hardware)\n\n(Image credit: Tom's Hardware)\n\n(Image credit: Tom's Hardware)\n\n(Image credit: Tom's Hardware)\n\n(Image credit: Tom's Hardware)\n\nThe above gallery shows some additional Stable Diffusion sample images, after\ngenerating them at a resolution of 768x768 and then using SwinIR_4X upscaling\n(under the \"Extras\" tab), followed by cropping and resizing. Hopefully we can\nall agree that these results look a lot better than the mangled Keanu Reeves\nattempts from above.\n\nFor testing, we followed the same procedures for all GPUs. We generated a\ntotal of 24 distinct 512x512 and 24 distinct 768x768 images, using the same\nprompt of \"messy room\" \u2014 short, sweet, and to the point. Doing 24 images per\nrun gave us plenty of flexibility, since we could do batches of 3x8 (three\nbatches of eight concurrent images), 4x6, 6x4, 8x3, 12x2, or 24x1, depending\non the GPU.\n\nWe did our best to optimize for throughput, which means running batch sizes\nlarger than one in many cases. Sometimes, the limiting factor in how many\nimages should be generated concurrently is VRAM capacity, but compute (and\ncache) also appear to factor in. As an example, the RTX 4060 Ti 16GB did best\nwith 6x4 batches, just like the 8GB model, while the 4070 did best with 4x6\nbatches.\n\nFor 512x512 image generation, many of Nvidia's GPUs did best generating three\nbatches of eight images each (the maximum batch size is eight), though we did\nfind that 4x6 or 6x4 worked slightly better on some of the GPUs. AMD's RX\n7000-series GPUs all liked 3x8 batches, while the RX 6000-series did best with\n6x4 on Navi 21, 8x3 on Navi 22, and 12x2 on Navi 23. Intel's Arc GPUs all\nworked well doing 6x4, except the A380 which used 12x2.\n\nFor 768x768 images, memory and compute requirements are much higher. Most of\nthe Nvidia RTX GPUs worked best with 6x4 batches, or 8x3 in a few instances.\n(Note that even the RTX 2060 with 6GB of VRAM was still best with 6x4\nbatches.) AMD's RX 7000-series again liked 3x8 for most of the GPUs, though\nthe RX 7600 needed to drop the batch size and ran 6x4. The RX 6000-series only\nworked at 24x1, doing single images at a time (otherwise we'd get garbled\noutput), and the 8GB RX 66xx cards all failed to render anything at the higher\ntarget output \u2014 you'd need to opt for Nod.ai and a different model on those\nGPUs.\n\n### Test Setup\n\nImage 1 of 3\n\n\"Messy Room\" on AMD GPU(Image credit: Tom's Hardware)\n\n\"Messy Room\" on Intel GPU(Image credit: Tom's Hardware)\n\n\"Messy Room\" on Nvidia GPU(Image credit: Tom's Hardware)\n\nStable Diffusion Testbed\n\nIntel Core i9-12900K MSI Pro Z690-A WiFi DDR4 Corsair 2x16GB DDR4-3600 CL16\nCrucial P5 Plus 2TB Cooler Master MWE 1250 V2 Gold Cooler Master PL360 Flux\nCooler Master HAF500 Windows 11 Pro 64-bit (22H2)\n\nOur test PC for Stable Diffusion consisted of a Core i9-12900K, 32GB of\nDDR4-3600 memory, and a 2TB SSD. We tested 45 different GPUs in total \u2014\neverything that has ray tracing hardware, basically, which also tended to\nimply sufficient performance to handle Stable Diffusion. It's possible to use\neven older GPUs, though performance can drop quite a bit if the GPU doesn't\nhave native FP16 support. Nvidia's GTX class cards were very slow in our\nlimited testing.\n\nIn order to eliminate the initial compilation time, we first generated a\nsingle batch for each GPU with the desired settings. Actually, we'd use this\nstep to determine the optimal configuration for batch size. Once we settled on\nthe batch size, we ran four iterations generating 24 images each, discarded\nthe slowest result, and averaged the time taken from the other three runs. We\nthen used this to calculate the number of images per minute that each GPU\ncould generate.\n\nOur chosen prompt was, again, \"messy room.\" We used the Euler Ancestral\nsampling method, 50 steps (iterations), with a CFG scale of 7. Because all of\nthe GPUs were running the same version 1.5 model from Stable Diffusion, the\nresulting images were generally comparable in content. We noticed previously\nthat SD2.1 tended to often generate \"messy rooms\" that weren't actually messy,\nand were sometimes cartoony. SD1.5 also seems to be preferred by many Stable\nDiffusion users as the later 2.1 models removed many desirable traits from the\ntraining data.\n\nThe above gallery shows an example output at 768x768 for AMD, Intel, and\nNvidia. Rest assured, all of the images appeared to be relatively similar in\ncomplexity and content \u2014 though I won't say I looked carefully at every one of\nthe thousands of images that were generated! For reference, the AMD GPUs\nresulted in around 2,500 total images, Nvidia GPUs added another 4,000+\nimages, with Intel only needing about 1,000 images. All of the same style\nmessy room.\n\n### Comparing Theoretical GPU Performance\n\nWhile the above testing looks at actual performance using Stable Diffusion, we\nfeel it's also worth a quick look at the theoretical GPU performance. There\nare two aspects to consider: First is the GPU shader compute, and second is\nthe potential compute using hardware designed to accelerate AI workloads \u2014\nNvidia Tensor cores, AMD AI Accelerators, and Intel XMX cores, as appropriate.\nNot all GPUs have additional hardware, which means they will use GPU shaders.\nLet's start there.\n\n## Stay on the Cutting Edge\n\nJoin the experts who read Tom's Hardware for the inside track on enthusiast PC\ntech news \u2014 and have for over 25 years. We'll send breaking news and in-depth\nreviews of CPUs, GPUs, AI, maker hardware and more straight to your inbox.\n\nBy submitting your information you agree to the Terms & Conditions and Privacy\nPolicy and are aged 16 or over.\n\nTheoretical GPU Shader Compute (Image credit: Tom's Hardware)\n\nFor FP16 compute using GPU shaders, Nvidia's Ampere and Ada Lovelace\narchitectures run FP16 at the same speed as FP32 \u2014 the assumption is that FP16\ncan and should be coded to use the Tensor cores. AMD and Intel GPUs in\ncontrast have double performance on half-precision FP16 shader calculations\ncompared to FP32, and that applies to Turing GPUs as well.\n\nThis leads to some potentially interesting behavior. The RTX 2080 Ti for\nexample has 26.9 TFLOPS of FP16 GPU shader compute, which nearly matches the\nRTX 3080's 29.8 TFLOPS and would clearly put it ahead of the RTX 3070 Ti's\n21.8 TFLOPS. AMD's RX 7000-series GPUs would also end up being much more\ncompetitive if everything were restricted to GPU shaders.\n\nClearly, this look at FP16 compute doesn't match our actual performance much\nat all. That's because optimized Stable Diffusion implementations will opt for\nthe highest throughput possible, which doesn't come from GPU shaders on modern\narchitectures. That brings us to the Tensor, Matrix, and AI cores on the\nvarious GPUs.\n\nImage 1 of 2\n\n(Image credit: Tom's Hardware)\n\n(Image credit: Tom's Hardware)\n\nNvidia's Tensor cores clearly pack a punch, except as noted before, Stable\nDiffusion doesn't appear to leverage sparsity with the TensorRT code. (It\ndoesn't use FP8 either, which could potentially double compute rates as well.)\nThat means, for the most applicable look at how the GPUs stack up, you should\npay attention to the first chart for Nvidia GPUs, which omits sparsity, rather\nthan the second chart that includes sparsity \u2014 also note that the non-TensorRT\ncode does appear to leverage sparsity.\n\nIt's interesting to see how the above chart showing theoretical compute lines\nup with the Stable Diffusion charts. The short summary is that a lot of the\nNvidia GPUs land about where you'd expect, as do the AMD 7000-series parts.\nBut the Intel Arc GPUs all seem to get about half the expected performance \u2014\nnote that my numbers use the boost clock of 2.4 GHz rather than the lower\n2.0GHz \"Game Clock\" (which is a worst-case scenario that rarely comes into\nplay, in my experience).\n\nThe RX 6000-series GPUs likewise underperform, likely because doing FP16\ncalculations via shaders is less efficient than doing the same calculations\nvia RDNA 3's WMMA instructions. Otherwise, the RX 6950 XT and RX 6900 XT\nshould at least manage to surpass the RX 7600, and that didn't happen in our\ntesting. (Again, performance on the RDNA 2 GPUs tends to be better using\nNod.ai's project, if you're using one of those GPUs and want to improve your\nimage throughput.)\n\nWhat's not clear is just how much room remains for further optimizations with\nStable Diffusion. Looking just at the raw compute, we'd think that Intel can\nfurther improve the throughput of its GPUs, and we also have to wonder if\nthere's a reason Nvidia's 30- and 40-series GPUs aren't leveraging their\nsparsity feature with TensorRT. Or maybe they are and it just doesn't help\nthat much? (I did ask Nvidia engineers about this at one point and was told\nit's not currently used, but these things are still a bit murky.)\n\nStable Diffusion, and other text to image generators, are currently one of the\nmost developed and researched areas of AI that are still readily accessible to\nconsumer level hardware. We've looked at some other areas of AI as well, like\nspeech recognition using Whisper and chatbot text generation, but so far\nneither of those seem to be as optimized or used as Stable Diffusion. If you\nhave any suggestions for other AI workloads we should test, particularly\nworkloads that will work on AMD and Intel as well as Nvidia GPUs, let us know\nin the comments.\n\nJarred Walton\n\nJarred Walton is a senior editor at Tom's Hardware focusing on everything GPU.\nHe has been working as a tech journalist since 2004, writing for AnandTech,\nMaximum PC, and PC Gamer. From the first S3 Virge '3D decelerators' to today's\nGPUs, Jarred keeps up with all the latest graphics trends and is the one to\nask about game performance.\n\nMore about gpus\n\nRare GeForce GTX 2070 engineering sample has surfaced \u2014 the unreleased GPU has\n128 fewer CUDA cores than the RTX 2070\n\nNvidia Blackwell and GeForce RTX 50-Series GPUs: Rumors, specifications,\nrelease dates, pricing, and everything we know\n\nLatest\n\nEKWB reportedly plagued with financial disarray \u2014 many employees and suppliers\nwere allegedly left unpaid for as long as four months\n\nSee more latest \u25ba\n\n30 Comments Comment from the forums\n\n  * Bikki\n\nThanks so much for this, truly generative model is consumer gpu next big thing\nbesides gaming. Meta LLama 2 should be next in the pipe\nhttps://huggingface.co/models?other=llama-2\n\nReply\n\n  * -Fran-\n\nI've learned a lot today, Jarred. Thanks a lot for your review and efforts\ninto explaining everything related to gauging performance for the GPUs for\nthese tasks. Fantastic job.\n\nRegards.\n\nReply\n\n  * JarredWaltonGPU\n\n> Bikki said:\n>\n> Thanks so much for this, truly generative model is consumer gpu next big\n> thing besides gaming. Meta LLama 2 should be next in the pipe\n> https://huggingface.co/models?other=llama-2\n\nI've poked at LLaMa stuff previously with text generation, but what I need is\na good UI and method of benchmarking that can run on AMD, Intel, and Nvidia\nGPUs and leverage the appropriate hardware. Last I looked, most (all) of the\nrelated projects were focused on Nvidia, but there are probably some\nalternatives I haven't seen.\n\nWhat I really need is the equivalent across GPU vendor projects that will use\nLLaMa, not the model itself. Running under Windows 11 would be ideal. If you\nhave any suggestions there, let me know.\n\nReply\n\n  * dramallamadingdong\n\n> Admin said:\n>\n> We've tested all the modern graphics cards in Stable Diffusion, using the\n> latest updates and optimizations, to show which GPUs are the fastest at AI\n> and machine learning inference.\n>\n> Stable Diffusion Benchmarks: 45 Nvidia, AMD, and Intel GPUs Compared : Read\n> more\n\nthank you. Very informative.\n\nReply\n\n  * thisisaname\n\nWhich scripts do I have to let run to display the pictures?\n\nReply\n\n  * kfcpri\n\n> Admin said:\n>\n> We've tested all the modern graphics cards in Stable Diffusion, using the\n> latest updates and optimizations, to show which GPUs are the fastest at AI\n> and machine learning inference.\n>\n> Stable Diffusion Benchmarks: 45 Nvidia, AMD, and Intel GPUs Compared : Read\n> more\n\nAs a SD user stuck with a AMD 6-series hoping to switch to Nv cards, I think:\n\n1\\. It is Nov 23 already if people buy a new card with SD in mind now, they\nabsolutely should consider SDXL and even somewhat plan for \"the version after\nSDXL\" and so omitting it in a benchmark report is like wasting your own time\nand effort. Like, making a detailed benchmarking on Counterstrike but not\nCyberpunk in 2023, Of course the old cards can't run it so I think maybe a\nseparate SDXL report on just the >=12GB latest gens cards? The tests should be\na basic 1024x1024 one, plus another larger dimension one that kind of simulate\na potential future SD version. Some other such test elsewhere shows that the\n4060 ti 16GB will be faster than the 4070 in such vram heavy operation, and I\nhave been hoping to see more tests like that to confirm.\n\n2\\. I can understand not mentioning AMD with Olive, which is a quick\noptimizations but with many limitations and required extra preps on the\nmodels. However AMD on Linux with ROCm support most of the stuff now with few\nlimitations and it runs way faster than AMD on Win DirectML, so it should\nworth a mention. (I prefer to switch to Nv soon though)\n\nReply\n\n  * JarredWaltonGPU\n\n> kfcpri said:\n>\n> As a SD user stuck with a AMD 6-series hoping to switch to Nv cards, I\n> think:\n>\n> 1\\. It is Nov 23 already if people buy a new card with SD in mind now, they\n> absolutely should consider SDXL and even somewhat plan for \"the version\n> after SDXL\" and so omitting it in a benchmark report is like wasting your\n> own time and effort. Like, making a detailed benchmarking on Counterstrike\n> but not Cyberpunk in 2023, Of course the old cards can't run it so I think\n> maybe a separate SDXL report on just the >=12GB latest gens cards? The tests\n> should be a basic 1024x1024 one, plus another larger dimension one that kind\n> of simulate a potential future SD version. Some other such test elsewhere\n> shows that the 4060 ti 16GB will be faster than the 4070 in such vram heavy\n> operation, and I have been hoping to see more tests like that to confirm.\n>\n> 2\\. I can understand not mentioning AMD with Olive, which is a quick\n> optimizations but with many limitations and required extra preps on the\n> models. However AMD on Linux with ROCm support most of the stuff now with\n> few limitations and it runs way faster than AMD on Win DirectML, so it\n> should worth a mention. (I prefer to switch to Nv soon though)\n\nToo many things are \"broken\" with SDXL right now to reliably test it on all of\nthe different GPUs, as noted in the text. TensorRT isn't yet available, and\nthe DirectML and OpenVINO forks may also be iffy. I do plan on testing it, but\nit's easy enough to use regular SD plus a better upscaler (SwinIR_4x is a good\nexample) if all you want is higher resolutions. But SDXL will hopefully\nproduce better results as well. Anyway, just because some people have switched\nto SDXL doesn't make it irrelevant, as part of the reason for all these\nbenchmarks is to give a reasonable look at general AI inference performance.\nSD has been around long enough that it has been heavily tuned on all\narchitectures; SDXL is relatively new by comparison.\n\nRegarding AMD with Olive, you do realize that this is precisely what the\nlinked DirectML instructions use, right? I didn't explicitly explain that, as\ninterested parties following the link will have the necessary details. AMD's\nlatest instructions are to use the DirectML fork, and I'd be surprised if ROCm\nis actually much faster at this point. If you look at the theoretical FP16\nperformance, I'm reasonably confident the DirectML version gets most of what\nis available. ROCm also has limitations in which GPUs are supported, at least\nlast I checked (which has been a while).\n\nReply\n\n  * forrmorr134567\n\nhow did you get to 24 images per minute on 2080 super?\n\nReply\n\n  * JarredWaltonGPU\n\n> forrmorr134567 said:\n>\n> how did you get to 24 images per minute on 2080 super?\n\nMaybe read the article?\n\n\"Getting things to run on Nvidia GPUs is as simple as downloading, extracting,\nand running the contents of a single Zip file. But there are still additional\nsteps required to extract improved performance, using the latest TensorRT\nextensions. Instructions are at that link, and we've previous tested Stable\nDiffusion TensorRT performance against the base model without tuning if you\nwant to see how things have improved over time.\"\n\nSo you have to do the extra steps to get the TensorRT extension installed and\nconfigured in the UI, then pre-compile static sizes, normally a batch size of\n8 with 512x512 resolution.\n\nReply\n\n  * Elegant spy\n\nHello i want to ask so Intel Arc gpus works well using the automatic1111\nopenVINO version right ? does Intel Arc gpus still able to run SD using\ndirectml like amd gpus does ? thank you\n\nReply\n\n##### Most Popular\n\nHands-on with Corsair's 2500D Airflow case: Roomy at the back, for rear-\nconnector motherboard cables\n\n120mm AIO Roundup: Testing Be Quiet, Corsair, Cooler Master, and Enermax\nmodels\n\nHands-on with InWin's F5 PC Case: Back-connector motherboard support and wood\nfront panels\n\nThe five best AMD CPUs of all time: From old-school Athlon to brand-new Ryzen\n\nHands-on with Be Quiet\u2019s Dark Base Pro 901: Decibel dampener\n\nThis is the fastest SSD we've ever tested \u2014 Phison E26 Max14um 2TB performance\npreview\n\nHands-On: Cooler Master's NCore 100 Max case stands tall and handles large\nGPUs\n\nI built a PC With MSI's Project Zero Motherboard: Moving all the ports to the\nback for a cleaner, quicker build with better airflow\n\nSurging on Smartphones, Folding Screen Adoption on Laptops Will Take Years to\nGo Mainstream\n\nTesting GPUs with AMD FSR3 and Avatar: Frontiers of Pandora \u2014 16 graphics\ncards and hundreds of benchmarks\n\nHands-On: Lian Li's LCD Screen fans turn heads and are surprisingly\naffordable, but not as configurable as I'd like\n\nTom's Hardware is part of Future US Inc, an international media group and\nleading digital publisher. Visit our corporate site.\n\n\u00a9 Future US, Inc. Full 7th Floor, 130 West 42nd Street, New York, NY 10036.\n\n", "frontpage": false}
