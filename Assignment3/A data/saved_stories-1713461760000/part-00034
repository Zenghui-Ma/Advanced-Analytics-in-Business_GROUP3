{"aid": "40074804", "title": "PCIe 6 Overview. (2022)", "url": "https://pcisig.com/blog/evolution-pci-express-specification-its-sixth-generation-third-decade-and-still-going-strong", "domain": "pcisig.com", "votes": 1, "user": "fanf2", "posted_at": "2024-04-18 10:42:04", "comments": 0, "source_title": "The Evolution of the PCI Express Specification: On its Sixth Generation, Third Decade and Still Going Strong | PCI-SIG", "source_text": "The Evolution of the PCI Express Specification: On its Sixth Generation, Third Decade and Still Going Strong | PCI-SIG\n\n# The Evolution of the PCI Express Specification: On its Sixth Generation,\nThird Decade and Still Going Strong\n\n  * Posted on: 11 January 2022\n  * By Dr. Debendra Das Sharma, Member and Treasurer of PCI-SIG Board\n\nStandards & Compliance\n\nPCIe 6.0\n\nPCI Express 6.0\n\nPAM4\n\nPCIe Storage\n\nPCI Express specification\n\nFLIT\n\nForward Error Correct\n\nIntroduction\n\nThe PCI Express\u00ae (PCIe\u00ae) architecture has served as the backbone for I/O\nconnectivity spanning three decades, enabling power-efficient, high-bandwidth,\nand low-latency communication between components. PCI Express technology just\ndebuted its sixth generation at 64.0 GT/s, as part of its evolution, doubling\nthe bandwidth every generation, while maintaining full backwards compatibility\n(Figure 1). PCIe technology continues to outpace competing I/O technologies in\nterms of market share, capacity, and bandwidth and has continued as the\nubiquitous I/O interconnect across the entire compute continuum through its\nexistence. Its usage spans diverse market segments such as hand-held, client,\nservers, workstations, data centers, High performance computing, automotive,\nembedded systems, industrial applications, etc. PCIe technology is so\npervasive with its compelling power-efficient performance and cost\neffectiveness that other protocols including ultra-latency sensitive load-\nstore I/O protocols with memory, storage, and coherency semantics (e.g.,\nCompute Express Link \u2013 CXL\u00ae, NVM Express\u00ae) run on PCIe PHY and leverage the\nPCIe infrastructure such as upper layers, software stack and platform\nconnectivity.\n\nFigure 1: PCIe specification evolution through six generations spanning three\ndecades: Doubling Bandwidth with leading power-efficient and cost-effective\nperformance satisfying the needs across the entire compute continuum as the\nubiquitous interconnect\n\nPCI-SIG\u00ae, a consortium of about 900 member companies, owns, manages and\ndevelops PCIe technology as an open industry standard. In order to address the\ninsatiable bandwidth demand of emerging applications such as Artificial\nIntelligence, Machine Learning, networking, communication systems, storage,\nand High-Performance Computing, PCI-SIG just released the sixth generation of\nPCIe technology. The purpose of this white paper is to provide insights into\nthe technical analysis and trade-offs that were considered for PCIe 6.0\nspecification in order to deliver cost-effective, scalable and power-efficient\nperformance in a backwards compatible manner.\n\nPCIe 6.0 Requirements\n\nPCI Express is a Load-Store interconnect with challenging latency, bandwidth\nand power requirements. Several segments that deploy PCIe technology also have\nvery stringent requirements in reliability and cost. As a ubiquitous I/O, PCIe\narchitecture needs to meet these requirements across the entire compute\ncontinuum, with full backwards compatibility, as detailed in Table 1 below.\n\nTable 1: PCIe 6.0 Specification Target Requirements to satisfy the needs of\nall platforms where PCIe technology is deployed\n\nError Model with PAM4 Signaling\n\nPCIe 6.0 specification requires significant improvements to the package,\nconnectors, and the materials, as we have done with prior speed increases. In\naddition to the channel improvements, PCIe 6.0 specification uses PAM4 (Pulse\nAmplitude Modulation, 4 levels) signaling to achieve similar channel reach as\nPCIe 5.0 specification. PAM4 uses 4 voltage levels to encode 2 bits of data,\nas shown in Figure 2, while running the clock at the same 16G Nyquist\nfrequency as PCIe 5.0 specification. While PAM4 helps with the channel reach,\nit comes at the expense of much higher bit error rate (BER), which has been\n10^-12 for the first five generations of PCIe specifications. BER is the\nmeasure of the number of bit errors in the Receiver divided by the total\nnumber of bits received. With PAM4 signaling, errors are expected to occur in\nclusters, as shown in Fig. 2. When a bit error happens, the decision feedback\nequalizer (DFE) in a Receiver may induce errors in subsequent bits due to\npropagation of feedback from the prior bit error(s), resulting in burst\nerrors. While the number of errors in a burst as well as the length of the\nburst can be mitigated by techniques such as proper selection of PHY\nequalization methods, gray-coding of the 4-levels of PAM4, and precoding, the\nburst errors in a Lane need to be addressed. Other forms of correlation across\nLanes in a Link are also possible due to common noise sources such as power\nsupply noise.\n\nFigure 2: PAM4 signaling and burst error in a Lane due to DFE as well as\ncorrelated errors across Lanes due to common noise sources\n\nError Handling with PCIe 6.0 Architecture at 64.0 GT/s\n\nWith PAM4 encoding, the bit error rate (BER) at 64.0 GT/s is expected to be\nseveral orders of magnitude worse than the BER of 10^-12 that existed in the\npast 5 generations of PCIe specifications. Even though PCIe architecture has a\nLink Level retry (LLR) mechanism to replay transactions impacted by error(s),\nthe loss of bandwidth due to retry makes it untenable with PAM4 error rates.\nThus, PCIe 6.0 specification uses a light-weight FEC in conjunction with the\nLLR mechanism to meet the performance metrics of low latency and low bandwidth\noverhead delineated in Table 1.\n\nFigure 3: Retry Probability vs. FBER for a 256B transfer for a x1 Link,\nassuming each FBER instance is assumed to be a Symbol that can be corrected by\nthe FEC. This demonstrates that a single FBER correct FEC with an FBER of\n10^-6 combined with a low-latency replay mechanism is the best solution to\nmeet the stringent latency requirements of PCIe applications. Even a two FBER\ncorrect FEC would have resulted in a latency increase in the range of tens of\nnsecs.\n\nPCIe 6.0 specification defines a parameter called `FBER\u2019 (First Bit Error\nRate). It is the probability of a (first) bit error occurring at the Receiver\npin. An FBER occurrence may cause other correlated errors, as shown in Figure\n2b. All those individual correlated errors count as one from an FBER count\nperspective. We have extensively studied the trade-offs associated between\nFBER along with error correlation on FIT, retry probability, and bandwidth\nloss due to retry, for different FEC capabilities. A simple study for a x1\nLink is shown in Figure 3 as an illustration. As expected, the retry\nprobability decreases with the number of FBER instances that can be corrected\nwith an 256-byte payload. However, retry probability decreases exponentially\nwith the decrease in FBER. A stronger FEC results in higher latency as the\ncomplexity increases exponentially with the number of Symbols corrected. On\nthe other hand, a lower FBER results in reduced channel reach. We chose a\n3-way interleaved single Symbol correct FEC (as shown later in Figure 4) to\ncorrect a single FBER instance with an FBER of 10^-6 in order to have an\nacceptable retry probability in the 10^-5 range. This light-weight causes the\nFEC correction latency to be less than 2ns. The lower FBER of 10^-6 (vs the\n10^-4 in networking standards) does result in shorter channel reach by about\n2-4 inches. However, in spite of this channel length trade-off, we are still\nwithin the channel reach of existing channels. Further, over time, we have\nseen constant improvements in packages, materials, and connectors, resulting\nin longer channel reach whereas the latency impact due to a stronger FEC does\nnot change over time. Thus, with our choice of light-weight FEC and a slightly\nstringent FBER, we have made the right trade-offs to stay flat on latency\nwhile meeting the existing channel reach requirements backed by rigorous\nanalysis and silicon data.\n\nOur studies also demonstrated the need for a strong CRC to achieve the\nreliability goals of a very low FIT due to counter the high rate of errors\nwith correlation. We chose a 64-bit CRC to protect every 256 bytes. Our CRC is\nbased on the Reed-solomon code with g(x) = (x+\u03b1)(x+\u03b1^2)... (x+\u03b1^8), where \u03b1 is\nthe root of a primitive polynomial over GF(2^8). This offers a very robust\nerror detection mechanism since it is guaranteed to detect up to 8 Bytes in\nerror (post-FEC) and has an aliasing probability of 2^-64 beyond that. The CRC\ncomplexity increases logarithmically with the number of bits since we are only\nchecking if a syndrome is zero. Thus, the increased number of CRC bits has a\nnegligible impact on the gate count or the latency. Thus, the light-weight FEC\ndelivers a received packet with an error probability of about 10^-5 whereas\nthe strong CRC practically guarantees that an erroneous packet will be\ndetected (with a FIT close to 0, as shown in Table 2) and a link layer replay\nwill cause the packet to be retransmitted all within very tight low-latency\nconstraints.\n\nFlit Mode in PCIe 6.0 Specification\n\nWe need a fixed sized transfer entity (we call it a Flit which stands for flow\ncontrol unit) that the FEC encode/ decode/ correction can be applied to. With\na fixed Flit size, it is natural to have the CRC-based error detection as well\nas ack/nak and retry mechanism to operate at the Flit level.\n\nA Flit can have multiple packets (both Data Link Layer Packet \u2013 DLLP and\nTransaction Layer Packet- TLP) and a TLP/DLLP may span across multiple Flits\n(as shown in Figure 4). Since the Flit includes the CRC, the DLLP and TLP do\nnot carry their individual CRC bytes as they did in prior generations. We have\nalso removed the PHY layer Framing Token for every TLP or DLLP, due to the\nfixed position of TLP/ DLLPs within a Flit. The fixed DLLP bytes helps with a\npredictable low-latency replay mechanism and reduces queuing overheads. These\nmechanisms help boost link efficiency to overcome the FEC and CRC overhead.\n\nA Flit comprises of 256 bytes. Each byte is sent on one Lane and bytes are\ninterleaved across the width of the Link, consistent with the layout in prior\ngenerations. The first 236 bytes of the flit are for TLP(s), followed by 6\nbytes for Data Link Layer Payload (DLP), followed by 8 bytes for CRC covering\nthe first 242 bytes of TLP and DLP, followed by 6 bytes of FEC, covering the\nentire Flit.\n\nFigure 4: TLP arrangement in a Flit in a x16 Link\n\nThree single Symbol correct capable FEC groups are interleaved across\nconsecutive bytes, as shown using different color combinations in Figure 4.\nEach color is a different FEC group, comprising of 85B, 85B, and 84B\nrespectively. Due to this 3-way FEC interleaving, no burst error can affect\ntwo Symbols of the same FEC group, as long as the burst length is <= 16. Thus,\none occurrence of FBER in a flit can be corrected if the burst length is <=16\nand we get no correlation errors across Lanes. PCIe 6.0 specification ensures\nthat the burst length > 16 occurs with a probability less than FBER by\nconstraining the DFE (Decision Feedback Equalizer) tap weights and balancing\nthe Transmitter Equalization (TxEQ), the CTLE (Continuous Time Linear\nEqualization) and DFE equalization parameters. Other techniques such as gray\ncoding and precoding have been deployed to mitigate the effect of correlated\nerrors. Extensive analysis has been deployed to ensure that this requirement\ncan be met across the wide range of channels across different platforms where\nPCIe technology is deployed.\n\nPerformance and Reliability Results\n\nWe present the various metrics in this section. Detailed mathematical analysis\nof the results presented here can be found in our 2021 IEEE Hot Interconnects\npaper.\n\nTable 2 demonstrates that the retry probability and bandwidth loss is within\nexpectation. Unlike prior generations, Flit retry will be common with 64.0\nGT/s data rate, although their impact on bandwidth will be minimal.\nFurthermore, with the expected retry time of about 100ns, we do not expect to\nsee much jitter since it is a fraction of the total access time and the\nqueuing delays in a loaded system will far exceed that additional latency in\ncase of a retry.\n\nTable 2: Retry Probability, B/W loss, and FIT as a function of FBER and Retry\ntime based on the FEC/ CRC for a x16 Link. The 10^-5 FBER column is provided\nto demonstrate that it is not a viable solution point with a bandwidth loss of\nabout 5%. Retries will be common with PCIe 6.0 architecture but with the low\nretry time, the performance impact will not be noticeable.\n\nFBER/ Retry Time| 10^-6/ 100ns| 10^-6/ 200ns| 10^-6/ 300ns| 10^-5/ 200ns  \n---|---|---|---|---  \nRetry probability per flit| 5 x 10^-6| 5 x 10^-6| 5 x 10^-6| 0.048  \nB/W loss with go-back-n (%)| 0.025| 0.05| 0.075| 4.8  \nFIT| 4 x 10^-7| 4 x 10^-7| 4 x 10^-7| 4 x 10^-4  \n  \nFigure 5 shows the bandwidth scaling of Flit mode at 64.0 GT/s vs the\n128b/130b encoding at 32.0 GT/s. The packet efficiency of Flit mode exceeds\nthat of the 128b/130b encoding for payloads up to 512 Bytes (32 DWs),\nresulting in an up to ~3X improvement in effective throughput for smaller\npayloads (2X from data rate increase and ~1.5X improvement in bandwidth\nefficiency). As the TLP size increases, this efficiency goes down and for the\n4KB data payload size, it reduces to 0.98, in line with the bandwidth\ninefficiency provided in the metrics of Table 1.\n\nFigure 5: Bandwidth scaling with Flit Mode at 64.0 GT/s over 128b/130b\nencoding at 32.0 GT/s. For practical systems, we expect to see more than 2X\nuseable bandwidth with PCIe 6.0 specification at 64.0 GT/s over the 32.0 GT/s\nwith PCIe 5.0 specification\n\nThe measured latency in the Flit Mode is lower at 64.0 GT/s than the prior\ngenerations except for lower link widths (x1/ x2) and TLPs with small payload\nsizes (<= 16 B). The higher data rate results in faster transmission time of a\npacket. Removal of PHY encoding mechanisms such as Framing Token and Sync\nheader further reduces the latency. For example, a TLP with a 256-B payload\nwill see a latency reduction greater than 1ns. Even for a x1 Link transmitting\na 3DW TLP, the increase in latency is expected to be less than 10ns, meeting\nthe requirements of Table 1.\n\nConclusions\n\nWe have demonstrated that PCIe 6.0 specification meets or exceeds all the\nrequirements of Table 1. An interconnect technology is considered successful\nif it can sustain three generations of bandwidth improvement spanning a\ndecade. PCIe architecture has far exceeded that mark. As we embark on the\nthird decade, PCIe technology is looking strong as the ubiquitous I/O\ninterconnect over the entire compute continuum. As the industry continues its\njourney to the sixth generation of PCIe specification, it will benefit from\nthe power-efficient performance it will deliver in a High Volume Manufacturing\n(HVM) environment with hundreds of Lanes connecting multiple devices in a\nplatform.\n\nAbout PCI-SIG\n\nPCI-SIG is the consortium that owns and manages PCI specifications as open\nindustry standards. The organization defines industry standard I/O\n(input/output) specifications consistent with the needs of its members.\nCurrently, PCI-SIG is comprised of about 900 industry-leading member\ncompanies. To join PCI-SIG, and for a list of the Board of Directors, visit\nwww.pcisig.com.\n\nPCI-SIG, PCI Express, and PCIe are trademarks or registered trademarks of PCI-\nSIG. All other trademarks are the property of their respective owners.\n\nCopyright \u00a9 2024 PCI-SIG. All rights reserved. View our privacy policy.\nContact Us.\n\nCopyright \u00a9 2024, . Theme by Devsaran.\n\n", "frontpage": false}
