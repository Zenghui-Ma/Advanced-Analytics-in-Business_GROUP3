{"aid": "40074831", "title": "Show HN: Balter, the Open Source, composable, distributed load testing framework", "url": "https://www.balterloadtesting.com/", "domain": "balterloadtesting.com", "votes": 1, "user": "wastib", "posted_at": "2024-04-18 10:46:36", "comments": 0, "source_title": "Balter Load Testing", "source_text": "Balter Load Testing\n\nBalter\n\n# Balter\n\n### The composable distributed load testing framework.\n\n### Open Source under the MIT License.\n\nStar Fork\n\n    \n    \n    use balter::prelude::*; #[tokio::main] async fn main() { tokio::join! { async { // First, set up a background load which either hits // 10K TPS, has a p95 latency of 200ms or has an // error rate of 5% set_background_load() .tps(10_000) .latency(Duration::from_millis(200), 0.95) .error_rate(0.05) .await; }, async { // After 300s of waiting, test our scaling ability // by running a scenario which achieves either // 100K TPS or a p90 latency of 1,000ms sleep(Duration::from_secs(300)).await; test_scaling_functionality() .tps(100_000) .latency(Duration::from_millis(1_000), 0.90) .duration(Duration::from_secs(3600)) .await; }, } }\n\nExplore More \u21e9\n\nBalter\n\nHow It Works\n\nComposability\n\nNative Metrics\n\nDistributed Runtime\n\nSupport\n\n# Balter\n\nBalter, A Load TestER, is a distributed load testing framework designed to\nmake testing high-traffic scenarios easy. Load test scenarios are written\nusing standard Rust code with two special attributes thrown in. Balter makes\nno assumptions about the service under test, and can be used for a variety of\nuse-cases, from HTTP services to local key-value stores written in any\nlanguage.\n\nHigh level features include:\n\n  * Flexible and composable testing via Scenario and Transaction abstractions.\n  * Constrain load tests with TPS, latency or error rate (including all three at once).\n  * Distributed runtime in just a few lines of code.\n  * Native metrics integration.\n  * Written with efficiency in mind. Don't break the bank with load testing.\n\nBalter is a new project and still has some rough edges. The project is being\nworked on full time, so if you run into any issues please let us know on\nGithub and we will try to resolve them as quickly as possible.\n\n# How It Works\n\nThe two abstractions Balter provides are the Scenario and the Transaction. A\nScenario is some characteristic load you want to run, such as an average user\nwork-flow, and it must call one or more Transactions (directly or indirectly).\nThe Scenario is the test, and the Transaction is how Balter keeps track of\nwhats going on.\n\nTo perform a load test, Balter creates many instances of a Scenario and runs\nthem in parallel. It then keeps track of statistical information around the\nTransactions, and is able to rate-limit outgoing transactions, increase\nconcurrency, distribute the work to other machines, etc.\n\nBoth a Scenario and a Transaction in Balter are async functions with an\nattribute on them, #[scenario] and #[transaction] respectively. See the guide\nfor type constraints that are currently required. An example of a Scenario\nwhich calls two Transactions is below:\n\n    \n    \n    #[scenario] async fn test_scaling_functionality() { let client = reqwest::Client::new(); loop { foo_transaction(&client).await; for _ in 0..10 { bar_transaction(&client).await; } } } #[transaction] async fn foo_transaction(client: &Client) -> Result<()> { client.post(\"https://example.com/api/foo\") .json(...) .send().await?; Ok(()) } #[transaction] async fn bar_transaction(client: &Client) -> Result<()> { client.post(\"https://example.com/api/bar\") .json(...) .send().await?; Ok(()) }\n\nA Scenario supercharges a Rust function with additional methods related to\nload testing. The simplest to understand is the .tps() method, which will run\nthe function in parallel and constrain the rate of transactions such that the\ntransactions per second (TPS) is equal to the value you set:\n\n    \n    \n    test_scaling_functionality() .tps(10_000) .await;\n\nBalter currently provides the following methods for a Scenario:\n\n  * .tps(u32) Run a Scenario such that the transactions per second is equal to the value set.\n  * .error_rate(f64) Constrain transaction rate to an average error rate.\n  * .latency(Duration, f64) Constrain transaction rate to a specific latency at a given percentile.\n  * .duration(Duration) Limit the Scenario to run for a given Duration (by default it runs indefinitely)\n\nThese methods can be used together. For example, let's say you want to scale a\nfunction to achieve a p90 latency of 200ms, but not go over 10,000 TPS or an\nerror rate of 3%, and run it for 3600s:\n\n    \n    \n    test_scaling_functionality() .latency(Duration::from_millis(200), 0.90) .tps(10_000) .error_rate(0.03) .duration(Duration::from_secs(3600)) .await;\n\nSee the guide for more information on the core primitives Balter provides and\ncurrent restrictions they have.\n\n# Composability\n\nWhat sets Balter apart from other load testing frameworks like JMeter or\nLocust is composability. Scenarios are normal async Rust functions, and this\nopens up a world of flexibility.\n\nFor example, you can call Scenarios one after another if you want to run a set\nof load tests:\n\n    \n    \n    test_normal_user_load() .tps(10_000) .error_rate(0.03) .duration(Duration::from_secs(3600)) .await; sleep(Duration::from_secs(3600)).await; test_edge_cases() .latency(Duration::from_millis(100), 0.99) .duration(Duration::from_secs(3600)) .await;\n\nWhere things get interesting is the ability to run Scenarios in parallel,\nusing the standard Tokio join! macro. For instance, being able to set up a\nbaseline amount of load against your service, and then slamming it with high\ntraffic for a few minutes, is made simple with Balter:\n\n    \n    \n    tokio::join! { async { // First, set up a background load which either hits // 10K TPS, has a p95 latency of 200ms or has an // error rate of 5% set_background_load() .tps(10_000) .latency(Duration::from_millis(200), 0.95) .error_rate(0.05) .await; }, async { // After 300s of waiting, test our scaling ability // by running a scenario which achieves either // 100K TPS or a p90 latency of 1,000ms sleep(Duration::from_secs(300)).await; test_scaling_functionality() .tps(100_000) .latency(Duration::from_millis(1_000), 0.90) .duration(Duration::from_secs(3600)) .await; }, }\n\nOf course, you aren't limited to just running Balter Scenarios. For example,\nyou can make API calls to disable certain services while a load test is\nrunning. The possibilities are endless! Balter aims to provide the minimal\nabstraction overhead to answer all high-load questions about your service.\n\n# Native Metrics\n\nMetrics are an important part of understanding load performance, and Balter\nnatively supports metrics via the metrics crate. This means you can plug in\nany adapter you need to output metrics in a way that integrates with your\nsystem. For instance, Prometheus integration is as easy as adding the\nfollowing:\n\n    \n    \n    PrometheusBuilder::new() .with_http_listener(\"0.0.0.0:8002\".parse::<SocketAddr>()?) .install()?;\n\nThe metrics output by Balter include statistical information on TPS, latency,\nerror-rates as well as information on the inner workings of Balter (such as\nthe concurrency and controller states).\n\nSee the guide for more information.\n\n# Distributed Runtime\n\nBalter provides a distributed runtime if your load testing requirements are\nhigher than what a single machine can handle. This runtime is currently in an\nexperimental state, though stabilization is a high priority for the near\nfuture. Complete documentation can be found in the guide.\n\nCurrently, the runtime just needs a port and at least a single peer (in order\nto start gossiping with). Then, rather than calling a Scenario from your\nmain() function, you instantiate the Runtime, which will automatically hook\ninto the various Scenario's you have.\n\n    \n    \n    #[tokio::main] async fn main() -> Result<()> { BalterRuntime::new() .port(7621) .peers(&[\"192.168.0.1\".parse()?]) .run() .await; }\n\nIn the background, the Balter runtime will start gossiping with peers and\nsharing work. In order to kick off a Scenario, you simply send an HTTP request\nto any Balter server, and everything else will be handled automatically. The\nguide covers more information on the distributed runtime, and the caveats that\ncurrently exist.\n\n# Support\n\nBalter is a brand new project, and any support is greatly appreciated!\n\nThe easiest way to support Balter is by giving us a star on Github. This helps\npeople find us and learn about the project. Star\n\nThe best way to financially support Balter's development is to hire us! We\nprovide contracting for building out your team's load testing infrastructure\nusing Balter. You will have full access and ownership over the source code,\nusing all Open Source technology, and have complete confidence in the\nperformance and load characteristics of your system. At the same time, you\nwill be supporting the project and giving us insights into how to improve the\nframework.\n\nIf you are interested in hiring us, please reach out to\nconsulting@balterloadtesting.com\n\nAdditionally, you can sponsor our developers on Github, which is a great way\nto show support, especially if there are features you're interested in seeing\nadded!\n\ncontact@balterloadtesting.com BalterLoadTesting.com \u00a9 2024\n\n", "frontpage": false}
