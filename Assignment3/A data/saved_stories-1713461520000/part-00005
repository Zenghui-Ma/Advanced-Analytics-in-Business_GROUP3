{"aid": "40073914", "title": "EagleX v2: Soaring past LLaMA2 7B in both English and Multi-lang evals (RWKV-v5)", "url": "https://blog.rwkv.com/p/eaglex-v2-soaring-past-llama2-7b", "domain": "rwkv.com", "votes": 1, "user": "oakpond", "posted_at": "2024-04-18 07:43:51", "comments": 0, "source_title": "\ud83e\udd85 EagleX v2 : Soaring past LLaMA2 7B in both English and Multi-lang evals (RWKV-v5)", "source_text": "\ud83e\udd85 EagleX v2 : Soaring past LLaMA2 7B in both English and Multi-lang evals\n(RWKV-v5)\n\n# RWKV Open Source Development Blog\n\nShare this post\n\n#### \ud83e\udd85 EagleX v2 : Soaring past LLaMA2 7B in both English and Multi-lang evals\n(RWKV-v5)\n\nblog.rwkv.com\n\n#### Discover more from RWKV Open Source Development Blog\n\nDevelopment blog for the RWKV open source architecture, and their derivative\nOSS models\n\nContinue reading\n\nSign in\n\n# \ud83e\udd85 EagleX v2 : Soaring past LLaMA2 7B in both English and Multi-lang evals\n(RWKV-v5)\n\n### You have seen the teaser with the EagleX 1.7T, now its here - the\ndefinitive version of linear transformer trained past, LLaMA 2 7B.\n\nRWKV\n\nApr 18, 2024\n\nShare this post\n\n#### \ud83e\udd85 EagleX v2 : Soaring past LLaMA2 7B in both English and Multi-lang evals\n(RWKV-v5)\n\nblog.rwkv.com\n\nShare\n\n#\n\nEagleX v2 - in short\n\nWe extended the training of the previous Eagle 7B from 1.1 Trillion tokens to\n2.25 Trillion tokens.\n\n  * A continuation based on the original Eagle 7B model, and the EagleX 1.7T model\n\n  * Ranks as the world\u2019s greenest 7B model (per token)\n\n  * Trained on 2.25 Trillion tokens across 100+ languages\n\n  * Outperforms all 7B class models in multi-lingual benchmarks\n\n  * Passes LLaMA2 (2T) in multiple English evals, approaches Mistral (>2T?)\n\n  * All while being an \u201cAttention-Free Transformer\u201d\n\nWe are releasing RWKV-v5 Eagle v2, licensed under Apache 2.0, which can be\nused personally or commercially without restrictions.\n\n  * Download from HuggingFace\n\n  * Try it online today on: recursal.ai cloud platform\n\n  * Try on: our HF gradio demo\n\n  * Use our reference pip inference package, or any other community inference options (Desktop App, RWKV.cpp, etc), and use it anywhere (even locally)\n\n  * Fine-tune using our Infctx trainer\n\n  * [Pending PR] Get support merged into Huggingface transformers!\n\n  * All eval data can be found in the google sheet here\n\n#\n\nBuilding on bold claims\n\nThe original EagleX 7B 1.7T, trained by Recursal AI, made history as the first\nsub-quadratic model, to pass llama2 7B 2T on average in English eval.\n\nToday we are releasing the 2.25T trained variant, which furthers the gap with\nthe llama2 model.\n\n> The following report follows the same general format of the 1.7T model\n> release, in eval details - to make direct comparision easier.\n\n##\n\nWinning English Perplexity\n\nWe start with the basics: Perplexity. Which is the loss value against the test\ndataset (lower score = better), i.e. how good the model is with the next token\nprediction.\n\nIn a major first, the EagleX model - now passes mistral in perplexity. And\ntakes the lead in the 7B model weight class.\n\nWhy do experts care about perplexity? Eval in general can be very subjective,\nand opinion-driven, and commonly give mixed results. Perplexity in a way gives\nthe TLDR summary for most experts to start with\n\n#\n\nLeading Multi-lang Perplexity & evals\n\nEagleX maintains the lead for best-in-class multi-lingual performance, with\nthe incremental improvements we\u2019re making to the Eagle line of models.\n\nMost of the tasks here are common sense reasoning tests of a wide variety of\nformats, across languages including 23 of the world\u2019s most widely used\nlanguages.\n\nFor the remaining languages, we urge the community to test and judge them\nthemselves, over 100+ languages were trained. Over time, we would want more\nlanguages to be added to evals.\n\nWhy is multi-lingual perf important?\n\nThe goal of the RWKV project & Eagle line of models is to build inclusive AI\nfor everyone regardless of their language. Our mission is to build AI models\nnot just made for English, but also for the 83% of the world\u2019s population\nusing a non-English language everyday.\n\n#\n\nGoing big on eval data\n\nAs per the previous 1.7T model, we ran ALL the benchmarks in EleutherAI `lm-\neval-harness`, at commit `f78e2da`, with the following limitations:\n\n  * It has to be completed in under 30 minutes on 8x4090 (we were running lots of evals)\n\n    * This rules out some of the rather more expensive long chain of thought evals\n\n  * We excluded all the personality/alignment evals\n\n  * Eval has to be executable across a wide variety of models, via lm-eval-harness\n\n  * All evals are 0 shot (no 5 shot-ing an MCQ question)\n\n  * We limited comparison to other models within the 7B weight class\n\nThese resulted in running 60+ major eval groups, which generated over 1,000+\ndata points per model. A data point count so high, that we had to drop\nstandard error deviations, just to ensure the raw CSV file can be loaded in\nMacOS numbers.\n\n#\n\n21 English Evals\n\nHowever, because 180+ evals are overwhelming, let\u2019s first reduce down to 21 of\nthe arguably most popular English evals, such as Lambada, Glue, Swag,\nWinogrande, TruthfulQA, MMLU:\n\nNarrowing it down to the models that most of us actually care about - LLaMA,\nMistral, EagleX, and Eagle-7b - the new EagleX v2 model outperforms LLaMA-2-7b\non average across the 21 evals, and lags not far behind Mistral.\n\nKeep in mind that this average shown is across all 21 evals\n\n####\n\nThe Good\n\nNow, let\u2019s look at where our model is blowing the rest of the models out of\nthe water.\n\nFirst, the big stand out is the following 5 evals, in which both our 1.7T and\n2.25T models beat even mistral 2T++ trained model (glue, anli, mmnli, swag),\nacross multiple tasks focused around either contextual-based simple Q&A with\ncommon sense reasoning, or deductive logic.\n\n> PS: The jump for glue/mnli was high enough, that we needed to check the\n> dataset specifically for contamination. Which we were not be able to find\n> any. This jump is currently being attributed to multiple training datasets,\n> along with data augmented / machine rewritten instruct dataset following a\n> similar structure.\n\nEagleX 2.25T, also performs better than LLaMA-2-7b in, lambada next token\nprediction and more importantly ...\n\nwinograde, wnli, truthfulqa evals, which imply that the EagleX model would be\napplicable in RAG use cases, which are mainly contextual Q&A, with the right\nprompt engineering.\n\nStrong common sense reasoning over context, has very strong applicable use\ncases for multiple RAG use cases\n\n####\n\nThe Mixed\n\nNext: the eval sets with mixed results.\n\nFor logiqa, we have very similar evals with 2 major variants. The results\nbetween EagleX and LLaMA are close enough, that it\u2019s hard to say which model\nis clearly better between the two for these evals.\n\nSimilarly, sciq got slightly worse between 1.7T to 2.25T, but in general, all\nmodels are within trading blows of each other at 90%+ scoring.\n\n####\n\nThe \u201cNot too bad\u201c and the \u201cReally Bad\u201d\n\nThese are the evals the EagleX model performs worse when compared to both\nMistral and LLaMA. However, for the evals that we\u2019ve lost to LLaMA, it\u2019s\ntypically by a narrow margin.\n\nOne major error that occurred in the 1.7T model, was the accidental exclusion\nof the math dataset, which caused a degradation of math performance.\n\nSince then, we have added back math text and math materials. Which boosted the\narithmetic score. However, given the number of tokens between 1.7T and 2.25T,\nand the learning rate, the increase in math score was limited.\n\n> Our recommendation still stands that realistically IMO - no one should be\n> depending on a 7B model for math (just saying)\n\n##\n\n180 English Evals\n\nAs per the previous 1.7T release. let\u2019s zoom out, and look at it holistically\nacross 180 English evals.\n\nYou can view the full results here\n\nAlthough using the overall averages across all the evals does have a bias on\nthe results towards larger eval sets (due to double counting, e.g. mmlu\noverall and many individual mmlu test), it does not change the ranking among\nthe EagleX, Mistral, LLaMA and the original Eagle models.\n\nHowever, these results are useful for smaller insights (as per the previous\nmodel as well). Such as measuring specifically gaps in knowledge by \u201csubject\ndomain\u201d within the models.\n\n#\n\nPerhaps a good dataset + Scalable architecture: is all you need?\n\nThe RWKV Open Source Foundation's goal is to ensure AI access is made\naccessible to everyone in the world, regardless of language or economic\nstatus.\n\nIn line with our goal, it does repeat the question. If the exact architecture,\nmatter less than the data for the model performance?\n\nCUDA computational time, for RWKV-based architecture -vs- transformer\narchitecture: that quadratic-vs-linear really scales!\n\nIf true, perhaps we should seek more efficient and scalable architecture, to\nincrease accessibility for everyone regardless of language or economic status.\n\nAll while going beyond the English language, which represents only 17% of the\nglobal population.\n\nAnd reducing the impact on our environment.\n\n#\n\nWhat\u2019s next for the RWKV group?\n\nThis release marks the final release of the Eagle line of RWKV models. With\nthe finalization of the v6 architecture as outlined in the paper here -\nhttps://arxiv.org/abs/2404.05892\n\nOur next step is to move onto the v6 Finch line of architecture, which we\nexpect to bring an incremental improvement on the v5 Eagle architecture.\n\nThis is made in consideration, that upcycling from Eagle to Finch line of\nmodels works from our existing experiments.\n\nRoadmap\n\n  * v6 Finch: 0.1B, 1.6B, 3B model release\n\n  * v6 Finch: 7B, 14B training, this would be an upcycle of the Eagle models\n\n  * MoE: (approximately) 8 x 22B\n\nBasically newer, better, bigger models - as we keep iterating on our goal to\nbuild a multi-lingual GPT4 class model, in open-source space, that can run on\ncommodity hardware.\n\nAnd ensure AI is accessible to everyone, regardless of language, or economic\nstatus.\n\n#\n\nAcknowledgment\n\nWe are grateful and would like to thank the following key groups:\n\n  * Recursal.ai team for financing the GPU resources, and managing the training of this foundation model - you can run the Eagle line of RWKV models on their cloud / on-premise platform today.\n\n  * EleutherAI for their support, especially in the v5/v6 Eagle/Finch paper\n\n  * Linux Foundation AI & Data group for supporting and hosting the RWKV project\n\nAlong with the various developers, working on the growing collection of RWKV-\nrelated projects.\n\nThanks for reading RWKV Open Source Development Blog! Subscribe for free to\nreceive new posts and support my work.\n\nShare this post\n\n#### \ud83e\udd85 EagleX v2 : Soaring past LLaMA2 7B in both English and Multi-lang evals\n(RWKV-v5)\n\nblog.rwkv.com\n\nShare\n\nComments\n\nReady for more?\n\n\u00a9 2024 RWKV\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": false}
