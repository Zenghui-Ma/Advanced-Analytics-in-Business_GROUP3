{"aid": "40133899", "title": "Multi-tenant apps with single-tenant SQLite databases in global Tigris buckets", "url": "https://fly.io/javascript-journal/single-tenant-sqlite-in-tigris/", "domain": "fly.io", "votes": 1, "user": "avinassh", "posted_at": "2024-04-23 16:37:10", "comments": 0, "source_title": "Multi-tenant apps with single-tenant SQLite databases in global Tigris buckets", "source_text": "Multi-tenant apps with single-tenant SQLite databases in global Tigris buckets\n\u00b7 The JavaScript Journal\n\nReading time \u2022 18 min Share this post on Twitter Share this post on Hacker\nNews Share this post on Reddit\n\n# Multi-tenant apps with single-tenant SQLite databases in global Tigris\nbuckets\n\nAuthor\n\n    \n\nName\n\n    Szymon Mentel\nImage by Annie Ruygt\n\nWe\u2019re Fly.io, where apps run on Fly Machines\u2014fast-starting VMs with a simple\nAPI\u2014in 30+ regions around the world. This article is about Fly Machines\nstoring their SQLite databases in a globally distributed Tigris bucket. If you\nhaven\u2019t tried Machines, take them for a spin! It\u2019s fast to get started.\n\nI got nerd-sniped by one of the ideas Jason brainstormed in his Phoenix Files\narticle about Tigris, an S3-compatible globally synced object store. Tigris\nis, not coincidentally, built on Fly.io. (If you haven\u2019t read Jason\u2019s article,\ngo read it! You\u2019ll be fine! There\u2019s hardly any Elixir in it.)\n\nHere\u2019s the idea I\u2019m here to explore: multi-tenant applications with single-\ntenant SQLite databases stored in Tigris and run on single-tenant Fly\nMachines:\n\nThe multi-tenant architecture with single-tenant SQLite databases.  \n---  \n  \nIf we\u2019re to handle a tenant request within this architecture, we\u2019d route the\nrequest to its Machine, ensure its database is there, serve the request,\nupdate database file in the bucket... and potentially auto-stop the Machine if\nthat was just a one-off request! (And yes, we can also wake it up if another\nrequest comes in.)\n\nThe sequence diagram  \n---  \n  \nTo learn more about Tigris, go here. To learn about Fly Machines, go here.\n\nHowever, there\u2019s a caveat to this approach: if we allow more than one Machine\nconcurrently access the tenants\u2019 SQLite database, the Machine updating the\ndatabase file in the bucket last would overwrite changes done by the other\nones. Also, these Machines would work on snapshots of the data without\nknowledge of the changes applied in parallel.\n\nThat\u2019s enough rambling about theory! Let\u2019s bring this idea to life. In this\npost, we\u2019ll build a Vanilla JavaScript app that keeps a counter in a SQLite\ndatabase and increments it every time a user visits a web page. It\u2019ll fetch\nthe database file from Tigris on the app start-up and send it back on\nshutdown. At the end, we\u2019ll make the app customizable so that an instance can\nbe run per tenant and deploy a multi-tenant setup at Fly.io.\n\nActually, I\u2019ve already build the app at: https://github.com/fly-apps/js-\nsqlite-in-tigris... let\u2019s go through it step by step together!\n\nIf you want to run the examples, make sure you\u2019ve got a Fly.io account.\n\n## Tigris setup\n\nIssue fly storage create to create a Tigris project. This will output a bunch\nof environment variables that have to be available in the shell to run the\nfollowing examples. The environment variables are important. If they\u2019re not\nset, the following examples won\u2019t work.\n\nThe simplest way to set them up is to issue a set of export statements in your\nshell:\n\n    \n    \n    export AWS_ACCESS_KEY_ID=<you-access-key-id> export AWS_ENDPOINT_URL_S3=https://fly.storage.tigris.dev export AWS_REGION=auto export AWS_SECRET_ACCESS_KEY=<your-secret-access-key> export BUCKET_NAME=<your-bucket-name>\n\nTo keep the variables handy, install a tool like dotenv and put the exports\ninto a .envrc file. dotenv will make them available for the shell commands run\ninside the current directory.\n\nGet the feel of Tigris with the AWS CLI:\n\n    \n    \n    aws s3api list-buckets # you should see the bucket you've created on setup aws s3api put-object --bucket $BUCKET_NAME --key mykey --body sampe-file # put a file into a bucket aws s3api list-objects-v2 --bucket $BUCKET_NAME # list objects in the bucket aws s3api get-object --bucket $BUCKET_NAME --key mykey sample-file # download the file fly storage dashboard $BUCKET_NAME # see the bucket in the Tigris console\n\nNow, let\u2019s switch gears and focus on the JS app and SQLite for a bit.\n\n## Node.js app with SQLite\n\nThe code corresponding with this step is available at 7303da9.\n\nIt\u2019s pretty easy to start building with our Node generator. You can install it\nwith npx --yes @flydotio/node-demo@latest (more on it here).\n\nWith that installed, if you run npx node-demo --esm --ejs --express --sqlite3\nand make a few tweaks for our use case, you\u2019ll get an example Vanilla\nJavaScript app running Express server with SQLite. To test it locally, start\nthe server: npm install & npm run start\n\nIf you visit http://localhost:3000/ and start refreshing the page, you\u2019ll see\na counter incrementing. If you restart the app, the counter should pick up\nwhere it left off - we do persist our state to the database, great!\n\nNothing fancy here code-wise:\n\n  * we create and set up an SQLite database:\n\n    \n    \n    // server.js process.env.DATABASE_PATH ||= './db.sqlite3' const db = new sqlite3.Database(process.env.DATABASE_PATH) ... db.run('CREATE TABLE IF NOT EXISTS \"welcome\" ( \"count\" INTEGER )')\n\n  * configure the Express server to listen at port 3000:\n\n    \n    \n    // server.js const app = express() ... // set up static content and ejs views app.use(express.static('public')) app.set('view engine', 'ejs') ... app.listen(3000, () => { ... })\n\n  * and expose a page to increment the counter in the database:\n\n    \n    \n    // server.js app.get('/', async(_request, response) => { // increment count, creating table row if necessary await new Promise((resolve, reject) => { db.get('SELECT \"count\" from \"welcome\"', (err, row) => { let query = 'UPDATE \"welcome\" SET \"count\" = ?' if (err) { reject(err) return } else if (row) { count = row.count + 1 } else { count = 1 query = 'INSERT INTO \"welcome\" VALUES(?)' } db.run(query, [count], err => { err ? reject(err) : resolve() }) }) }) // render HTML response response.render('index', { count }); })\n\nAnd that\u2019s the essence of the functionality our app will be providing to our\nusers. Let\u2019s see how we can store the database file in Tigris.\n\n## Store the tenant database file in Tigris!\n\nThe code corresponding with this step is available at d8496f6.\n\nIf you played with Tigris in one of the previous sections, you probably know\nwhat we\u2019re going to do now: just grab the db.sqlite3 file and store it in our\nbucket! Not only that, but we\u2019ll also parametrize the app so that it knows\nwhat tenant it is supposed to serve.\n\nTo make the latter work, we make the app read the CUSTOMER_ID environment\nvariable and parametrize the route path with it:\n\n    \n    \n    // server.js const customerId = process.env.CUSTOMER_ID || 0 ... app.get(`/customers/${customerId}`, async (request, response) => { // the counter logic }) app.get(`/customers/:customerId`, async (request, response) => { response.status(403).send(); })\n\nIf we get a request to a CUSTOMER_ID that we\u2019re not serving, 403 will be\nreturned.\n\nTo talk to Tigris, we\u2019ll use the @aws-sdk/client-s3 library - as with any\nother S3-compatible service. For this to work, we need to set up our S3 client\nto point at the Tigris endpoint:\n\n    \n    \n    // server.js const S3 = new S3Client({ region: \"auto\", endpoint: `https://fly.storage.tigris.dev`, });\n\nWe also need to read our bucket name and construct the object key under which\nwe\u2019ll store a customer\u2019s database file:\n\n    \n    \n    // server.js const bucketName = process.env.BUCKET_NAME const databaseKey = `/customer/${customerId}/db.sqlite3`;\n\nOn the server startup, we retrieve the database file from the bucket, or we\u2019ll\nuse the pre-created one:\n\n    \n    \n    // server.js const db = new sqlite3.Database(databasePath) // that file gets overwritten ... const checkDbInS3 = async () => { try { const { Body } = await S3.send(new GetObjectCommand({ Bucket: bucketName, Key: databaseKey })); fs.writeFileSync(databasePath, await Body.transformToByteArray()); } catch (error) { // error handling } }; ... checkDbInS3();\n\nOn the server shutdown, we\u2019ll be uploading the database file to the bucket:\n\n    \n    \n    const sendDbToS3 = async () => { try { const fileContent = fs.readFileSync(databasePath); await S3.send(new PutObjectCommand({ Bucket: bucketName, Key: databaseKey, Body: fileContent })); process.exit(0); } catch (error) { process.exit(1); } }; ... process.on(\"SIGINT\", sendDbToS3); process.on(\"SIGTERM\", sendDbToS3);\n\nAnd that\u2019s it for storing our tenant\u2019s database file in Tigris!\n\nLittle reminder: to be able to run the examples in this post, the environment\nvariables mentioned in the Tigris setup section must be exported in the shell\nyou run the examples in.\n\nIf you want to make your hands dirty, give it a try: npm install &&\nCUSTOMER_ID=10 npm start run\n\nJump to http://localhost:3000/customers/10 to see it in action. The server\nlogs should be telling us we fetch/store data from/in Tigris:\n\n    \n    \n    > start > node server.js run Server is listening on port 3000, serving customer_id: 10 Successfully downloaded the db file from S3. [customer_id=0] Received request for customer: 10 [customer_id=0] Received request for customer: 10 [customer_id=0] Received request for customer: 10 ^CSuccessfully sent the db file to S3. # NOTE that I'm sending Ctrl+C here\n\nYou can always fly storage dashboard $BUCKET_NAME to see the bucket on the\nTigris console.\n\nThat\u2019s great! Try to run the app as a different customer, that should start\ncounting from the beginning; remember to remove the db.sqlite3 file: rm\ndb.sqlite3 && CUSTOMER_ID=300 npm start run.\n\nOK, you\u2019ve got pretty far already... ready for a big thing?\n\n## The big thing #1: deploy to Fly.io\n\nActually...I should have said #2 since the #1 big thing was to set up Tigris\nand use it! But I guess it was easy enough not to notice! But let\u2019s reflect on\nit: your SQLite database can now be served from servers close to your users\nsince it will be automatically cached in the regions it will be accessed from.\n\nAnyhow, let\u2019s jump straight to the deployment part. If you want to follow the\neasy path, check out commit 87ab970 and:\n\n  1. Supply the app name in the fly.toml: app = '<app-name>'\n\n  2. Create the app at Fly.io: fly apps create <app-name> (an app is just a bag of resources on our platform)\n\n  3. Make the secrets available in the app\u2019s Machines (this assumes you have the $AWS_* variables exported as described in the Tigris setup section):\n    \n        fly secrets set AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID fly secrets set AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY\n\n  4. Allocate an IP address for the app: fly ips allocate-v4 --shared\n\n  5. Run a machine for a customer (single tenant):\n    \n        fly machine run . --name customer10 --port 443:3000/tcp:http:tls \\ --env CUSTOMER_ID=10 \\ --env AWS_REGION=$AWS_REGION \\ --env AWS_ENDPOINT_URL_S3=$AWS_ENDPOINT_URL_S3 \\ --env BUCKET_NAME=$BUCKET_NAME\n\n  6. Fire up logs to see as things are happening: fly logs\n\n  7. Visit the https://<app-name>.fly.dev/customers/10 and celebrate \ud83c\udf89\n\n  8. Kill the Machine with fly machine destroy --force\n\nThe slightly harder path involves generating the Dockerfile and fly.toml on\nyour own. We\u2019ll use fly launch for that and customize them a little. To the\npoint:\n\n  1. Check out the repo at commit d8496f6\n  2. Issue fly launch --build-only --no-deploy --name <app-name> and decline tweaking the settings\n  3. Clean up the generated files so that they reflect what\u2019s in the repo at this commit\n\nThen follow the easy path 3-7 steps and... Wow! We\u2019re live now.\n\nRefreshing the https://<app-name>.fly.dev/customers/10  \n---  \n  \nCouldn\u2019t we just go multi-tenant by spinning up multiple machines for our\ncustomers and call it a day? If you give it a try you\u2019ll quickly bump into a\nrouting/port assigning mess - and this is the big thing #2 we\u2019ll tackle!\n\n## The big thing #2: multi-tenancy with fly-replay\n\nFor this to work, we\u2019ll need:\n\n  * Dynamic Request Routing - to route requests to the tenants\u2019 machines,\n  * Fly.io internal addressing - to lookup a Machine ID we want to route a request to,\n  * Machine\u2019s metadata - to mark a machine with its CUSTOMER_ID so we can easily discover them using our tenants\u2019 IDs.\n\nWow, that was probably the most buzzword-intensive statement in this post, but\nbear with me - it will work, it will make sense, and life will be beautiful\nagain \u2764\ufe0f. Anyhow, if you are curious, see our Globally Distributed Postgres\npost to see how powerful are the features we\u2019re dealing with here.\n\nIf there\u2019s more than one Machine exposing the same service, the Fly Proxy will\nload balance requests to our app across all of them (this is how our proxy\nworks by default). This is not what we want since a request for a given\ncustomer must reach its machine.\n\nThe way to achieve this is to make each machine execute business logic and act\nas a router for our app and replay requests that are not destined for it. For\nexample, if a machine for customer 100 gets a request for customer 200, it\nwill route the request to the customer\u2019s 200 machine. If there\u2019s no machine\nfor a given customer, 404 will be returned. The following diagram depicts a\nrequest flow in which a Machine acts as a router:\n\nTenant-1 user gets load-balanced to a Tenant-2 Machine (2), which replays the\nrequest to Tenant-1 (3-4) machine, where it gets served.  \n---  \n  \nAt the high level, we\u2019ll have 2 Express routes: one for the CUSTOMER_ID the\nMachine is running for and the other one for the routing:\n\n    \n    \n    // server.js const customerId = process.env.CUSTOMER_ID || 0 ... app.get(`/customers/${customerId}`, async (request, response) => { // business logic (the counter) for the CUSTOMER_ID this machine IS serving }); app.get(`/customers/:customerId`, async (request, response) => { // routing logic for CUSTOMER_IDs this machine IS NOT serving let machineId = await get_machine_id(request.params.customerId); if (machineId) { response.set('fly-replay', `instance=${machineId}`).send(); } else { response.status(404).send(\"Not found\"); } })\n\nIf the get_machine_id(request.params.customerId) returns a valid machineId, we\nset the fly-replay HTTP header with that customer\u2019s machineId and send a\nresponse back. Because of the header, the proxy won\u2019t return the response to\nthe caller but will replay the request to the given machine. That way, each\ntenant hits its instance.\n\nNow, how does the get_machine_id(request.params.customerId) work? It relies on\neach machine having a CUSTOMER_ID metadata key being set. That allows us to\nmake a DNS lookup to fetch a customer\u2019s machine IPv6 via a special address:\ndig aaaa +short <CUSTOMER_ID>.customer_id.kv._metadata.<app-name>.internal.\nHaving the IP address, we can do one more lookup (a reverse one) to fetch the\nmachine ID that we need for the fly-replay header: dig +short -x\nfdaa:6:32a:a7b:23c8:3d9b:fe49:2 (the last argument is an example IPv6 address\nof a Machine).\n\nThe get_machine_id(customerId) function uses the node-dig-dns package to\nimplement these queries in JS:\n\n    \n    \n    // server.js const appName = process.env.FLY_APP_NAME; const get_machine_id = async (customerId) => { // return a sample ID the app is not deployed on Fly.io if (!appName) { return \"abcd1234\" }; try { const ip = await dig([`${customerId}.customer_id.kv._metadata.${appName}.internal`, 'aaaa', '+short']) const addr = await dig(['+short', '-x', ip]); return addr.split('.')[0]; } catch (error) { // error handling } };\n\nnode-dig-dns requires the dnsutils OS package, thus we install it in the\nDockerfile.\n\nYou may have spotted the addr.split('.')[0]. This is to extract only the 1st\nsegment of the returned string since the reverse lookup (the one with -x)\nreturns an FQDN address, e.g.: 3d8dd15c1ed778.vm.jts.internal..\n\nFinally, how do we set that metadata? There are a few ways to do that, like\nwith an API, but we will do that at the machine startup as fly machine run ...\n--metadata customer_id=100.\n\nLastly, let\u2019s put the cherry on the cake and deploy the whole thing for 3\ncustomers. Take it easy, check out the main branch, and providing you\u2019ve set\nup the <app-name>, secrets, and the IPv4 address as described in The big thing\n#1, then just execute the following:\n\n    \n    \n    for i in `seq 1 3`; do fly machine run . --name customer${i}00 --port 443:3000/tcp:http:tls \\ --env CUSTOMER_ID=${i}00 \\ --env AWS_REGION=$AWS_REGION \\ --env AWS_ENDPOINT_URL_S3=$AWS_ENDPOINT_URL_S3 \\ --env BUCKET_NAME=$BUCKET_NAME \\ --metadata customer_id=${i}00 done\n\nThat will spin up Machines for customers with IDs 100, 200 and 300 and make\nyour app available at https://<app-name>.fly.io/customer/<CUSTOMER_ID>.\n\nThat\u2019s is it! We\u2019ve made it - we have a demo multi-tenant application with\nexclusive computing with Machines and data storage with SQLite database hosted\nat Tigris.\n\n## Conclusion\n\nThere are a number of things that we could elaborate on here, but let me focus\non just two things: automating tenant provisioning and scalability aspects of\nthe app we\u2019ve built.\n\nI will start with the former because it\u2019s simpler: we have all the tooling to\nautomate tenant provisioning and its lifecycle management. The Machines API\nmakes it possible to programmatically create and manage the lifecycle of the\nmachines. Also, by tweaking the machines config appropriately, we can instruct\nthe proxy on things like automatic shutdown or wake-up. That\u2019s probably a big\ntopic for another post, so I will just stop here.\n\nWhen it comes to scalability, our approach is far from perfect since if we had\nmore than one customer Machine (i.e., if we attempted to scale horizontally),\nwe\u2019d end up mutating the SQLite database in parallel, which would result in\noverwrites. One possible solution to this problem would be to transactionally\ncreate a machine.lock file in a customer bucket containing the Machine\u2019s ID\nthat performs the DB mutations at the current moment. That would possibly\nallow a Machine failing to acquire the lock to fly-replay the request to the\none holding it. Once the \u201cmutator\u201d is done, the lock will be released by\nremoving the file.\n\nTigris allows for Conditional Operations, which would help ensure that only\none request creates the lock file and all the other concurrent ones would\nfail.\n\nHappy multi-tenanting!\n\nLast updated\n\n\u2022\n\n    Apr 17, 2024\n\nShare this post on Twitter Share this post on Hacker News Share this post on\nReddit\n\nAuthor\n\n    \n\nName\n\n    Szymon Mentel\n\nPrevious post \u2193\n\n    Building a Remix app locally with Docker\n\nPrevious post \u2193\n\n    Building a Remix app locally with Docker\n\nCompany\n\n    About Pricing Jobs\n\nArticles\n\n    Blog Phoenix Files Laravel Bytes Ruby Dispatch Django Beats JavaScript Journal\n\nResources\n\n    Docs Support Status\n\nContact\n\n    GitHub Twitter Community\n\nLegal\n\n    Security Privacy policy Terms of service\n\nCopyright \u00a9 2024 Fly.io\n\n", "frontpage": false}
