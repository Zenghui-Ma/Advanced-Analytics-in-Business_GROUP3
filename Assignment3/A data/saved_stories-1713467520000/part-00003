{"aid": "40075838", "title": "Disengage Your Autopilot", "url": "https://jjain.substack.com/p/disengage-your-autopilot", "domain": "jjain.substack.com", "votes": 1, "user": "jinay", "posted_at": "2024-04-18 13:11:07", "comments": 0, "source_title": "Disengage your autopilot", "source_text": "Disengage your autopilot - by Jinay Jain\n\n# Inductive Biases\n\nShare this post\n\n#### Disengage your autopilot\n\njjain.substack.com\n\n#### Discover more from Inductive Biases\n\nA collection of thoughts around tech, AI, startups, and my career experiences.\n\nContinue reading\n\nSign in\n\n# Disengage your autopilot\n\n### A lesson from reinforcement learning\n\nJinay Jain\n\nApr 18, 2024\n\nShare this post\n\n#### Disengage your autopilot\n\njjain.substack.com\n\nShare\n\nA few months ago, my life had finally settled into a comfortable routine. The\ninitial stress of a new job, apartment hunting in NYC, and moving to an\nunfamiliar city had settled. What remained was a cycle of taking the same\ncommute to work, talking to the same people, and eating the same foods. My\nturbulent takeoff into a new life had succeeded, and I was cruising at 35,000\nfeet\u2014autopilot engaged.\n\nLiving on autopilot is both a luxury and a curse. Having a solid routine means\nI\u2019ve tackled my greatest challenges, letting me live a relatively peaceful\nexistence above the stormy clouds below. It equips me with a flight plan that\nguides me to some ultimate endpoint. The person who set that destination isn\u2019t\nme, though. It is a more naive version of me, whose goals and aspirations\ninevitably differ from what they are now.\n\nAutopilot takes you to the peak of the circled region, but won\u2019t let you leave\nit\n\nIn optimization terms, I was stuck in a local optimum\u2014a solution that seems\ngood in the moment, but may not be the best overall outcome. Fortunately for\nme, there's a field riddled with local optima in unknown environments:\nreinforcement learning (RL). Reinforcement learning is a kind of AI where the\nanswers aren\u2019t presented to the model from the start. Instead, we give it an\nenvironment and the occasional thumbs up or thumbs down (the \u201creward\nfunction\u201d), telling it how well it\u2019s doing. The model interacts with its\nsurroundings to find what series of actions will yield the highest reward.\nOnce it finds an action sequence that seems to work, it must retrace its steps\nand find a strategy for consistently replicating those results. It\u2019s not only\nfiguring out how to learn (like most machine learning models), but also what\nto learn in the first place.\n\nSound familiar?\n\nIt\u2019s no coincidence that training an RL model feels more like raising a child\nthan any other form of machine learning1. The model starts as a baby with an\ninfinitely malleable brain. Knowing nothing about the world, it tries anything\nit can but often fails to make much meaningful progress. After some crude\nlearning, it proudly presents you with its first solution.\n\nThrilling! But the score in the bottom left tells a different story. We reward\nthe model for how fast it can finish the track, but the model seems to think\nit\u2019s being rewarded for the number of burnouts. Its limited experiences of the\nworld have shown it enough to know that it\u2019s doing something right, but not\nenough to pinpoint what exactly that something is. So, after earning some\npoints for driving a few feet on the track, the model veers off into the grass\nto begin its spiraling dance. It has found its first local optimum, one which\nwould require many changes to go from drift king to Formula 1 driver.\n\nIf we let this behavior continue, we might find that it learns to drift faster\nand tighter circles. As the omniscient designers of the environment, however,\nwe know there\u2019s a much more lucrative solution available. In RL, this stems\nfrom the exploration-exploitation trade-off. We want the AI to exploit the\nknowledge it already has gained, but we also want it to explore a variety of\npotential solutions. It\u2019s a trade-off because these motives directly oppose\neach other. The model can\u2019t be exploring and exploiting at the same time.\n\nThere are many ways we can handle this trade-off2, but the simplest strategy\nis to force the model to explore some percent of the time. That means\nhijacking the model\u2019s best judgement and forcing it to take a random action.\nWe start by doing this 100% of the time, and ramp it down as our confidence in\nwhat the model has learned increases. Importantly, this plateaus at 5% so that\neven an expert model has the chance to explore. This simple strategy produces\nimpressive results3:\n\nThe complexity of this racing environment pales in comparison to the\nintricacies of the world we live in, yet there\u2019s much we can learn from our\nlittle model. Though we may never notice it, we might be doing some burnouts\nof our own, making illusory progress towards maximizing our true reward\nfunction. The more you see of the world, the clearer your definition of that\nreward function will be\u2014the one that keeps you motivated, happy, and\nfulfilled. From time to time, disregard your tendency to exploit what you\u2019re\nfamiliar with, and instead, explore what is unknown. Going beyond your comfort\nzone won\u2019t cut it here. What I mean by exploration is actively disregarding\nyour intuition and moving orthogonal to your judgement. Take a random walk\nthrough your city. Talk to someone from another country. Do everything except\nwhat you think you should be doing.\n\nIt\u2019s not an easy ask. We have no omniscient guide forcing us to explore and\nchoosing our random actions for us, so we rely on our imperfect selves\u2014humans\nare notoriously bad at generating random numbers4. Disregarding intuition\nrequires a level of meta-cognition (thinking about thought) that few possess.\nFrankly, I\u2019ve struggled to replicate true exploration myself, but the pursuit\nof it has improved my understanding of what I value. There will be times where\nyou fall back into long periods of autopilot. It\u2019s your job to disengage it,\nenter free-fall, and see what landscape lives below the clouds.\n\nWe apply human biases to machine learning models all the time, but there\u2019s\nmuch we can learn from them. If you\u2019d like to hear more, consider subscribing.\n\np.s. check out my latest project, SOTA Stream, a research paper digest that\ntailors to your interests\n\n1\n\nThough it\u2019s common to develop an attachment to your models, reinforcement\nlearning takes this to another level. Almost nothing is guaranteed to work,\nwhich makes the successes all the more emotional.\n\n2\n\nExploration Strategies in Deep Reinforcement Learning\n\n3\n\nSource Code: https://github.com/JinayJain/deep-racing\n\n4\n\nhttps://crypto.stackexchange.com/a/103900\n\nShare this post\n\n#### Disengage your autopilot\n\njjain.substack.com\n\nShare\n\nComments\n\nInterfaces all the way down\n\nHow better interfaces lead to happier developers\n\nJul 23, 2023 \u2022\n\nJinay Jain\n\n6\n\nShare this post\n\n#### Interfaces all the way down\n\njjain.substack.com\n\nThe Myth of the Finished Project\n\nAnd why you should release anyways\n\nApr 5, 2023 \u2022\n\nJinay Jain\n\n1\n\nShare this post\n\n#### The Myth of the Finished Project\n\njjain.substack.com\n\nReady for more?\n\n\u00a9 2024 Jinay Jain\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": false}
