{"aid": "40127374", "title": "Efficient finetuning of Llama 3 with FSDP QDoRA", "url": "https://www.answer.ai/posts/2024-04-26-fsdp-qdora-llama3.html", "domain": "answer.ai", "votes": 7, "user": "jph00", "posted_at": "2024-04-23 01:07:28", "comments": 2, "source_title": "Efficient finetuning of Llama 3 with FSDP QDoRA", "source_text": "Answer.AI - Efficient finetuning of Llama 3 with FSDP QDoRA\n\n# Efficient finetuning of Llama 3 with FSDP QDoRA\n\nWe\u2019re releasing FSDP QDoRA, a scalable and memory-efficient method to close\nthe gap between parameter efficient finetuning and full finetuning.\n\nAuthor\n\nKerem Turgutlu\n\nPublished\n\nApril 22, 2024\n\n## Introduction\n\n> This introductory note is from Answer.AI co-founder Jeremy Howard. The\n> remainder of the article after this section is by Kerem Turgutlu and the\n> Answer.AI team.\n\nWhen Eric and I created Answer.AI, a key foundation of our research thesis was\nbased on two trends we expected to see:\n\n  1. A dramatic increase in the size and capability of open source models\n  2. Much larger opportunities for finetuning these models, using \u201ccontinued pre-training\u201d. (You can learn more about my thoughts on this in my \u201cLatent Space\u201d podcast interview, The End of Finetuning).\n\nA few days ago, our expectations were realized when Meta announced their Llama\n3 models. The largest will have over 400 billion parameters and, although\ntraining isn\u2019t finished, it is already matching OpenAI and Anthropic\u2019s best\nLLMs. Meta also announced that they have continuously pre-trained their models\nat far larger scales than we\u2019ve seen before, using millions of carefully\ncurated documents, showing greatly improved capability from this process.\n\nFrom the day we launched the company, we\u2019ve been working on the technologies\nnecessary to harness these two trends. Last month, we completed the first\nstep, releasing FSDP/QLoRA, which for the first time allowed large 70b models\nto be finetuned on gaming GPUs. This helps a lot with handling larger models.\n\nToday we\u2019re releasing the next step: QDoRA. This is just as memory efficient\nand scalable as FSDP/QLoRA, and critically is also as accurate for continued\npre-training as full weight training. We think that this is likely to be the\nbest way for most people to train^1 language models. We\u2019ve ran preliminary\nexperiments on Llama 2, and completed some initial ones on Llama 3. The\nresults are extremely promising.\n\nI expect that QDoRA with Llama 3 will allow open source developers to create\nbetter models for their tasks than anything that exists today, free or paid.\nHere\u2019s a taste of the very impressive out-of-the-box results we\u2019ve seen,\nshowing Llama3 with QDoRA, or with full finetuning, greatly outperforming\nQLoRA and Llama2 when training on mathematical data (and note that full\nfinetuning uses far more memory than the other approaches):\n\nComparison of the loss curves of Llama3-8B + QLoRA, Llama3-8B + QDoRA against\nthe equivalent Llama2 7B training runs and Llama3-8B full-finetune.\n\nI particularly want to highlight the exceptional work done by Kerem Turgutlu\nin kicking off and leading this project. Kerem was one of my masters students\nat the University of San Francisco years ago, and he really stood out with his\ncreativity, work ethic, and intellect. I was very confident he was going to\nachieve great things \u2013 and I very much hoped that we could work together one\nday. With Answer.AI, I hoped that we had created just the kind of environment\nthat Kerem, and others like him, could reach their full potential. It looks\nlike we may have done just that!\n\n## Launching 2 new scalable training methods\n\n> *If you\u2019re not familiar with FSDP and LoRA already, please first read our\n> article https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html to learn\n> about the fundamentals on top of which today\u2019s work is built.\n\nToday we\u2019re launching two significant quantized parameter efficient training\nmethods with FSDP compatibility, DoRA (Weight-Decomposed Low-Rank Adaptation)\nand Llama-Pro (Progressive LLaMA with Block Expansion). These methods are\navailable for use now at the FSDP QLoRA github repository - see Code and\nModels to get started.\n\nOur early results suggest that QDoRA (\u201cQuantized DoRA\u201d) is especially\nvaluable. There are no other libraries offering QDoRA or quantized Llama-Pro\ntraining with FSDP support that we know of. You can see the results fine\ntuning Llama 2 on the Orca-Math dataset below, where the low-memory QDoRA\n(quantized DoRA) implementation produces better performance than other\nmethods, whilst using much less memory than full fine-tuning:^2\n\nPreliminary Llama-2 7B Orca-Math finetuning results without hyperparameter\ntuning.\n\nIn essence, DoRA combines much of the parameter efficiency of plain QLoRA with\nthe more granular optimization of full finetuning. Using our code, you can use\nthese methods right now for efficient finetuning the most popular open source\nLLM models including Llama 3 on your own GPUs.\n\nIntrigued? In the sections below, this article will cover the following areas:\n\n  * Explanation. Summarizing DoRA and Llama-Pro, to provide a working intuition for how these two optimizations work.\n  * Deep Dive. Discussing the training setup, benchmarks, and code you can use to run these training optimizations yourself. This benchmark results compare the performance of different training methods, using the Orca-Math dataset\n  * Hosting. Discussing current options for hosting, a topic which requires special care since not all hosting frameworks support such optimized models\n  * Future Work. Covering future improvements on production-ready inference and optimized fused kernels for quantized LoRA, DoRA..\n  * The release of Llama-3. Discussing the potential impact of ever-improving open source base models, and how to leverage our work with them.\n\n## LoRA\n\nInspired by prior work, Hu et al^3 hypothesized that most finetuning updates\noperate at a low intrinsic rank. In other words, updating all the model\u2019s\nparameters during finetuning is unnecessary, we should be able to finetune\nmodels by only updating a subset of parameters. Low-Rank Adaptation (LoRA)\nimplements this idea by freezing the original linear layer weights and instead\ntraining a low dimension reparameterization. The output of the frozen layer\nand trainable LoRA layer are combined to produce the finetuned output.\n\nThis setup, especially when combined with quantization such as QLoRA, greatly\nreduces the memory requirements of finetuning models and popularized\nParameter-Efficient Fine-Tuning (PEFT) methods.\n\nWhile LoRA often matches or nearly matches full finetuning performance, there\nare cases where it cannot match full finetuning performance, as observed by\nAnyScale in their in-depth comparison of LoRA and full finetuning.\n\n## DoRA\n\nIn DoRA: Weight-Decomposed Low-Rank Adaptation, Liu et al^4 investigated the\ncapacity gap between LoRA and full finetuning. Inspired by Weight\nNormalization, they proposed splitting the LoRA layers into two components,\none for magnitude and one for direction, and finetuning both. This\ndecomposition allows DoRA to better match the performance of full finetuning\nwhile adding a marginal number of parameters to train relative to LoRA.\n\nFigure 2 from Liu et al shows that while DoRA doesn\u2019t completely match full\nfinetuning, DoRA\u2019s magnitude and direction modifications are more closely\ncorrelated with full finetuning while LoRA\u2019s updates are not.\n\nAs seen from the weight decomposition analysis in Figure 2. of the DoRA paper,\nfull finetuning (FT) independently optimizes the direction and magnitude of\nupdates to weights. LoRA only updates them in tandem. DoRA is the best of both\nworlds: it updates them independently, like normal FT, but with the parameter\nefficiency of LoRA.\n\nOur implementation of FSDP-compatible QDoRA mirrors our QLoRA implementation.\nExisting pretrained layers are frozen and quantized using bitsandbytes\nnormalized 4-bit float 4 (BnB NF4) or half-quadratic quantization (HQQ) 4-bit\nformats and are applied to most linear layers, specifically the attention\nquery, key, and value layers and the MLP upscale, gating, and downscale\nlayers. These trainable QDoRA layers represent approximately two percent of\nthe total model parameters.\n\n## Llama-Pro\n\nIn LLaMA Pro: Progressive LLaMA with Block Expansion, Wu et al^5 explores an\ninnovative method for enhancing large language models (LLMs) through a\ntechnique called block expansion.\n\nThis technique strategically adds Transformer blocks to improve model\nspecialization without sacrificing existing capabilities. Specifically, it\ninterleaves new transformer decoder layers which are initialized as identity\nfunctions to maintain model output while integrating new, domain-specific\ninformation from tailored datasets such as programming and mathematics. This\napproach allows the model to extend its depth and refine its specialization\nareas, providing an advantageous blend of broad general knowledge and sharp,\ndomain-specific expertise without the common downsides of full model\nretraining or extensive finetuning. The main motivation is to keep the\noriginal pretrained weights unchanged and train new layers with skip\nconnections (like resnets) initialized from identity blocks to add new\ncapabilities. So in the perfect scenario, the model should have no regressions\nfrom the past training tasks.\n\nIn the original implementation of the Llama-Pro, a number of new trainable\ndecoder blocks are interleaved. In our experiments, we follow a similar\napproach by adding a new decoder block after every 10 layers while quantizing\nthe frozen layers. This is referred to as a 10% expansion rate, which is\nanother hyperparameter that can be tuned depending on the task complexity.\nThese new decoder layers are initialized with the same weights from the\npreceding layer except for down and output projection layers which are zero-\ninitialized to make the new decoder layer an identity layer.\n\nFigure 3. from Llama-Pro paper contrasting a regular transformer decoder layer\nblock vs a new block added after block expansion. MHSA (Multi-head Self\nAttention) and FFN (Feed Forward Network).\n\n## Training Performance\n\n### Training and Evaluation Setup\n\nIn most of our experiments we used the Llama-2-7b base model (our work started\nbefore the Llama-3 release), the Orca-Math dataset and the following\nhyperparameters:\n\nParameter| Value  \n---|---  \nEpoch| 1  \nPrecision| bf16  \nBatch Size| 32  \nOptimizer| AdamW  \nLearning Rate| 1e-5  \nLearning Rate Schedule| Constant  \nWeight Decay| 0.1  \nContext Length| 2048  \nLoRA Rank| 64  \nLoRA Target Modules| k_proj, q_proj, v_proj, up_proj, down_proj, gate_proj  \nLlama-Pro Expansion Rate| 0.1  \n  \nThe dataset contains ~200K grade school math word problems. All the answers in\nthis dataset are generated using GPT4-Turbo. You may refer to Orca-Math:\nUnlocking the potential of SLMs in Grade School Math for details about the\ndataset construction. Math is a good testbed for general reasoning skills and\nit is easier to evaluate compared to more open-ended tasks such as chatbots.\n\nWe extract the ground truth labels of the Orca-Math dataset using regex, which\nidentifies the last occurrence of digits that may include leading currency\nsymbols, decimal points, and ratios. We use the exact match score as the\nevaluation metric. The same set of experiments is conducted with small and\nlarge training sample sizes, 10k and 100k respectively. 500 samples are held\nout for evaluation. In addition to quantization-aware trained models, we also\nevaluated zero-shot, few-shot, and full finetuning with post-quantization,\nwhere all parameters of the model are trained and later quantized with BnB\nNF4.\n\n### Results\n\nLoss curve of Full Fine-tuning, BnB QLoRA, BnB QDoRA, and BnB Llama-Pro\ntrained models with 10k samples.\n\nEvaluation results of Zero-Shot, Few-Shot, Full Fine-tuning, Full Fine-tuning\nPost Quantization, BnB QLoRA, BnB QDoRA, and BnB Llama-Pro trained models with\n10k samples.Model| Method| Train sample size| Eval sample size| Exact match\nscore  \n---|---|---|---|---  \nllama-2-7b| zero-shot| -| 500| 0.068  \nllama-2-7b| 5-shot| -| 500| 0.08  \nllama-2-7b| full finetune| 10k| 500| 0.182  \nllama-2-7b| full finetune + post quant.| 10k| 500| 0.14  \nllama-2-7b| QLoRA| 10k| 500| 0.098  \nllama-2-7b| QDoRA| 10k| 500| 0.176  \nllama-2-7b| quantized llama pro| 10k| 500| 0.134  \n  \nLoss curve of Full Fine-tuning, BnB QLoRA, BnB QDoRA, and BnB Llama-Pro\ntrained models with 100k samples.\n\nEvaluation results of Zero-Shot, Few-Shot, Full Fine-tuning, Full Fine-tuning\nPost Quantization, BnB QLoRA, BnB QDoRA, and BnB Llama-Pro trained models with\n100k samples.Model| Method| Train sample size| Eval sample size| Exact match\nscore  \n---|---|---|---|---  \nllama-2-7b| zero-shot| -| 500| 0.068  \nllama-2-7b| 5-shot| -| 500| 0.08  \nllama-2-7b| full finetune| 100k| 500| 0.26  \nllama-2-7b| full finetune + post quant.| 100k| 500| 0.168  \nllama-2-7b| QLoRA| 100k| 500| 0.118  \nllama-2-7b| QDoRA| 100k| 500| 0.312  \nllama-2-7b| quantized llama pro| 100k| 500| 0.134  \n  \nThe key insight from this preliminary study is QDoRA\u2019s edge as a top choice\namong other quantized, parameter-efficient methods. In our experiments to\ndate, it matches or exceeds the performance of full finetuning, but requires\nfar less memory. (We expect extensive hyperparameter optimization might\nultimately push full finetuning to yield the best results, but we have not\nobserved this in our tests.)\n\nWhile post-quantization significantly degrades performance, it\u2019s worth\nmentioning that we used BnB NF4 for post-quantization\u2014a weight-only\nquantization method. Using activation-aware quantization methods like AWQ or\nGPTQ should improve results, though these methods can\u2019t be used with\nquantization-aware finetuning and may be susceptible to data biases and\nshifts. Additionally, the advanced quantization-aware finetuning techniques\nintroduced here are set to revolutionize training for the largest open-source\nmodels using FSDP. They promise to cut GPU server costs dramatically while\nstill closely approximating the performance of full finetuning.\n\n## Code and Models\n\nTo train your own quantization aware fine-tuned models or to reproduce our\nresults you can take a look at the different training options available here -\nwhich is from the original FSDP-QLoRA github repo. You can also access all the\ntrained models from our Hugging Face collection.\n\nTo train with QDoRA using the same 10k Orca-Math samples:\n\n    \n    \n    # Assuming 4 GPUs, if different adjust `gradient_accumulation_steps` to make bs=32. python fsdp_qlora/train.py \\ --train_type bnb_dora \\ --model_name meta-llama/Llama-2-7b-hf \\ --dataset orca_math \\ --dataset_samples 10000 \\ --batch_size 4 \\ --context_length 2048 \\ --gradient_accumulation_steps 2 \\ --sharding_strategy full_shard \\ --use_gradient_checkpointing true \\ --reentrant_checkpointing true \\ --use_cpu_offload false \\ --use_activation_cpu_offload false \\ --log_to wandb \\ --project_name \"fsdp-quantized-ft-exps\" \\ --save_model true \\ --output_dir models/llama-7b-orca-math-10k-bnb-QDoRA\n\nBefore Llama-Pro training you need to prepare the expanded version of the\nmodel weights to be used during model initialization:\n\n    \n    \n    # Adds a new block after every 10 blocks and saves the weights to directory. python fsdp_qlora/scripts/block_expansion.py \\ --model_name meta-llama/Llama-2-7b-hf \\ --output_dir /path/to/llama_pro_weights_directory \\ --expansion_rate 0.1\n\nTo train with Llama-Pro:\n\n    \n    \n    # Assuming 4 GPUs, if different adjust `gradient_accumulation_steps` to make bs=32. python fsdp_qlora/train.py \\ --train_type bnb_llama_pro \\ --llama_pro_path /path/to/llama_pro_weights_directory \\ --model_name meta-llama/Llama-2-7b-hf \\ --dataset orca_math \\ --dataset_samples 10000 \\ --batch_size 4 \\ --context_length 2048 \\ --gradient_accumulation_steps 2 \\ --sharding_strategy full_shard \\ --use_gradient_checkpointing true \\ --reentrant_checkpointing true \\ --use_cpu_offload false \\ --use_activation_cpu_offload false \\ --log_to wandb \\ --project_name \"fsdp-quantized-ft-exps\" \\ --save_model true \\ --output_dir models/llama-7b-orca-math-10k-bnb-qdora\n\nIn the above examples, you can replace the training type with hqq_dora and\nhqq_llama_pro to use HQQ 4-bit quantization instead of BnB.\n\nTo evaluate the trained models on Orca-Math dataset you can refer to our\nstandalone evaluation python script and evaluation bash script. This\nevaluation script uses HF\u2019s model.generate() method to evaluate the exact\nmatch score between the ground truth answers and the generated text. Using\nthis method on very large datasets is not recommended as it is neither\noptimized nor suitable for production deployment.\n\nNext, we will take a look at how we can overcome the slow inference problem\nand potentially make inference much more efficient.\n\n## Inference\n\nAs we\u2019ve discussed, the QDoRA optimizations allow much faster training.\nHowever, they also require some corresponding changes in the inference\nframework, in order to be able to serve the modified models efficiently.\n\nWe looked at vLLM. vLLM is a robust, production-ready framework designed for\nserving LLM endpoints, offering high throughput and good latency. You can\nlearn more about it in an article on Better Programming which compares and\ncontrasts different frameworks for deploying LLMs. vLLM already supports\nvarious quantization methods such as AWQ, GPTQ, SqueezeLLM, and Marlin.\nUnfortunately, the main branch of vLLM does not currently support weight-only\nquantization libraries like the ones which we use, BnB and HQQ.\n\nAs an initial effort to serve quantization-aware finetuned models, we added\nBnB and assessed the post-quantization and QDoRA inference performance.\nAlthough this implementation is still inefficient and not thoroughly\noptimized, it performs better than the vanilla HF generate(). Our experimental\nimplementation only works in eager mode and does not work with the CUDA graphs\nmode in vLLM. Using vLLM\u2019s eager mode, a performance improvement of 1.5-2x can\nbe achieved. This article from Fireworks.ai explains the benefits of using\nCUDA graphs in the context of decoder-only LLM inference. To enable CUDA graph\nmode, modifications related to CUDA streams might be necessary, as discussed\nin this PR. Also, our QDoRA layer is not implemented as a fused kernel,\nmeaning that sequential and separate CUDA kernel launches are required to\ndequantize, to merge the pretrained weights and LoRA weights, and to compute\nthe final matrix multiplication - we will talk more about potential\noptimizations in the Future Work section. You can explore our experimental\nvLLM branch for more details and how the BnB quantization method is integrated\ninto vLLM.\n\nWe evaluated throughput and latency of the following models: full finetuned\n(FFT) post quantized (BnB NF4), QDoRA with separate quantized and LoRA\nweights, QDoRA with merged weights, and a GPTQ-Marlin post-quantized version\nof the QDoRA merged model. GPTQ-Marlin FP16x4bit matmul kernel offers the best\nspeedup compared to other methods according to their benchmark results. Note\nthat the QDoRA model with merged weights doesn\u2019t save any memory as the\nquantized weights are dequantized and merged with LoRA layers. So it is\neffectively the same as serving a non-quantized model. During GPTQ-Marlin\nquantization, after merging the QDoRA weights we quantized the model using\n3000 samples from the same Orca-Math dataset used during training. We adapted\nthe code from here.\n\nIn our experiments, we also tested tensor parallelism by using multiple GPUs,\nin which the matrix multiplications are parallelized with either row-\nparallelism or column parallelism in different layers of the model. Our multi-\nGPU machine with 4xA5000 is rented from RunPod, which does not have an optimal\ntopology, so the multi-GPU results could be further improved with better\ninterconnect configurations, such as NVLinks. During inference benchmarks, 50\nhold-out samples from the validation set were used to compute the exact match\nscore. Variable results were observed, which can be attributed to the tensor-\nparallelism algorithm.\n\nAs a reference vanilla HF generate() method using FFT (full finetune) + post\nquantization attains a throughput of 7 req/min.Model| Compilation Mode| Model\nCompr. Rate| TP| Throughput (req/min)| Throughput (tok/sec)| Latency\n(tok/sec)| Exact match score  \n---|---|---|---|---|---|---|---  \nFFT + Post quant.| Eager| 4X| 1| 41| 231| 15.6| 0.16  \nFFT + Post quant.| Eager| 4X| 2| 65| 389| 27.5| 0.2  \nFFT + Post quant.| Eager| 4X| 4| 74| 381| 28.1| 0.2  \nQDoRA BNB| Eager| 4X| 1| 15| 76| 5.4| 0.24  \nQDoRA BNB| Eager| 4X| 2| 27.5| 152| 9.9| 0.22  \nQDoRA BNB| Eager| 4X| 4| 50| 271| 18.3| 0.24  \nQDoRA (merged)| CUDA Graphs| 1X| 1| 104| 546| 46| 0.42  \nQDoRA (merged)| CUDA Graphs| 1X| 2| 142| 835| 67| 0.36  \nQDoRA (merged)| CUDA Graphs| 1X| 4| 172| 1003| 76| 0.38  \nQDoRA (merged) + GPTQ Marlin| CUDA Graphs| 4X| 1| 194| 1122| 130| 0.38  \nQDoRA (merged) + GPTQ Marlin| CUDA Graphs| 4X| 2| 200| 1008| 79| 0.34  \n  \nExperiments are conducted on the same 4xA5000 machine using the pretrained\nllama-2 models with different training methods. We leverage the existing vLLM\ncode or our own vLLM BnB integrations where applicable. TP: number of GPUs\nused for tensor parallelism. Throughput: requests / minute and tokens /\nminute. Latency: tokens / second, computed by sending a single request with 5\ninput tokens and forcing 1024 output tokens during generation. Exact match\nscore: 50 held-out sample exact match score.\n\nTo prepare the vLLM compatible weight files from the pretrained Orca-Math\nmodels you can refer to our standalone python script and bash script.\n\nEven though our custom implementation has much better performance compared to\nvanilla Hugging Face Transformers, it is still not close to optimized vLLM\nmethods. Full finetuned and post quantized model is nearly 5x slower compared\nto GPTQ Marlin, and QDoRA is 15x slower. We can also see that GPTQ Marlin is\nboth faster and 4x more memory efficient than the un-quantized vLLM model\n(QDoRA merged). This inspires future work to improve the weight-only\nquantization methods with faster kernels, and to implement a fused QDoRA\nlayer.\n\nThat being said, GPTQ-Marlin post-quantization following the merging of DoRA\nweights could serve as a temporary solution if deploying merged weights\ndirectly is not feasible. We are actively collaborating with the authors of\nopen-source quantization libraries to enhance the efficiency of HQQ-LoRA/DoRA\nintegrations.\n\n## Future Work\n\nWe\u2019ve seen in the case of work such as flash attention that IO-aware fused\nkernels can provide substantial performance improvements. We\u2019re collaborating\nwith others in the community to build out fused kernels for quantization and\nLoRA/DoRa adapters to make the use of these finetuning approaches more\nefficient in both training and inference. We will also continue to work on\nenabling CUDA Graphs for BnB and HQQ in vLLM, as well as Marlin compatible\nHQQ-DoRA finetuning. Feel free to reach out if you\u2019re interested in\ncontributing to this effort.\n\n## Enter Llama 3\n\nLast Thursday, Meta AI released two potent new base models for finetuning with\nthe release of Llama 3 8B and 70B, with an even larger 400B+ parameters model\non the way. Both of the released models have been trained on >15 trillion\ntokens, and exhibit extremely strong performance on both formal benchmarks and\nthe LMSYS Chatbot Arena.\n\nAs Jeremy discussed in the introduction of this post, we believe that powerful\nopen source models finetuned with the right tools, will continuously improve\nand yield better performance than even the strongest proprietary models. Llama\n3 represents a gigantic step in this direction, and indeed the community is\nmore than eager to put this theory to the test: in the space of just a single\nweek-end, thousands of Llama 3 finetunes have already been uploaded to the\nHuggingFace model hub.\n\nWe are eager for our work to make these advances even more accessible. The\nwork we have done on FSDP-QLoRA and QDoRA is immediately applicable to\nLlama-3: the only change necessary to use FSDP-QDoRA with Llama3 is updating\nthe --model-name parameter in the scripts above to meta-llama/Meta-\nLlama-3-{8B|70B}-{|Instruct}.\n\n    \n    \n    # Assuming 4 GPUs, if different adjust `gradient_accumulation_steps` to make bs=32. python fsdp_qlora/train.py \\ --train_type bnb_dora \\ --model_name meta-llama/Meta-Llama-3-8B \\ --dataset orca_math \\ --dataset_samples 10000 \\ --batch_size 4 \\ --context_length 2048 \\ --gradient_accumulation_steps 2 \\ --sharding_strategy full_shard \\ --use_gradient_checkpointing true \\ --reentrant_checkpointing true \\ --use_cpu_offload false \\ --use_activation_cpu_offload false \\ --log_to wandb \\ --project_name \"fsdp-quantized-ft-exps\" \\ --save_model true \\ --output_dir models/Llama-3-8b-orca-math-10k-bnb-QDoRA\n\nOur training code allows for quick, efficient and powerful finetuning of the\nnew models. As a result, it is now within reach of the community to match\ntask-specific GPT-4 performance at home with a dozen lines of code. We\u2019re very\nexcited to see what exciting continuously pre-trained models will appear over\nthe next few weeks!\n\n## General timeline\n\n## Credits\n\nSpecial thanks to Benjamin Clavi\u00e9 for helping with the vLLM BnB integration,\nto Johno Whitaker for running the initial experiments on Llama-3, to Alexis\nGallagher for editorial comments and suggestions, and to Austin Huang,\nBenjamin Warner, Griffin Adams, Eric Ries and Jeremy Howard for reviewing and\nimproving the initial version of this blog post.\n\n## Footnotes\n\n  1. In this post \u201ctrain\u201d means using gradient descent to modify weights in a model. That could be used for training random weights from scratch (\u201cpre-training\u201d) or starting from pre-trained weights (\u201cfinetuning\u201d or \u201ccontinued pre-training\u201d). In practice, there\u2019s very rarely any need for starting with random weights, so nearly everyone reading this will be interested in finetuning, and that\u2019s all that we\u2019ve actually tested QDoRA with. But the term \u201cfinetuning\u201d comes with a lot of now-obsolete assumed limitations, and \u201ccontinuous pre-training\u201d is a bit of a mouthful.\u21a9\ufe0e\n\n  2. Because Llama 3 has only just been released, we haven\u2019t had time to do extensive experiments yet. We\u2019d expect that with better hyperparameter optimization we\u2019ll see full finetuning and QDoRA at around the same accuracy.\u21a9\ufe0e\n\n  3. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, LoRA: Low-Rank Adaptation of Large Language Models (2021)\u21a9\ufe0e\n\n  4. Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, Min-Hung Chen, DoRA: Weight-Decomposed Low-Rank Adaptation (2024)\u21a9\ufe0e\n\n  5. Chengyue Wu and Yukang Gan and Yixiao Ge and Zeyu Lu and Jiahao Wang and Ye Feng and Ping Luo and Ying Shan, LLaMA Pro: Progressive LLaMA with Block Expansion (2024)\u21a9\ufe0e\n\n", "frontpage": false}
