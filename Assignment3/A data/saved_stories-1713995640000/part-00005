{"aid": "40145883", "title": "Show HN: VSCode extension with Local LLM, OpenAI, and Redis embeddings", "url": "https://marketplace.visualstudio.com/items?itemName=ekbanasolutions.codellm", "domain": "visualstudio.com", "votes": 1, "user": "gobeam", "posted_at": "2024-04-24 15:53:03", "comments": 0, "source_title": "Codellm: Use Ollama and OpenAI to write code - Visual Studio Marketplace", "source_text": "Codellm: Use Ollama and OpenAI to write code - Visual Studio Marketplace\n\nSkip to content\n\n| Marketplace\n\nSign in\n\nVisual Studio Code>Programming Languages>Codellm: Use Ollama and OpenAI to\nwrite codeNew to Visual Studio Code? Get it now.\n\n# Codellm: Use Ollama and OpenAI to write code\n\n## ekbanasolutions\n\nekbana.com|661 installs| (4) | FreeUse local LLM models or OpenAI right inside the IDE to enhance and automate your coding with AI-powered assistanceInstallationLaunch VS Code Quick Open (Ctrl+P), paste the following command, and press enter.Copied to clipboardMore Info  \n---  \n  \n# Codellm: Opensource LLM and OpenAI extension for VSCode\n\n# Visual Studio Code Extension for Large Language ModelsThis Visual Studio\nCode extension integrates with the Large Language Model (OLLAMA), an open-\nsource language model, offering both offline and online functionality. It's\ndesigned to simplify generating code or answering queries directly within the\neditor.\n\n  * Embeddings Support: Use local or OpenAI embeddings via Redis for enhanced context in your prompts.\n  * File Type Support: Upload and embed content from PDF, DOCX, JSON, and TXT files.\n  * Codebase Embedding: Embed your entire codebase in Redis vector storage or OpenAI for relevant response generation.\n  * OpenAI API Integration: Access OpenAI's official API to utilize GPT-3, GPT-4, or ChatGPT models for code generation and natural language processing.\n\nThis extension makes it easy for developers at any skill level to integrate\nadvanced language models into their development process, enhancing\nproductivity and creativity.\n\n## Features\n\n  * \ud83d\udca1 Ask general questions or use code snippets from the editor to query from locally available LLM modes, OpenAI Models via an input box in the sidebar\n  * \ud83d\uddb1\ufe0f Right click on a code selection and run one of the context menu shortcuts\n    * automatically write documentation for your code\n    * explain the selected code\n    * refactor or optimize it\n    * find problems with it\n    * Copy the code to the clipboard\n    * Download Conversation as a JSON file\n    * Upload Conversation from a JSON file\n  * \ud83d\udcda View the history of your conversations with the AI\n  * \ud83d\udccc Pin conversations to keep them in the sidebar\n  * \ud83d\udce6 Save conversations to a file\n  * \ud83d\udce4 Export conversations to a file\n  * \ud83d\udce5 Import conversations from a file\n  * \ud83d\udcbb View GPT's responses in a panel next to the editor\n  * \ud83d\udcdd Insert code snippets from the AI's response into the active editor by clicking on them\n  * \ud83c\udf9b\ufe0f Customize the prompt that will be sent to the AI for each command\n\n## Usage\n\nYou can select some code in the editor, right click on it and choose one of\nthe following shortcuts from the context menu:\n\n#### Commands:\n\n  * Ask Codellm: will provide a prompt for you to enter any query\n  * Codellm: Explain selection: will explain what the selected code does\n  * Codellm: Refactor selection: will try to refactor the selected code\n  * Codellm: Find problems: looks for problems/errors in the selected code, fixes and explains them\n  * Codellm: Optimize selection: tries to optimize the selected code\n  * Codellm: Download conversation history: Download conversation history\n  * Codellm: Upload conversation history: Upload conversation history\n\nAsk Codellm is also available when nothing is selected. For the other four\ncommands, you can customize the exact prompt that will be sent to the AI by\nediting the extension settings in VSCode Preferences.There, you can also\nchange the model that will be used for the requests. The default is ChatGPT\nwhich is smartest and currently free, but you can change it to another model\n(text-davinci-003 is the best of the paid ones, code-davinci-002 of the free)\nif it doesn't work. You can also change the temperature and number of tokens\nthat will be returned by the AI. The default values are 0.5 and 1024,\nrespectively.\n\n## Installation of extension (Manual):\n\n## Step 1: Download the VSIX Extension File\n\nObtain the VSIX extension of codellm. This file usually has the extension\n\".vsix\".\n\n## Step 2: Open VSCode Extensions View\n\nLaunch Visual Studio Code and go to the Extensions view. You can do this by\nclicking on the Extensions icon in the Activity Bar on the side of the window,\nor by going to the Extensions option in the View menu in the top menu bar.\n\n## Step 3: Install from VSIX\n\nIn the Extensions view, click on the ellipsis (three dots) in the top right\ncorner and select \"Install from VSIX\" from the drop-down menu.\n\n## Step 4: Browse and Select VSIX File\n\nNavigate to the location where you downloaded the VSIX extension file in Step\n1 using the file picker dialog that appears after clicking \"Install from\nVSIX.\" Select the VSIX file and click \"Open\" to start the installation\nprocess.\n\n## Step 5: Review Extension Details\n\nAfter you select the VSIX file, VSCode will display the extension details,\nincluding the extension's name, publisher, and version. Review these details\nto ensure that you are installing the correct extension.\n\n## Step 6: Confirm Extension Installation\n\nClick the \"Install\" button to confirm the installation of the VSIX extension.\nVSCode will then install the extension and display a notification once the\ninstallation is complete.\n\n## Step 7: Restart VSCode\n\nAfter the installation is complete, VSCode may prompt you to restart the\neditor. If prompted, click the \"Reload\" button to restart VSCode with the\ninstalled extension.\n\n## Step 8: Use the Custom Extension\n\nOnce VSCode has restarted, you should be able to use the installed custom\nextension just like any other extension. You can access its features and\nsettings through the Extensions view, and customize it according to your\nneeds.\n\n## Locating extension\n\nAt left side of the VSCode, click on the extensions icon. You will see the\ninstalled extensions. Click on below Icon to open the extension. //load image\nfrom examples folderAfter clicking on the icon, you will see the extension.\nClick on the extension to open it.\n\n## Configure Local LLM\n\n### Install LLM locally with OLLAMA\n\nVisit OLLAMA and download the LLM model. You can find the instructions to\ninstall the LLM model in the documentation. After installing the LLM model,\nyou can use the local LLM in the extension.\n\n### Configuration in Extension\n\nTo configure local LLM, click on the gear icon at the bottom left corner of\nthe VSCode. Steps:\n\n  1. Click on the gear icon at the bottom left corner of the VSCode.\n  2. Choose \"ollama\" from the dropdown for LLM Provider.\n  3. Input your custom ollama url as API endpoint or keep as default.\n  4. Donot change anything on API key since local LLM doesnot require API key.\n  5. When you choose local LLM, extension will automatically fetch the available models from the local server in extension main panel and you can select it from the dropdown as shown in the image below.\n\n## Configure OpenAI API\n\nTo configure OpenAI API, click on the gear icon at the bottom left corner of\nthe VSCode. Steps:\n\n  1. Click on the gear icon at the bottom left corner of the VSCode.\n  2. Choose \"openai\" from the dropdown for LLM.\n  3. Input API key in the key section.\n  4. Choose model from the dropdown as shown in the image below.\n\n## Write a query\n\nTo write a query, click on the \"Ask Codellm\" textarea at the bottom of the\nextension. You will see a textarea to write a query. Write a query and click\non the \"Send\" button. You will see the response from the LLM in the panel.\n\n## Edit/Delete/Insert/Copy conversation\n\nTo edit conversation click on the edit icon on top right corner of your\nconversation. You can also delete the conversation by clicking on the delete\nicon. You can also insert the conversation to the editor by clicking on the\ninsert icon. You can also copy the conversation to the clipboard by clicking\non the copy icon.\n\n## View Chat History\n\nTo view chat history, click on the \"Chat\" button at the top right corner of\nthe extension panel. You will see the chat history in the panel.\n\n## Pin/Unpin conversation history\n\nTo pin a conversation, click on the pin icon on top right corner of the chat\nhistory. You can also unpin the conversation by clicking on the unpin icon.\nYou can search conversation by clicking on the search icon and input the query\nin the search bar.\n\n## Edit/Delete conversation history\n\nTo edit conversation click on the edit icon on top right corner of your\nconversation. You can also delete the conversation by clicking on the delete\nicon.\n\n## Export conversation history\n\nTo export conversation history, right click on your editor and click on the\n\"Codellm: Download conversation history\" option. Click on the button to export\nthe conversation history.\n\n## Import conversation history\n\nTo import conversation history, right click on your editor and click on the\n\"Codellm: Upload conversation history\" option. Click on the button to import\nthe conversation history.\n\n# Embeddings\n\nWhen using OLLAMA as your Large Language Model (LLM) provider through this\nextension, it leverages Redis for storing embeddings. Additionally, the\nextension supports using OpenAI embeddings, offering the flexibility to\ncombine OpenAI with the Redis vector store for enhanced embedding\ncapabilities. These embeddings are crucial for grasping the context of the\ncode, improving the relevance of generated responses.\n\n## Redis-Stack-Server Requirement\n\nTo utilize the embedding features, you must have the redis-stack-server\ninstalled, as the extension is compatible exclusively with this version and\nnot with the standalone Redis server. Installation instructions for the redis-\nstack-server can be found here. It's important to note that this setup is\nnecessary to enable the advanced embedding features offered by the extension.\n\n## Session-Specific Context\n\nPlease be aware that the context generated from your codebase is only\navailable for the current chat session. If you initiate a new chat session,\nyou will need to re-upload your codebase to regenerate the context. This\nensures that each session's suggestions are as accurate and relevant as\npossible.\n\n## Conversation-Specific Context\n\nIn the main chat panel, you'll find the 'Rescan Workspace' button, providing\nthe ability to upload your entire codebase to a Redis server to create these\nembeddings. This process is crucial for tailoring the model's responses to\nyour specific coding environment or project.We understand that your codebase\nmay contain unnecessary files, so by default, the system takes into account\nthe .gitignore file and ignores the directories and files mentioned in it.\nAdditionally, you can specify more files and directories to ignore by adding\nthem to the .codellmignore file in the root of your workspace.For example, to\nignore the following files and directories, create a .codellmignore file in\nthe root of your workspace and add the listed lines:\n\n    \n    \n    .vscode/ .husky/ .github/ bin/ config coverage/ dist/ node_modules/ public/ i18n/\n\nIf you make any changes in your codebase, you can rescan the workspace by\nclicking the 'Rescan Workspace' button in the main chat panel. This action\nupdates the embeddings with the latest changes in your codebase by comparing\nchecksums of the files.Now, with the embeddings, you can generate more\naccurate and relevant responses from the model.To use the workspace file\nembeddings, type '@file' in the prompt. You'll then see a dropdown list of\nfiles in the workspace from which you can select to utilize the embeddings, as\nshown in the image below:\n\n## Refresh Option\n\nClick on the 'Refresh' button in the main chat panel to refresh the models and\nembedded files in the workspace.\n\n## Contributors\n\n  * Roshan Ranabhat (Extension)\n  * Sandeep Risal (Design)\n\n## Acknowledgements\n\n  * CodeGPT: GPT3 and ChatGPT extension for VSCode\n  * Ekbana\n  * OpenAI\n  * OLLAMA\n  * VSCode\n  * Code icons created by Azland Studio - Flaticon\n\n  \n---  \n  \n  * Contact us\n  * Jobs\n  * Privacy\n  * Manage cookies\n  * Terms of use\n  * Trademarks\n\n\u00a9 2024 Microsoft\n\n", "frontpage": false}
