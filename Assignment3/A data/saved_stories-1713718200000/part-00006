{"aid": "40103323", "title": "Dataflow Analyses and Compiler Optimizations That Use Them, for Free", "url": "https://blog.regehr.org/archives/2578", "domain": "regehr.org", "votes": 5, "user": "ingve", "posted_at": "2024-04-21 05:12:06", "comments": 0, "source_title": "Dataflow Analyses and Compiler Optimizations that Use Them, for Free \u2013 Embedded in Academia", "source_text": "Dataflow Analyses and Compiler Optimizations that Use Them, for Free \u2013\nEmbedded in Academia\n\nSkip to content\n\n# Embedded in Academia\n\n# Dataflow Analyses and Compiler Optimizations that Use Them, for Free\n\nCompilers can be improved over time, but this is a slow process. \u201cProebsting\u2019s\nLaw\u201d is an old joke which suggested that advances in compiler optimization\nwill double the speed of a computation every 18 years \u2014 but if anything this\nis optimistic. Slow compiler evolution is never a good thing, but this is\nparticularly problematic in today\u2019s environment of rapid innovation in GPUs,\nTPUs, and other entertaining platforms.\n\nOne of my research group\u2019s major goals is to create technologies that enable\nself-improving compilers. Taking humans out of the compiler-improvement loop\nwill make this process orders of magnitude faster, and also the resulting\ncompilers will tend to be correct by construction. One such technology is\nsuperoptimization, where we use an expensive search procedure to discover\noptimizations that are missing from a compiler. Another is generalization,\nwhich takes a specific optimization (perhaps, but not necessarily, discovered\nby a superoptimizer) and turns it into a broadly applicable form that is\nsuitable for inclusion in a production compiler.\n\nTogether with a representative benchmark suite, superoptimization +\ngeneralization will result in a fully automated self-improvement loop for one\npart of an optimizing compiler: the peephole optimizer. In the rest of this\npiece I\u2019ll sketch out an expanded version of this self-improvement loop that\nincludes dataflow analyses.\n\nThe goal of a dataflow analysis is to compute useful facts that are true in\nevery execution of the program being compiled. For example, if we can prove\nthat x is always in the range [5..15], then we don\u2019t need to emit an array\nbound check when x is used as an index into a 20-element array. This\nparticular dataflow analysis is the integer range analysis and compilers such\nas GCC and LLVM perform it during every optimizing compile. Another analysis \u2014\none that LLVM leans on particularly heavily \u2014 is \u201cknown bits,\u201d which tries to\nprove that individual bits of SSA values are zero or one in all executions.\n\nOut in the literature we can find a huge number of dataflow analyses, some of\nwhich are useful to optimize some kinds of code \u2014 but it\u2019s hard to know which\nones to actually implement. We can try out different ones, but it\u2019s a lot of\nwork implementing even one new dataflow analysis in a production compiler. The\neffort can be divided into two major parts. First, implementing the analysis\nitself, which requires creating an abstract version of each instruction in the\ncompiler\u2019s IR: these are called dataflow transfer functions. For example, to\nimplement the addition operation for integer ranges, we can use [lo1, hi1] +\n[lo2, hi2] = [lo1 + lo2, hi1 + hi2] as the transfer function. But even this\nparticularly easy case will become tricker if we have to handle overflows, and\nthen writing a correct and precise transfer function for bitwise operators is\nmuch less straightforward. Similarly, consider writing a correct and precise\nknown bits transfer function for multiplication. This is not easy! Then, once\nwe\u2019ve finished this job, we\u2019re left with the second piece of work which is to\nimplement optimizations that take advantage of the new dataflow facts.\n\nCan we automate both of these pieces of work? We can! There\u2019s an initial bit\nof work in creating a representation for dataflow facts and formalizing their\nmeaning that cannot be automated, but this is not difficult stuff. Then, to\nautomatically create the dataflow transfer functions, we turn to this very\nnice paper which synthesizes them basically by squeezing the synthesized code\nbetween a hard soundness constraint and a soft precision constraint.\nBasically, every dataflow analysis ends up making approximations, but these\napproximations can only be in one direction, or else analysis results can\u2019t be\nused to justify compiler optimizations. The paper leaves some work to be done\nin making this all practical in a production compiler, but it looks to me like\nthis should mainly be a matter of engineering.\n\nA property of dataflow transfer functions is that they lose precision across\ninstruction boundaries. We can mitigate this by finding collections of\ninstructions commonly found together (such as those implementing a minimum or\nmaximum operation) and synthesizing a transfer function for the aggregate\noperation. We can also gain back precision by special-casing the situation\nwhere both arguments to an instruction come from the same source. We don\u2019t\ntend to do these things when writing dataflow transfer functions by hand, but\nin an automated workflow they would be no problem at all. Another thing that\nwe\u2019d like to automate is creating efficient and precise product operators that\nallow dataflow analyses to exchange information with each other.\n\nGiven a collection of dataflow transfer functions, creating a dataflow\nanalysis is a matter of plugging them into a generic dataflow framework that\napplies transfer functions until a fixpoint is reached. This is all old hat.\nThe result of a dataflow analysis is a collection of dataflow facts attached\nto each instruction in a file that is being compiled.\n\nTo automatically make use of dataflow facts to drive optimizations, we can use\na superoptimizer. For example, we taught Souper to use several of LLVM\u2019s\ndataflow results. This is easy stuff compared to creating a superoptimizer in\nthe first place: basically, we can reuse the same formalization of the\ndataflow analysis that we already created in order to synthesize transfer\nfunctions. Then, the generalization engine also needs to fully support\ndataflow analyses; our Hydra tool already does a great job at this, there are\nplenty of details in the paper.\n\nNow that we\u2019ve closed the loop, let\u2019s ask whether there are interesting\ndataflow analyses missing from LLVM, that we should implement? Of course I\ndon\u2019t know for sure, but one such domain that I\u2019ve long been interested in\ntrying out is \u201ccongruences\u201d where for a variable v, we try to prove that it\nalways satisfies v = ax+b, for a pair of constants a and b. This sort of\ndomain is useful for tracking values that point into an array of structs,\nwhere a is the struct size and b is the offset of one of its fields.\n\nOur current generation of production compilers, at the implementation level,\nis somewhat divorced from the mathematical foundations of compilation. In the\nfuture we\u2019ll instead derive parts of compiler implementations \u2014 such as\ndataflow analyses and peephole optimizations \u2014 directly from these\nfoundations.\n\nApril 20, 2024\n\nregehr\n\nUncategorized\n\n### Leave a Reply\n\nEmbedded in Academia\n\nProudly powered by WordPress\n\n", "frontpage": true}
