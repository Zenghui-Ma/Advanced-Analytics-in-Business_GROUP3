{"aid": "40112202", "title": "Migrating a Trillion Entries of Uber's Ledger Data from DynamoDB to LedgerStore", "url": "https://www.uber.com/en-GB/blog/migrating-from-dynamodb-to-ledgerstore/", "domain": "uber.com", "votes": 1, "user": "unripe_syntax", "posted_at": "2024-04-22 07:50:32", "comments": 0, "source_title": "Migrating Uber's Ledger Data from DynamoDB to LedgerStore", "source_text": "Migrating Uber's Ledger Data from DynamoDB to LedgerStore | Uber Blog\n\nSkip to main content\n\nSchedule rides in advance\n\nReserve a rideReserve a ride\n\nSchedule rides in advance\n\nReserve a rideReserve a ride\n\nEngineering\n\n# Migrating a Trillion Entries of Uber\u2019s Ledger Data from DynamoDB to\nLedgerStore\n\n11 April / Global\n\nShare\n\n# Introduction\n\nLast week, we explored LedgerStore (LSG) \u2013 Uber\u2019s append-only, ledger-style\ndatabase. This week, we\u2019ll dive into how we migrated Uber\u2019s business-critical\nledger data to LSG. We\u2019ll detail how we moved more than a trillion entries\n(making up a few petabytes of data) transparently and without causing\ndisruption, and we\u2019ll discuss what we learned during the migration.\n\n### History\n\nGulfstream is Uber\u2019s payment platform. It was launched in 2017 using DynamoDB\nfor storage. At Uber\u2019s scale, DynamoDB became expensive. Hence, we started\nkeeping only 12 weeks of data (i.e., hot data) in DynamoDB and started using\nUber\u2019s blobstore, TerraBlob, for older data (i.e., cold data). TerraBlob is\nsimilar to AWS S3.\n\nFor a long-term solution, we wanted to use LSG. It was purpose-built for\nstoring payment-style data. Its key features are:\n\n  * It is verifiably immutable (i.e., you can check that records have not been altered using cryptographic signatures)\n  * Tiered storage to manage cost (the hot data is kept at a place that is best to serve requests and cold data is stored at a place that is optimized for storage)\n  * Better lag for eventually consistent secondary indexes\n\nSo, by 2021, Gulfstream was using a combination of DynamoDB, TerraBlob, and\nLSG to store data.\n\n  * DynamoDB for the last 12 weeks of data\n  * TerraBlob, Uber\u2019s internal blob store, for cold data\n  * LSG, where we were writing data, and wanted to migrate to it\n\n## Why Migrate?\n\nLSG is better suited for storing ledger-style data because of its\nimmutability. The recurring cost savings by moving to LSG were significant.\n\nGoing from three to a single storage would simplify the code and design of the\nGulfstream services responsible for interacting with storage and creating\nindexes. This in turn makes it easy to understand and maintain the services.\n\nLSG promised shorter indexing lag (i.e., time between when a record is written\nand its secondary index is created). Additionally, it would give us faster\nnetwork latency because it was running on-premises within Uber\u2019s data centers.\n\nFigure 1: Data flow before and after the migration\n\n## Nature of Data & Associated Risk\n\nThe data we were migrating is all of Uber\u2019s ledger data for all of Uber\u2019s\nbusiness since 2017:\n\n  * Immutable records \u2013 1.2 PB compressed size\n  * Secondary indexes \u2013 0.5 PB uncompressed size\n\nImmutable records should not be modified. So, for all practical purposes, once\nwe have written a record, it can\u2019t be changed. We do have the flexibility of\nmodifying secondary index data for correcting problems.\n\n## Checks\n\nTo ensure that the backfill is correct and acceptable in all respects, we need\nto check that we can handle the current traffic and the data that is not being\naccessed currently is correct. The criteria for this was:\n\n  * Completeness: All the records were backfilled.\n  * Correctness: All the records were correct.\n  * Load: LSG should be able to handle current load.\n  * Latency: The P99 latency of LSG was within acceptable bounds.\n  * Lag: The secondary indexes are created in the background. We want to make sure that the delay of the index creation process was within acceptable limits.\n\nThe checks were done using a combination of shadow validation and offline\nvalidation.\n\n### Shadow Validation\n\nThis compares the response that we had been returning before migration with\nthe one that we would return with the LSG as data source. This helps us ensure\nthat our current traffic will be disrupted by neither data migration issues\nnor code bugs. We wanted our backfill to be at least 99.99% complete and\ncorrect as measured by shadow validation. We also had a 99.9999% upper bound\nfor the same. The reason for having an upper bound are:\n\n  * When migrating historical data, there are always data corruption issues. Sometimes this is because data was not written correctly during the initial development time of the service. It is also possible to see data corruption because of scale. As an example, S3 gives 11 nines of durability guarantee then you can expect 10 corruptions in 1 trillion records.\n  * Indexes are eventually consistent, which means that some records will appear after a few seconds. So, the shadow validation will flag them as missing. This is a false positive that shows up at a large scale.\n  * For 6 nines, you have to look at data of 100 million comparisons to give any results with good confidence. This means if your shadow validation is comparing 1,000 records/second, then you need to wait for a bit more than one day just to collect sufficient data. With 7 nines, you will have to wait 12 days. In practical terms this would slow the project to a halt.\n  * With a well-defined upper bound, you are not forced to look at every potential issue that you suspect. Say if the occurrence of a problem is 1/10 of the upper bound, you need not even investigate it.\n  * With 6 nines, we could end up with slightly more than 1 million corrupt records. Even though 6 nines of confirmed correctness could mean a real cost to the company, the savings generated by this project outweighed the potential cost.\n\nDuring shadow validation you are essentially duplicating production traffic on\nLSG. So by monitoring LSG, we can verify that it can handle our production\ntraffic while meeting our latency and lag requirements. It gives us good\nconfidence in the code that we wrote for accessing the data from LSG.\nAdditionally, it also gives us some confidence about completeness and\ncorrectness of data, particularly with data that is currently being accessed.\nWe developed a single generic shadow validation code that was reused multiple\ntimes for different parts of the migration.\n\nDuring the migration process we found latency and lag issues because of\nmultiple bugs in different parts and fixed them.\n\n  * Partition key optimization for better distribution of index data\n  * Index issues causing scan of the record instead of point lookup\n\nUnfortunately, live shadow validation can\u2019t give strong guarantees about our\ncorpus of rarely-accessed historical data.\n\n### Offline Validation & Incremental Backfill\n\nThis compares complete data from the LSG with the data dump from DynamoDB.\nBecause of various data issues, you have to skip over bad records to ensure\nthat your backfill can go through. Additionally, there can be bugs in the\nbackfill job itself. Offline validation ensures that the data backfill has\nhappened correctly and it covers complete data. This has to be done in\naddition to shadow validation because live traffic tends to access only recent\ndata. So, if there are any problems lurking in the cold data that is\ninfrequently accessed, it will not be caught by shadow validation.\n\nThe key challenge in offline validation is size of data. The biggest data that\nwe tackled was 70 TB compressed (estimated 300 TB uncompressed) in size and we\ncompared 760 billion records in a single job. This type of Apache Spark^TM job\nrequires data shuffling and Distributed Shuffle as a Service for Spark\ncombined with Dynamic Resource Allocation and Speculative Execution let us do\nexactly that at a reasonable speed under resource constraints.\n\nOffline validation found missing records and its output was used for\nincremental backfill. We iterated between offline validation and backfill to\nensure that all the records were written.\n\n## Backfill Issues\n\nEvery backfill is risky. We used Uber\u2019s internal offering of Apache Spark for\nthe backfills. Here are the different problems that we encountered and how we\nhandled them.\n\n### Scalability\n\nYou want to start at a small scale and scale up gradually till you hit the\nlimit of the system. If you just blindly push beyond this point then you are\neffectively creating a DDoS attack on your own systems. At this point, you\nwant to find the bottleneck, address it, and then scale up your job. Most of\nthe time it\u2019s just a matter of scaling up downstream services, other times it\ncan be something more complex. In either case, you don\u2019t want to scale your\nbackfill job beyond the capability of the bottleneck of the system. It\u2019s a\ngood idea to scale up in small increments and monitor closely after each\nscale-up.\n\n### Incremental Backfills\n\nWhen you try to backfill 3 years\u2019 worth of data in say 3 months, you are\ngenerating traffic that puts 10x the normal traffic load and the system may\nnot be able to cope with this traffic. As an example, you will need 120 days\nto backfill 100B records at 10K/sec rate when your production normally handles\n1K/sec rate. So, you can expect the system to get overloaded. If there is even\na remote chance of the backfill job causing an ongoing problem, you must shut\nit down. So, it is unrealistic to expect that a backfill job can run from\nstart to finish in one go, and therefore you have to run backfills\nincrementally.\n\nA simple and effective way to do this is to break the backfill into small\nbatches that can be done one by one, such that each batch can complete within\na few minutes. Since your job may shut down in the middle of a batch, it has\nto be idempotent. Every time you complete a batch you want to dump the\nstatistics (such as records read, records backfilled, etc.) to a file. As your\nbackfill continues, you can aggregate numbers from them to check the progress.\n\nIf you can delete or update existing records, it lowers the risk and cost of\nmistakes and code bugs during the backfill.\n\n### Rate Control\n\nTo backfill safely, you want to make sure that your backfill job behaves\nconsistently. So, your job should have rate control that can be easily tweaked\nto scale up or scale down. In Java/Scala you can use Guava\u2019s RateLimiter.\n\n### Dynamic Rate Control\n\nIn some cases, you may be able to go faster when there is less production\ntraffic. For this you need to monitor the current state of the system and see\nif it\u2019s ok to go faster. We adjusted RPS on the lines of additive\nincrease/multiplicative decrease. We still had an upper bound on the traffic\nfor safety.\n\n### Emergency Stop\n\nThe migration process needs the ability to stop backfill quickly in case there\nis an outage or even suspicion of overload. Any backfill during an outage has\nto be stopped as both a precaution and as a potential source of noise. Even\npost-outage, systems tend to get extra load as systems recover. Having the\nability to stop backfill also helps debug scale-related issues.\n\n### Size of Data File\n\nWhen dumping data, keep the size of the files to around 1GB with 10x\nflexibility on both sides. If the size of the file is too big, you run into\nissues such as MultiPart limitation of different tools. If your file size is\nsmall, then you have too many files and even listing them will take\nsignificant time. You may even start hitting ARGMAX limit of when running\ncommands in a shell. This becomes significant enough to make sure that every\ntime you do something with data it has been applied to all files and not just\nsome of them.\n\n### Fault Tolerance\n\nAll backfill jobs need some kind of data transformation. When you do this you\ninevitably run into data quality/corruption issues. You can\u2019t stop the\nbackfill job every time this happens because such bad records tend to be\nrandomly distributed. But you can\u2019t ignore them as well because it might also\nbe because of a code bug. To deal with this, you dump problematic records\nseparately and monitor statistics. If the failure rate is high then you can\nstop the backfill manually, fix the problem, and continue. Otherwise, let the\nbackfill continue and look at the failures in parallel.\n\nAnother reason for records not getting written is RPC timeout. You can retry\nfor this, but at some point, you have to give up and move ahead irrespective\nof the reason to make sure you can make progress.\n\n### Logging\n\nIt is tempting to log during backfill to help with debugging and monitor\nprogress, but this may not be possible because of the pressure that it will\nput on the logging infrastructure. Even if you can keep logs, there will be\ntoo much log data to keep around. The solution is to use a rate limiter to\nlimit the amount of logs that you are producing. You need to rate limit only\nthe parts that produce most of the logs. You can even choose to log all the\nerrors if they happen infrequently.\n\n## Mitigating Risk\n\nIn addition to analyzing data from different validation and backfill stats we\nalso were conservative with the rollout of LSG. We rolled it out over a few\nweeks and with go-aheads from on-call engineers of the major callers of our\nservice. We initially rolled out with fallback (i.e., if the data was not\nfound in LSG, we would try to fetch it from DynamoDB). We looked at the\nfallback logs before we removed the fallback. For every record that was\nflagged as missing in the fallback logs we checked LSG to make sure that it\nwas not really missing. Even after that we kept the DynamoDB data around for a\nmonth before we stopped writing data to it, took a final backup, and dropped\nthe table.\n\nFigure 2: LSG Rollout\n\n# Conclusion\n\nIn this article, we covered the migration of massive amounts of business-\ncritical money data from one datastore to another. We covered different\naspects of the migration, including criteria for migration, checks, backfill\nissues, and safety. We were able to do this migration over two years without\nany downtime or outages during or after the migration.\n\n### Acknowledgments\n\nThanks to Amit Garg and Youxian Chen for helping us migrate the data from\nTerraBlob to LSG. Thanks to Jaydeepkumar Chovatia, Kaushik Devarajaiah, and\nRashmi Gupta from the LSG team for supporting us throughout this work. Thanks\nto Menghan Li for migrating data for Uber Cash\u2019s ledger.\n\nCover photo attribution: \u201cWaterfowl Migration at Sunset on the Huron Wetland\nManagement District\u201d by USFWS Mountain Prairie is marked with Public Domain\nMark 1.0.\n\nAmazon Web Services, AWS, and the Powered by AWS logo are trademarks of\nAmazon.com, Inc. or its affiliates.\n\nApache\u00ae, Apache SparkTM, and SparkTM are either registered trademarks or\ntrademarks of the Apache Software Foundation in the United States and/or other\ncountries. No endorsement by The Apache Software Foundation is implied by the\nuse of these marks.\n\nRaghav Gautam\n\nRaghav Kumar Gautam is a Staff Software Engineer on the Money Data and\nInsights teams at Uber. He primarily focuses on data related problems for the\nMoney Team. Raghav holds a Master\u2019s degree in Internet Science and Engineering\nfrom Indian Institute of Science, Bengaluru.\n\nErik Seaberg\n\nErik Seaberg is a former Staff Software Engineer who joined Uber to focus on\ntransaction storage and reporting at scale. He has a Bachelor\u2019s degree in\nComputer Science from the University of Washington in Seattle.\n\nAbhishek Kanhar\n\nAbhishek Kanhar is a Senior Software Engineer at Uber. Currently, he focuses\non developing a scalable and unified architecture for generating reports\ntailored to Uber\u2019s large enterprise partners. Abhishek holds a bachelor\u2019s\ndegree in computer science from the Indian Institute of Technology, Roorkee.\n\nPosted by Raghav Gautam, Erik Seaberg, Abhishek Kanhar\n\nCategory:\n\nEngineering\n\n### Related articles\n\nEngineering\n\n##### Ensuring Precision and Integrity: A Deep Dive into Uber\u2019s Accounting\nData Testing Strategies\n\n18 April / Global\n\nEngineering, Backend, Data / ML\n\n##### How LedgerStore Supports Trillions of Indexes at Uber\n\n4 April / Global\n\nEngineering, AI, Data / ML\n\n##### Scaling AI/ML Infrastructure at Uber\n\n28 March / Global\n\nEngineering, Data / ML\n\n##### Model Excellence Scores: A Framework for Enhancing the Quality of\nMachine Learning Systems at Scale\n\n21 March / Global\n\nEngineering, Data / ML\n\n##### Balancing HDFS DataNodes in the Uber DataLake\n\n14 March / Global\n\n## Most popular\n\nMerchants, Restaurants19 February / United Kingdom\n\n###### Your guide to Order Error Adjustments & Best Practices\n\nEngineering, Backend, Web20 February / Global\n\n###### Building Scalable, Real-Time Chat to Improve Customer Experience\n\nRide23 February / United Kingdom\n\n###### How Uber works with the Police to report crime\n\nBusiness23 February / United Kingdom\n\n###### Uber for Business | 2024 UK Business travel trends outlook\n\nView more stories\n\nUber\n\nVisit Help Center\n\nDownload Uber Modern Slavery Act Transparency Statement\n\nDownload Uber Eats Modern Slavery Act Transparency Statement\n\nCompany\n\nAbout us\n\nOur offerings\n\nNewsroom\n\nInvestors\n\nBlog\n\nCareers\n\nAI\n\nGift cards\n\nProducts\n\nRide\n\nDrive\n\nDeliver\n\nEat\n\nUber for Business\n\nUber Freight\n\nGlobal citizenship\n\nSafety\n\nDiversity and Inclusion\n\nSustainability\n\nTravel\n\nAirports\n\nCities\n\n\u00a9 2024 Uber Technologies Inc.\n\nPrivacy\n\nAccessibility\n\nTerms\n\n  * ### Products\n\n    * Earn\n\nResources for driving and delivering with Uber\n\n    * Ride\n\nExperiences and information for people on the move\n\n    * Restaurants\n\nInspiration and product details for the places that feed us\n\n    * Merchants\n\nPutting stores within reach of a world of customers\n\n    * Business\n\nTransforming the way companies move and feed their people\n\n    * Transit\n\nExpanding the reach of public transportation\n\n  * ### Company\n\n    * Careers\n\nExplore how Uber employees from around the globe are helping us drive the\nworld forward at work and beyond\n\n    * Engineering\n\nThe technology behind Uber Engineering\n\n    * Newsroom\n\nUber news and updates in your country\n\n    * Uber.com\n\nProduct, how-to, and policy content\u2014and more\n\n## Sign up to drive\n\n## Sign up to ride\n\n", "frontpage": false}
