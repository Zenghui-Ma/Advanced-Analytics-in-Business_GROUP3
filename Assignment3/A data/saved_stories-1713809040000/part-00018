{"aid": "40112098", "title": "Measuring Developer Productivity via Humans", "url": "https://martinfowler.com/articles/measuring-developer-productivity-humans.html", "domain": "martinfowler.com", "votes": 1, "user": "BerislavLopac", "posted_at": "2024-04-22 07:25:35", "comments": 0, "source_title": "Measuring Developer Productivity via Humans", "source_text": "Measuring Developer Productivity via Humans\n\n# Measuring Developer Productivity via Humans\n\nMeasuring developer productivity is a difficult challenge. Conventional\nmetrics focused on development cycle time and throughput are limited, and\nthere aren't obvious answers for where else to turn. Qualitative metrics offer\na powerful way to measure and understand developer productivity using data\nderived from developers themselves. Organizations should prioritize measuring\ndeveloper productivity using data from humans, rather than data from systems.\n\n19 March 2024\n\nAbi Noda\n\nAbi Noda is the founder and CEO of DX, focused on helping organizations\nmeasure and improve developer productivity. As a programmer and researcher,\nAbi regularly publishes content and research on developer experience. Prior to\nDX, Abi was the founder and CEO of Pull Panda, which was acquired by GitHub in\n2019.\n\nTim Cochran\n\nTim Cochran is a Principal in Amazon\u2019s Software Builder Experience (ASBX)\ngroup. He was previously a Technical Director at Thoughtworks.\n\nTim has over 20 years of experience working with both scaleups and\nenterprises. He advises on technology strategy and making the right technology\ninvestments to enable digital transformation goals. He is a vocal advocate for\nthe developer experience and passionate about using data-driven approaches to\nimprove it.\n\nmetrics\n\nproductivity\n\n## Contents\n\n  * What is a qualitative metric?\n  * Advocating for qualitative metrics\n\n    * Misconception: Qualitative data is only subjective\n    * Misconception: Qualitative data is unreliable\n  * The two types of qualitative metrics\n  * The benefits of qualitative metrics\n\n    * Qualitative metrics allow you to measure things that are otherwise unmeasurable\n    * Qualitative metrics provide missing visibility across teams and systems\n    * Qualitative metrics provide context for quantitative data\n  * How to capture qualitative metrics\n\n    * Segment results by team and persona\n    * Free text comments are often most valuable\n    * Compare results against benchmarks\n    * Use transactional surveys where appropriate\n    * Avoid survey fatigue\n    * Survey Template\n  * Using qualitative and quantitative metrics together\n\nSomewhere, right now, a technology executive tells their directors: \u201cwe need a\nway to measure the productivity of our engineering teams.\u201d A working group\nassembles to explore potential solutions, and weeks later, proposes\nimplementing the metrics: lead time, deployment frequency, and number of pull\nrequests created per engineer.\n\nSoon after, senior engineering leaders meet to review their newly created\ndashboards. Immediately, questions and doubts are raised. One leader says:\n\u201cOur lead time is two days which is \u2018low performing\u2019 according to those\nbenchmarks \u2013 but is there actually a problem?\u201d. Another leader says: \u201cit\u2019s\nunsurprising to see that some of our teams are deploying less often than\nothers. But I\u2019m not sure if this spells an opportunity for improvement.\u201d\n\nIf this story arc is familiar to you, don\u2019t worry \u2013 it's familiar to most,\nincluding some of the biggest tech companies in the world. It is not uncommon\nfor measurement programs to fall short when metrics like DORA fail to provide\nthe insights leaders had hoped for.\n\nThere is, however, a better approach. An approach that focuses on capturing\ninsights from developers themselves, rather than solely relying on basic\nmeasures of speed and output. We\u2019ve helped many organizations make the leap to\nthis human-centered approach. And we\u2019ve seen firsthand the dramatically\nimproved understanding of developer productivity that it provides.\n\nWhat we are referring to here is qualitative measurement. In this article, we\nprovide a primer on this approach derived from our experience helping many\norganizations on this journey. We begin with a definition of qualitative\nmetrics and how to advocate for them. We follow with practical guidance on how\nto capture, track, and utilize this data.\n\nToday, developer productivity is a critical concern for businesses amid the\nbackdrop of fiscal tightening and transformational technologies such as AI. In\naddition, developer experience and platform engineering are garnering\nincreased attention as enterprises look beyond Agile and DevOps\ntransformation. What all these concerns share is a reliance on measurement to\nhelp guide decisions and track progress. And for this, qualitative measurement\nis key.\n\nNote: when we say \u201cdeveloper productivity\u201d, we mean the degree to which\ndevelopers' can do their work in a frictionless manner \u2013 not the individual\nperformance of developers. Some organizations find \u201cdeveloper productivity\u201d to\nbe a problematic term because of the way it can be misinterpreted by\ndevelopers. We recommend that organizations use the term \u201cdeveloper\nexperience,\u201d which has more positive connotations for developers.\n\n## What is a qualitative metric?\n\nWe define a qualitative metric as a measurement comprised of data provided by\nhumans. This is a practical definition \u2013 we haven\u2019t found a singular\ndefinition within the social sciences, and the alternative definitions we\u2019ve\nseen have flaws that we discuss later in this section.\n\nFigure 1: Qualitative metrics are measurements derived from humans\n\nThe definition of the word \u201cmetric\u201d is unambiguous. The term \u201cqualitative,\u201d\nhowever, has no authoritative definition as noted in the 2019 journal paper\nWhat is Qualitative in Qualitative Research:\n\n> There are many definitions of qualitative research, but if we look for a\n> definition that addresses its distinctive feature of being \u201cqualitative,\u201d\n> the literature across the broad field of social science is meager. The main\n> reason behind this article lies in the paradox, which, to put it bluntly, is\n> that researchers act as if they know what it is, but they cannot formulate a\n> coherent definition.\n\nAn alternate definition we\u2019ve heard is that qualitative metrics measure\nquality, while quantitative metrics measure quantity. We\u2019ve found this\ndefinition problematic for two reasons: first, the term \u201cqualitative metric\u201d\nincludes the term metric, which implies that the output is a quantity (i.e., a\nmeasurement). Second, quality is typically measured through ordinal scales\nthat are translated into numerical values and scores \u2013 which again,\ncontradicts the definition.\n\nAnother argument we have heard is that the output of sentiment analysis is\nquantitative because the analysis results in numbers. While we agree that the\ndata resulting from sentiment analysis is quantitative, based on our original\ndefinition this is still a qualitative metric (i.e., a quantity produced\nqualitatively) unless one were to take the position that \u201cqualitative metric\u201d\nis altogether an oxymoron.\n\nAside from the problem of defining what a qualitative metric is, we\u2019ve also\nencountered problematic colloquialisms. One example is the term \u201csoft metric\u201d.\nWe caution against this phrase because it harmfully and incorrectly implies\nthat data collected from humans is weaker than \u201chard metrics\u201d collected from\nsystems. We also discourage the term \u201csubjective metrics\u201d because it\nmisconstrues the fact that data collected from humans can be either objective\nor subjective \u2013 as we discuss in the next section.\n\nQualitative metrics: Measurements derived from humansType| Definition| Example  \n---|---|---  \nAttitudinal metrics| Subjective feelings, opinions, or attitudes toward a\nspecific subject.| How satisfied are you with your IDE, on a scale of 1\u201310?  \nBehavioral metrics| Objective facts or events pertaining to an individual's\nwork experience.| How long does it take for you to deploy a change to\nproduction?  \n  \nLater in this article we provide guidance on how to collect and use these\nmeasurements, but first we\u2019ll provide a real-world example of this approach\nput to practice\n\nPeloton is an American technology company whose developer productivity\nmeasurement strategy centers around qualitative metrics. To collect\nqualitative metrics, their organization runs a semi-annual developer\nexperience survey led by their Tech Enablement & Developer Experience team,\nwhich is part of their Product Operations organization.\n\nThansha Sadacharam, head of tech learning and insights, explains: \u201cI very\nstrongly believe, and I think a lot of our engineers also really appreciate\nthis, that engineers aren't robots, they're humans. And just looking at basic\nnumbers doesn't drive the whole story. So for us, having a really\ncomprehensive survey that helped us understand that entire developer\nexperience was really important.\u201d\n\nEach survey is sent to a random sample of roughly half of their developers.\nWith this approach, individual developers only need to participate in one\nsurvey per year, minimizing the overall time spent on filling out surveys\nwhile still providing a statistically significant representative set of data\nresults. The Tech Enablement & Developer Experience team is also responsible\nfor analyzing and sharing the findings from their surveys with leaders across\nthe organization.\n\nFor more on Peloton\u2019s developer experience survey, listen to this interview\nwith Thansha Sadacharam.\n\n## Advocating for qualitative metrics\n\nExecutives are often skeptical about the reliability or usefulness of\nqualitative metrics. Even highly scientific organizations like Google have had\nto overcome these biases. Engineering leaders are inclined toward system\nmetrics since they are accustomed to working with telemetry data for\ninspecting systems. However, we cannot rely on this same approach for\nmeasuring people.\n\nAvoid pitting qualitative and quantitative metrics against each other.\n\nWe\u2019ve seen some organizations get into an internal \u201cbattle of the metrics\u201d\nwhich is not a good use of time or energy. Our advice for champions is to\navoid pitting qualitative and quantitative metrics against each other as an\neither/or. It\u2019s better to make the argument that they are complementary tools\n\u2013 as we cover at the end of this article.\n\nWe\u2019ve found that the underlying cause of opposition to qualitative data are\nmisconceptions which we address below. Later in this article, we outline the\ndistinct benefits of self-reported data such as its ability to measure\nintangibles and surface critical context.\n\n### Misconception: Qualitative data is only subjective\n\nTraditional workplace surveys typically focus on the subjective opinions and\nfeelings of their employees. Thus many engineering leaders intuitively believe\nthat surveys can only collect subjective data from developers.\n\nAs we describe in the following section, surveys can also capture objective\ninformation about facts or events. Google\u2019s DevOps Research and Assessment\n(DORA) program is an excellent concrete example.\n\nSome examples of objective survey questions:\n\n  * How long does it take to go from code committed to code successfully running in production?\n  * How often does your organization deploy code to production or release it to end users?\n\n### Misconception: Qualitative data is unreliable\n\nOne challenge of surveys is that people with all manner of backgrounds write\nsurvey questions with no special training. As a result, many workplace surveys\ndo not meet the minimum standards needed to produce reliable or valid\nmeasures. Well designed surveys, however, produce accurate and reliable data\n(we provide guidance on how to do this later in the article).\n\nSome organizations have concerns that people may lie in surveys. Which can\nhappen in situations where there is fear around how the data will be used. In\nour experience, when surveys are deployed as a tool to help understand and\nimprove bottlenecks affecting developers, there is no incentive for\nrespondents to lie or game the system.\n\nWhile it\u2019s true that survey data isn\u2019t always 100% accurate, we often remind\nleaders that system metrics are often imperfect too. For example, many\norganizations attempt to measure CI build times using data aggregated from\ntheir pipelines, only to find that it requires significant effort to clean the\ndata (e.g. excluding background jobs, accounting for parallel jobs) to produce\nan accurate result\n\n## The two types of qualitative metrics\n\nThere are two key types of qualitative metrics:\n\n  1. Attitudinal metrics capture subjective feelings, opinions, or attitudes toward a specific subject. An example of an attitudinal measure would be the numeric value captured in response to the question: \u201cHow satisfied are you with your IDE, on a scale of 1-10?\u201d.\n  2. Behavioral metrics capture objective facts or events pertaining to an individuals\u2019 work experiences. An example of a behavioral measure would be the quantity captured in response to the question: \u201cHow long does it take for you to deploy a change to production?\u201d\n\nWe\u2019ve found that most tech practitioners overlook behavioral measures when\nthinking about qualitative metrics. This occurs despite the prevalence of\nqualitative behavioral measures in software research, such as the Google\u2019s\nDORA program mentioned earlier.\n\nDORA publishes annual benchmarks for metrics such as lead time for changes,\ndeployment frequency, and change fail rate. Unbeknownst to many, DORA\u2019s\nbenchmarks are captured using qualitative methods with the survey items shown\nbelow:\n\nLead time\n\nFor the primary application or service you work on, what is your lead time for\nchanges (that is, how long does it take to go from code committed to code\nsuccessfully running in production)?\n\nMore than six months\n\nOne to six months\n\nOne week to one month\n\nOne day to one week\n\nLess than one day\n\nLess than one hour\n\nDeploy frequency\n\nFor the primary application or service you work on, how often does your\norganization deploy code to production or release it to end users?\n\nFewer than once per six months\n\nBetween once per month and once every six months\n\nBetween once per week and once per month\n\nBetween once per day and once per week\n\nBetween once per hour and once per day\n\nOn demand (multiple deploys per day)\n\nChange fail percentage\n\nFor the primary application or service you work on, what percentage of changes\nto production or releases to users result in degraded service (for example,\nlead to service impairment or service outage) and subsequently require\nremediation (for example, require a hotfix, rollback, fix forward, patch)?\n\n0\u201315%\n\n16\u201330%\n\n31\u201345%\n\n46\u201360%\n\n61\u201375%\n\n76\u2013100%\n\nTime to restore\n\nFor the primary application or service you work on, how long does it generally\ntake to restore service when a service incident or a defect that impacts users\noccurs (for example, unplanned outage, service impairment)?\n\nMore than six months\n\nOne to six months\n\nOne week to one month\n\nOne day to one week\n\nLess than one day\n\nLess than one hour\n\nWe\u2019ve found that the ability to collect attitudinal and behavioral data at the\nsame time is a powerful benefit of qualitative measurement.\n\nFor example, behavioral data might show you that your release process is fast\nand efficient. But only attitudinal data could tell you whether it is smooth\nand painless, which has important implications for developer burnout and\nretention.\n\nTo use a non-tech analogy: imagine you are feeling sick and visit a doctor.\nThe doctor takes your blood pressure, your temperature, your heart rate, and\nthey say \u201cWell, it looks like you\u2019re all good. There\u2019s nothing wrong with\nyou.\u201d You would be taken aback! You'd say, \"Wait, I\u2019m telling you that\nsomething feels wrong.\u201d\n\n## The benefits of qualitative metrics\n\nOne argument for qualitative metrics is that they avoid subjecting developers\nto the feeling of \u201cbeing measured\u201d by management. While we\u2019ve found this to be\ntrue \u2013 especially when compared to metrics derived from developers\u2019 Git or\nJira data \u2013 it doesn\u2019t address the main objective benefits that qualitative\napproaches can provide.\n\nThere are three main benefits of qualitative metrics when it comes to\nmeasuring developer productivity:\n\n### Qualitative metrics allow you to measure things that are otherwise\nunmeasurable\n\nSystem metrics like lead time and deployment volume capture what\u2019s happening\nin our pipelines or ticketing systems. But there are many more aspects of\ndevelopers\u2019 work that need to be understood in order to improve productivity:\nfor example, whether developers are able to stay in the flow or work or easily\nnavigate their codebases. Qualitative metrics let you measure these\nintangibles that are otherwise difficult or impossible to measure.\n\nAn interesting example of this is technical debt. At Google, a study to\nidentify metrics for technical debt included an analysis of 117 metrics that\nwere proposed as potential indicators. To the disappointment of Google\nresearchers, no single metric or combination of metrics were found to be valid\nindicators (for more on how Google measures technical debt, listen to this\ninterview).\n\nWhile there may exist an undiscovered objective metric for technical debt, one\ncan suppose that this may be impossible due to the fact that assessment of\ntechnical debt relies on the comparison between the current state of a system\nor codebase versus its imagined ideal state. In other words, human judgment is\nessential.\n\n### Qualitative metrics provide missing visibility across teams and systems\n\nMetrics from ticketing systems and pipelines give us visibility into some of\nthe work that developers do. But this data alone cannot give us the full\nstory. Developers do a lot of work that\u2019s not captured in tickets or builds:\nfor example, designing key features, shaping the direction of a project, or\nhelping a teammate get onboarded.\n\nIt\u2019s impossible to gain visibility into all these activities through data from\nour systems alone. And even if we could theoretically collect all the data\nthrough systems, there are additional challenges to capturing metrics through\ninstrumentation.\n\nOne example is the difficulty of normalizing metrics across different team\nworkflows. For example, if you\u2019re trying to measure how long it takes for\ntasks to go from start to completion, you might try to get this data from your\nticketing tool. But individual teams often have different workflows that make\nit difficult to produce an accurate metric. In contrast, simply asking\ndevelopers how long tasks typically take can be much simpler.\n\nAnother common challenge is cross-system visibility. For example, a small\nstartup can measure TTR (time to restore) using just an issue tracker such as\nJira. A large organization, however, will likely need to consolidate and\ncross-attribute data across planning systems and deployment pipelines in order\nto gain end-to-end system visibility. This can be a yearlong effort, whereas\ncapturing this data from developers can provide a baseline quickly.\n\n### Qualitative metrics provide context for quantitative data\n\nAs technologists, it is easy to focus heavily on quantitative measures. They\nseem clean and clear, afterall. There is a risk, however, that the full story\nisn\u2019t being told without richer data and that this may lead us into focusing\non the wrong thing.\n\nOne example of this is code review: a typical optimization is to try to speed\nup the code review. This seems logical as waiting for a code review can cause\nwasted time or unwanted context switching. We could measure the time it takes\nfor reviews to be completed and incentivize teams to improve it. But this\napproach may encourage negative behavior: reviewers rushing through reviews or\ndevelopers not finding the right experts to perform reviews.\n\nCode reviews exist for an important purpose: to ensure high quality software\nis delivered. If we do a more holistic analysis \u2013 focusing on the outcomes of\nthe process rather than just speed \u2013 we find that optimization of code review\nmust ensure good code quality, mitigation of security risks, building shared\nknowledge across team members, as well as ensuring that our coworkers aren\u2019t\nstuck waiting. Qualitative measures can help us assess whether these outcomes\nare being met.\n\nAnother example is developer onboarding processes. Software development is a\nteam activity. Thus if we only measure individual output metrics such as the\nrate new developers are committing or time to first commit, we miss important\noutcomes e.g. whether we are fully utilizing the ideas the developers are\nbringing, whether they feel safe to ask questions and if they are\ncollaborating with cross-functional peers.\n\n## How to capture qualitative metrics\n\nMany tech practitioners don\u2019t realize how difficult it is to write good survey\nquestions and design good survey instruments. In fact, there are whole fields\nof study related to this, such as psychometrics and industrial psychology. It\nis important to bring or build expertise here when possible.\n\nBelow are few good rules for writing surveys to avoid the most common mistakes\nwe see organizations make:\n\n  * Survey items need to be carefully worded and every question should only ask one thing.\n  * If you want to compare results between surveys, be careful about changing the wording of questions such that you\u2019re measuring something different.\n  * If you change any wording, you must do rigorous statistical tests.\n\nIn survey parlance, \u201dgood surveys\u201d means \u201cvalid and reliable\u201d or\n\u201cdemonstrating good psychometric properties.\u201d Validity is the degree to which\na survey item actually measures the construct you desire to measure.\nReliability is the degree to which a survey item produces consistent results\nfrom your population and over time.\n\nOne way of thinking about survey design that we\u2019ve found helpful to tech\npractitioners: think of the survey response process as an algorithm that takes\nplace in the human mind.\n\nWhen an individual is presented a survey question, a series of mental steps\ntake place in order to arrive at a response. The model below is from the\nseminal 2012 book, The Psychology of Survey Response:\n\nComponents of the Response ProcessComponent| Specific Processes  \n---|---  \nComprehension| Attend to questions and instructionsRepresent logical form of\nquestionIdentify question focus (information sought)Link key terms to relevant\nconcepts  \nRetrieval| Generate retrieval strategy and cuesRetrieve specific, generic\nmemoriesFill in missing details  \nJudgment| Assess completeness and relevance of memoriesDraw inferences based\non accessibilityIntegrate material retrievedMake estimate based on partial\nretrieval  \nResponse| Map Judgement onto response categoryEdit response  \n  \nDecomposing the survey response process and inspecting each step can help us\nrefine our inputs to produce more accurate survey results. Developing good\nsurvey items requires rigorous design, testing, and analysis \u2013 just like the\nprocess of designing software!\n\nBut good survey design is just one aspect of running successful surveys.\nAdditional challenges include participation rates, data analysis, and knowing\nhow to act on data. Below are some of the best practices we\u2019ve learned.\n\n### Segment results by team and persona\n\nA common mistake made by organizational leaders is to focus on companywide\nresults instead of data broken down by team and persona (e.g., role, tenure,\nseniority). As previously described, developer experience is highly contextual\nand can differ radically across teams or roles. Focusing only on aggregate\nresults can lead to overlooking problems that affect small but important\npopulations within the company, such as mobile developers.\n\n### Free text comments are often most valuable\n\nWe\u2019ve been talking about qualitative metrics but free text comments are an\nextremely valuable form of qualitative data. Beyond describing the friction or\nworkflow, developers will have many great ideas to improve their developer\nexperience, the free text allows us to capture those, and identify who to\nfollow up with. Free text comments can also surface areas that your survey did\nnot cover, which could be added in the future.\n\n### Compare results against benchmarks\n\nComparative analysis can help contextualize data and help drive action. For\nexample, developer sentiment toward code quality commonly skews negative,\nmaking it difficult to identify true problems or gauge their magnitude. The\nmore actionable data point is: \u201care our developers more frustrated about code\nquality than other teams or organizations?\u201d Teams with lower sentiment scores\nthan their peers and organizations with lower scores than their industry peers\ncan surface notable opportunities for improvement.\n\n### Use transactional surveys where appropriate\n\nTransactional surveys capture feedback during specific touchpoints or\ninteractions in the developer workflow. For example, platform teams can use\ntransactional surveys to prompt developers for feedback while they are in the\nmidst of creating a new service in an internal developer portal. Transactional\nsurveys can also augment data from periodic surveys by producing higher-\nfrequency feedback and more granular insights.\n\n### Avoid survey fatigue\n\nMany organizations struggle to sustain high participation rates in surveys\nover time. Lack of follow-up can cause developers to feel that repeatedly\nresponding to surveys is not worthwhile. It is therefore critical that leaders\nand teams follow up and take meaningful action after surveys. While a\nquarterly or semi-annual survey cadence is optimal for most organizations,\nwe\u2019ve seen some organizations be successful with more frequent surveys that\nare integrated into regular team rituals such as retrospectives.\n\n### Survey Template\n\nBelow are a simple set of survey questions for getting started. Load the\nquestions below into your preferred survey tool, or get started quickly by\nmaking a copy of our ready-to-go Google Forms template.\n\nThe template is intentionally simple, but surveys often become quite sizable\nas your measurement strategy matures. For example, Shopify's developer survey\nis 20-minutes long and Google's is over 30-minutes long.\n\nAfter you've collected responses, score the multiple choice questions using\neither mean or top box scoring. Mean scores are calculated by assigning each\noption a value between 1 and 5 and taking the average. Top box scores are\ncalculated by the percentages of responses that choose one of the top two most\nfavorable options.\n\nBe sure to review open text responses which can contain great information. If\nyou've collected a large number of comments, LLM tools such as ChatGPT can be\nuseful for extracting core themes and suggestions. When you've finished\nanalyzing results, be sure to share your findings with respondents so their\ntime filling out the survey feels worthwhile.\n\nHow easy or difficult is it for you to do work as a developer or technical\ncontributor at [INSERT ORGANIATION NAME]?\n\nVery difficult\n\nSomewhat difficult\n\nNeither easy nor difficult\n\nSomewhat easy\n\nVery easy\n\nFor the primary application or service you work on, what is your lead time for\nchanges (that is, how long does it take to go from code committed to code\nsuccessfully running in production)?\n\nMore than one month\n\nOne week to one month\n\nOne day to one week\n\nLess than one day\n\nLess than one hour\n\nHow often do you feel highly productive in your work?\n\nNever\n\nA little of the time\n\nSome of the time\n\nMost of the time\n\nAll of the time\n\nPlease rate your agreement or disagreement with the following statements:\n\nStrongly disagree| Disagree| Neutral| Agree| Strongly agree  \n---|---|---|---|---  \nMy team follows development best practices| \u25a1| \u25a1| \u25a1| \u25a1| \u25a1  \nI have enough time for deep work.| \u25a1| \u25a1| \u25a1| \u25a1| \u25a1  \nI am satisfied with the amount of automated test coverage in my project.| \u25a1|\n\u25a1| \u25a1| \u25a1| \u25a1  \nIt's easy for me to deploy to production.| \u25a1| \u25a1| \u25a1| \u25a1| \u25a1  \nI'm satisfied with the quality of our CI/CD tooling.| \u25a1| \u25a1| \u25a1| \u25a1| \u25a1  \nMy team's codebase is easy for me to contribute to.| \u25a1| \u25a1| \u25a1| \u25a1| \u25a1  \nThe amount of technical debt on my team is appropriate based on our goals.| \u25a1|\n\u25a1| \u25a1| \u25a1| \u25a1  \nSpecifications are continuously revisited and reprioritized according to user\nsignals.| \u25a1| \u25a1| \u25a1| \u25a1| \u25a1  \n  \nPlease share any additional feedback on how your developer experience could be\nimproved\n\n[open textarea]\n\n## Using qualitative and quantitative metrics together\n\nQualitative metrics and quantitative metrics are complementary approaches to\nmeasuring developer productivity. Qualitative metrics, derived from surveys,\nprovide a holistic view of productivity that includes both subjective and\nobjective measurements. Quantitative metrics, on the other hand, provide\ndistinct advantages as well:\n\n  * Precision. Humans can tell you whether their CI/CD builds are generally fast or slow (i.e., whether durations are closer to a minute or an hour), but they cannot report on build times down to millisecond precision. Quantitative metrics are needed when a high degree of precision is needed in our measurements.\n  * Continuity. Typically, the frequency at which an organization can survey their developers is at most once or twice per quarter. In order to collect more frequent or continuous metrics, organizations must gather data systematically.\n\nUltimately, it is through the combination of qualitative and quantitative\nmetrics \u2013 a mixed-methods approach \u2013 that organizations can gain maximum\nvisibility into the productivity and experience of developers. So how do you\nuse qualitative and quantitative metrics together?\n\nWe\u2019ve seen organizations find success when they start with qualitative metrics\nto establish baselines and determine where to focus. Then, follow with\nquantitative metrics to help drill in deeper into specific areas.\n\nEngineering leaders find this approach to be effective because qualitative\nmetrics provide a holistic view and context, providing wide understanding of\npotential opportunities. Quantitative metrics, on the other hand, are\ntypically only available for a narrower set of the software delivery process.\n\nGoogle similarly advises its engineering leaders to go to survey data first\nbefore looking at logs data for this reason. Google engineering researcher\nCiera Jaspan explains: \u201cWe encourage leaders to go to the survey data first,\nbecause if you only look at logs data it doesn't really tell you whether\nsomething is good or bad. For example, we have a metric that tracks the time\nto make a change, but that number is useless by itself. You don't know, is\nthis a good thing? Is it a bad thing? Do we have a problem?\u201d.\n\nA mixed methods approach allows us to take advantage of the benefits of both\nqualitative and quantitative metrics while getting a full understand of\ndeveloper productivity:\n\n  1. Start with qualitative data to identify your top opportunities\n  2. Once you know what you want to improve, use quantitative metrics to drill-in further\n  3. Track your progress using both qualitative and quantitative metrics\n\nIt is only by combining as much data as possible \u2013 both qualitative and\nquantitative \u2013 that organizations can begin to build a full understanding of\ndeveloper productivity.\n\nIn the end, however, it\u2019s important to remember: organizations spend a lot on\nhighly qualified humans that can observe and detect problems that log-based\nmetrics can\u2019t. By tapping into the minds and voices of developers,\norganizations can unlock insights previously seen as impossible.\n\n## Acknowledgements\n\nOur thanks to Laura Tacho, Max Kanat-Alexander, Laurent Ploix, Martin Fowler,\nBethany Otto, Andrew Cornwall, Carol Costello, and Vanessa Towers for their\nfeedback on this article.\n\n\u00a9 Martin Fowler | Privacy Policy | Disclosures\n\n", "frontpage": false}
