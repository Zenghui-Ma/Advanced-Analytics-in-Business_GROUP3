{"aid": "40069474", "title": "Tiktokify: A Hackathon Winning Product", "url": "https://akshatsharma.xyz/posts/blog/experience_mongo_gen_ai_hackathon/", "domain": "akshatsharma.xyz", "votes": 2, "user": "akshat2602", "posted_at": "2024-04-17 20:03:06", "comments": 0, "source_title": "tiktokify: A Hackathon winning product", "source_text": "tiktokify: A Hackathon winning product | Akshat Sharma\n\nHome \u00bb Posts \u00bb Blogs\n\n# tiktokify: A Hackathon winning product\n\nA post detailing my experience at the MongoDB Gen AI Hackathon and the product\nme and my team built over the span of 8 hours.\n\n14 April, 2024 hackathongenAIdevelopmentembeddingsmongoDB 1233 words 6 min | Suggest Changes\n\nLast week, me and my team, http418, won the MongoDB Gen AI Hackathon. We\ncreated a tool called \u201ctiktokify\u201d which can automatically generate a small 30\nsecond highlight clip from a video using clever prompt engineering and\nSoTA(State-of-The-Art) embedding models. We won the \u201cBest Use of Nomic\nPlatform\u201d prize and here is my account of what we built and how we won.\n\n### TL;DR(presented by ChatGPT):\n\nOur team won the MongoDB Gen AI Hackathon with \u201ctiktokify,\u201d a tool generating\n30-second highlight clips from videos. Leveraging cutting-edge embeddings and\nprompt engineering, we automated the creation of engaging content. We used\nNomic AI\u2019s embeddings model, transcribed video audio for text summaries, and\nextracted video clip embeddings stored in MongoDB. Stitching together the most\nrelevant clips, we added AI-generated narration for a complete TikTok-ready\nhighlight. Check out our project code on GitHub and explore AWS services for\nsimilar functionality.\n\nAfter hours of building, the money shot\n\n## The problem\n\nGoing in, we knew we wanted to do something video related because let\u2019s face\nit, RAGs(Retrieval Augmented Generations) have already been proven to work\npretty well and if you want create a high quality RAG then it becomes all\nabout the quality of data. We wanted to try something different, something\nnew, something video related, something which none of us had tried before.\nAfter the breakfast and some very quick talks by the sponsors of the\nhackathons, we started brainstorming about things we could try out in the\nvideo space.\n\n> Retrieval Augmented Generation(RAGs) explained^1\n\nWe naturally looked at TikTok, Instagram Reels and YouTube Shorts and how they\nhave blown up the short video content space. Something we realized is that the\nsort of stuff that people like looking at is very short 15-30 seconds summary\nof any and every thing, let it be programming tutorials, videos about reddit\nthreads, football goals, cricket clips everything is TikTokable. What if we\ncan make this content creation space automated? What if we help make the dead\ninternet theory come true?\n\n## The solution\n\nWe settled on creating a video summarizer, something that would take in a long\nvideo and \u201ctiktokify\u201d it, i.e., generate a highlight from it. To add to this,\nwe also decided to add an AI voice narration so that the audio is not all junk\nand we can create a hook to our \u201ctiktokified\u201d video.\n\n## The approach\n\nBefore we dive into the approach, one very important thing which we used, that\nI feel might need some explanation, is embeddings.\n\n#### Embeddings\n\nEmbeddings are basically numerical representations of real-world data in a\nform which is easier for Neural Nets to understand. Embeddings are created in\nsuch a way that similar types of objects are placed near to each other in a\nlatent space. Typically embeddings have been representations of only one form\nof data(either image, audio or text) but, we wanted a multimodal embedding\nmodel because of obvious reasons.\n\n> A visual aid for learning embeddings^2\n\nWe initially were gonna use OpenAI\u2019s CLIP embeddings model but, Andriy, the\nCTO of Nomic AI, told us about their embedding model which had a larger token\nsize than CLIP by almost a scale of 100. So naturally, we decided to use\nNomic\u2019s Embedding Model instead.\n\nOkay, so now that embeddings are out of the way the basic idea of what we\nwanted to build is pretty simple, here are the steps:\n\n  1. Generate some kind of text summary of the video\n\na. Using some kind of video model to directly generate the summary.\n\nb. Extract images at specific intervals, run image captioning on them and\ngenerate a summary using the image captions.\n\nc. Transcribing the audio from the video and generate a summary of it.\n\n  2. Create video clip embeddings for querying\n\n  3. Use the generated text summary in the first step as a query vector and perform a cosine similarity on the video clip embeddings from step 2\n\n  4. Get the clips and stitch them together.\n\n#### Step 1: Generate a text summary\n\nOption 1 wasn\u2019t really feasible for us because we had not explored this space\nbefore and from the looks of it, this required a lot computational power which\nwe didn\u2019t have, considering we are still students.\n\nOption 2 seemed really promising since we already had some experience working\nwith BLIP, an image captioning model. One thing we didn\u2019t realise was, since\nwe are going to be feeding images from a video to BLIP, the context of the\nvideo as a whole will be lost. For example, the first few 10 frames from the\nvideo would produce the same exact output. This is far from ideal and could\ncreate problems while generating the summary, not to mention the loss of\ncomputational power which we already were short on. This option still was very\npromising but we didn\u2019t really have the time to explore it more.\n\nOption 3 fit our usecase perfectly because for our demo and testing purposes,\nwe had chosen a football game as our input, which has a lot of commentary in\nit. Obviously, this would render other types of \u201csilent\u201d videos un-\ntiktokifyable but we didn\u2019t really have a choice. We realized, we could even\nresort to using the SRT file associated with a video instead of relying on the\naudio itself. We wrote a really clever prompt and fed the SRT file to ChatGPT\nand got a great summary from it.\n\n#### Step 2: Create video clip embeddings for querying\n\nWe decided to use a very rudimentary approach to create embeddings from the\nvideo. We extracted frames, at every 0.5 second, from the video and fed them\nto the embedding model to generate embeddings. We stored these embeddings\nalong with their timestamps in our MongoDB Atlas instance.\n\n#### Step 3 & 4: Stitching it all together\n\nNow that we have our video embeddings and our search query vector, we just had\nto create an index in our MongoDB and run a cosine similarity search using our\nquery vector. We got the top 50 most relevant images, sorted them in an\nascending order and then extracted short 0.5 seconds clips starting from those\ntimestamps. For example, let\u2019s say I have the most relevant timestamp as 12.5\nsecond, I will extract a clip that has the range of 12.5-13 seconds, in this\nway we got a 25(50 frames * 0.5 sec clips) second highlight video. These clips\nwere then merged using ffmpeg and then again, using ffmpeg, we added in an AI\ngenerated text-to-speech of the text summary in the video.\n\nWe have a football highlight completely using AI up and running!!!\n\n## The end\n\nThanks for reading the story of what we built and how we won the MongoDB Gen\nAI Hackathon. Thanks to MongoDB for organizing this hackathon and also huge\nthanks for Nomic AI and Andriy for helping us out during the hackathon.\n\nYou can checkout the code for the hackathon here:\nhttps://github.com/herzo175/mongodb-apr-2024-hackathon\n\nIf you have money you can use this AWS service to achieve the same thing:\nhttps://aws.amazon.com/blogs/media/video-summarization-with-aws-artificial-\nintelligence-ai-and-machine-learning-ml-services/\n\nThe video we used: https://www.youtube.com/watch?v=h4m68r8kWAc\n\nGreat blog on storing and search embeddings using Nomic and MongoDB:\nhttps://blog.nomic.ai/posts/nomic-embed-mongo\n\nP.S. If you\u2019re looking to hire, I have experience in building RAGs, working in\nsystems, building all sorts of random things. You can have a look at my resume\nor contact me using LinkedIn, Email or Twitter/X and checkout my GitHub\n\n  1. This image was taken from Pinecone\u2019s article about RAGs. https://www.pinecone.io/learn/retrieval-augmented-generation/ \u21a9\ufe0e\n\n  2. The image was taken from Pinecone\u2019s article about embeddings. https://www.pinecone.io/learn/vector-embeddings/ \u21a9\ufe0e\n\n\u00a9 2024 Akshat Sharma Powered by Hugo\n\n", "frontpage": false}
