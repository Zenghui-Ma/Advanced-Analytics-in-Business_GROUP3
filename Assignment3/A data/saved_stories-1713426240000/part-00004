{"aid": "40069511", "title": "Distilabel: Synthetic Data Generation and Rlaif at Scale", "url": "https://argilla.io/blog/introducing-distilabel-1/", "domain": "argilla.io", "votes": 1, "user": "dvilasuero", "posted_at": "2024-04-17 20:07:20", "comments": 0, "source_title": "Distilabel: Bringing Synthetic Data Generation and AI Feedback to Everyone", "source_text": "Distilabel: Bringing Synthetic Data Generation and AI Feedback to Everyone\n\nArgilla open-source tool\n\nDemo\n\nDocs\n\nAbout\n\nBlog\n\nPricing\n\n3,091\n\n# Distilabel: Bringing Synthetic Data Generation and AI Feedback to Everyone\n\nApril 17, 2024\n\n\u25cf\n\nDaniel Vila Suero, Gabriel Mart\u00edn Bl\u00e1zquez, \u00c1lvaro Bartolom\u00e9, Agust\u00edn Piqueres\nLajar\u00edn, Ben Burtenshaw, David Berenstein\n\nA few months ago, we introduced distilabel, a Python library for synthetic\ndata generation and AI feedback using LLMs. At the time, we started with a\nsimple approach, where the user could generate synthetic data using a single\nLLM called generator and then label it using another LLM called labeller. This\napproach was useful for many scenarios, especially generating preference\ndatasets. Thanks to this first version, we've generated and openly shared\nimpactful datasets like argilla/distilabel-capybara-dpo-7k-binarized,\nargilla/OpenHermesPreferences, argilla/distilabel-intel-orca-dpo-pairs, among\nothers, that have been used to train several SOTA models. We also saw an\nincreasing adoption of distilabel in the community, with cool community\nprojects such as davanstrien/haiku.\n\nHaving that said, we are aware that the previous implementation was not\nsuitable for more complex synthetic data generation pipelines, like DEITA,\nwhich requires running several LLMs and more complex steps. On top of that, we\nwanted to make the project less complex to scale and easier for the community\nto contribute. The decision was clear: we needed to rewrite the library from\nscratch to address this and make it more extensible, maintainable, and\nscalable.\n\nToday, we\u2019re happy to announce distilabel 1.0.0, a new version of the library\nthat brings a new architecture allowing to build complex data processing\npipelines with LLMs, and with the hope to make it easier for the community to\ncreate and share synthetic data generation pipelines.\n\n## Pipelines, Steps, Tasks and LLMs\n\nThis new version of distilabel allows to build Pipelines with any number of\nSteps or Tasks, that can be connected between, so the output of one step or\ntask is fed as input to another. It's no longer about one generator and one\nlabeller, but about a series of steps that can be chained together to build\ncomplex data processing pipelines with LLMs.\n\nA Step is a more general node relying on a base class that does not require an\nLLM or model to be used. The input of each Step is a batch of data, containing\na list of dictionaries, where each dictionary represents a row of the dataset,\nand the keys are the column names. An Step then can:\n\n  1. Add or remove keys from the dictionaries to modify the columns of the final dataset.\n  2. Filter out dictionaries to remove rows from the dataset.\n  3. Add new dictionaries to the batch to add new rows to the dataset.\n\nIn addition, a Step offers a simple life cycle, exposing a load method that\ncan be used to create the necessary resources that will be used in the process\nmethod, where the actual data processing is done. Finally, each Step can\ndefine a list of runtime parameters that can be used to configure the behavior\nof the Step for each pipeline execution.\n\nBuilding on this basic idea, distilabel offers two additional kind of steps\napart from the normal Step which are the GeneratorStep and the GlobalStep.\n\nGeneratorSteps are nodes that loads data from a source (for example, from a\ndataset from the Hugging Face Hub) or generates new data (for example, using\nSelfInstruct and a list of topics), therefore they are the starting nodes of\nthe pipeline and do not require any incoming edges.\n\nOn the other hand, GlobalSteps works exactly the same as the Steps but they\nreceive all the data from the previous steps all at once, allowing to\naggregate data from previous steps or to perform operations that require the\nfull dataset, like filtering out rows that are repeated.\n\nContinuing on from the Step concept, we have evolved the Task concept from the\nprevious version, which is now a Step that knows how to use an LLM to perform\na specific task such as text generation, evolving instructions or responses,\njudging the quality of a text, etc.\n\n## Pipeline execution\n\nFor this new version, we've also changed the way the pipelines are executed.\nIn the previous version, the execution was sequential, meaning that the\ngenerator was executed first, and then the labeller was executed.\n\nNow, the execution is parallel using several processes, each one executing a\ndifferent step of the pipeline. When the subprocess is created, it will\nexecued the load method of the step, and then it will start processing batches\nreceived from an input queue. The resulting batches will be sent back to the\nmain process through an output queue, where they will be distributed to the\nnext steps in the pipeline.\n\nFor this first version, we have decided to use the multiprocessing module from\nthe Python standard library to manage the subprocesses and to have a single\nnode pipeline execution, which is enough for most of the cases. Having that\nsaid, we gave a lot of thought to the design of the architecture to add\nsupport for distributed execution in the future, using libraries like Ray.\n\n## Sharing pipelines\n\nOne of the main goals of this new version of distilabel is to make it easier\nfor the community to create and share synthetic data generation pipelines. To\nachieve this, we have added a new feature that allows to serialize a pipeline\nto a JSON or YAML file, and to load it back from the file, allowing to tweak\nthe runtime parameters of the pipeline and to run it again. In addition,\npushing the resulting dataset to the Hugging Face Hub will automatically push\nthe pipeline to the Hub as well, and add a nice description of the pipeline to\nthe dataset card, making it easier to reexecute the pipeline in the future.\n\nand if you are you not a script person \ud83d\udc68\ud83c\udffb\ud83d\udcbb? ...\n\nNo worries, we got you covered! We have also added a CLI that allows to get\nthe info of a pipeline from a file or URL:\n\n    \n    \n    distilabel pipeline info --config \"https://huggingface.co/datasets/distilabel-internal-testing/instruction-dataset-mini-with-generations/raw/main/pipeline.yaml\"\n\nand to run a pipeline from a file or URL:\n\n    \n    \n    distilabel pipeline run --config \"https://huggingface.co/datasets/distilabel-internal-testing/instruction-dataset-mini-with-generations/raw/main/pipeline.yaml\" \\ --param load_dataset.repo_id=distilabel-internal-testing/instruction-dataset-mini \\ --param load_dataset.split=test \\ --param generate_with_gpt35.llm.generation_kwargs.max_new_tokens=512 \\ --param generate_with_gpt35.llm.generation_kwargs.temperature=0.7 \\ --param to_argilla.dataset_name=text_generation_with_gpt35 \\ --param to_argilla.dataset_workspace=admin\n\n## An overview of the differences\n\nThe main differences between the former and the current version are:\n\ndistilabel \u2264 0.6.0| distilabel \u2265 1.0.0  \n---|---  \n# of LLMs| 2 at most| From 0 to N (not mandatory to use LLMs)  \n# of Tasks| 2 at most (generator and labeller)| From 1 to N  \nIntegrations| OpenAI, vLLM, Llama.cpp, Transformers, Inference Endpoints,\nTogether, Anyscale, Ollama and Vertex AI| Same as before, but also Cohere,\nAzure OpenAI and LiteLLM  \nFlow| Generator \u2192 Labeller| Any \u2192 ... \u2192 Any (where ... can be an arbitrary\nnumber of tasks)  \nExecution| Sequential| Parallel  \nEase of contribution| Medium-Hard| Easy  \nArgilla| Integrated on every pipeline| Detached, following a plug and play\napproach anytime  \nHierarchy| LLM| Pipeline > Step > Task (> LLM)  \nSyntax| generator and labeller| Arbitrary, defined by the user  \nApproach| Chained Python functions| DAG  \nSharing| Hard to share pipelines| Easy to share pipelines thanks to the\nserialization and the CLI  \n  \n### Former\n\n  * Only for generator-labeller scenarios\n  * Hard to scale / maintain\n  * Not suitable for most of the synthetic data generation pipelines\n\n    \n    \n    from datasets import load_datasetfrom distilabel.llm import OpenAILLMfrom distilabel.pipeline import pipelinefrom distilabel.tasks import TextGenerationTaskdataset = ( load_dataset(\"HuggingFaceH4/instruction-dataset\", split=\"test[:10]\") .remove_columns([\"completion\", \"meta\"]) .rename_column(\"prompt\", \"input\"))task = TextGenerationTask()generator = OpenAILLM(task=task, max_new_tokens=512)pipeline = pipeline(\"preference\", \"instruction-following\", generator=generator)dataset = pipeline.generate(dataset)\n\n### Current\n\n  * Can have any number of steps of any kind (not only LLMs)\n  * Is more extensible, maintainable and scalable\n  * May require more computing for local LLMs as the steps run in parallel\n\n    \n    \n    from distilabel.llms import OpenAILLMfrom distilabel.pipeline import Pipelinefrom distilabel.steps import LoadDataFromDictsfrom distilabel.steps.tasks import TextGenerationwith Pipeline() as pipeline: load_dataset = LoadDataFromDicts( name=\"load_dataset\", data=[ { \"instruction\": \"Write a short story about a dragon that saves a princess from a tower.\", }, ], ) text_generation = TextGeneration( name=\"text_generation\", llm=OpenAILLM(model=\"gpt-4\"), ) load_dataset.connect(text_generation) ...if __name__ == \"__main__\": distiset = pipeline.run( parameters={ \"text_generation\": { \"llm\": { \"generation_kwargs\": { \"temperature\": 0.7, \"max_new_tokens\": 512, } } }, ... }, ) distiset.push_to_hub( \"distilabel-internal-testing/instruction-dataset-mini-with-generations\" )\n\n## So what's next?\n\nYou can check distilabel GitHub repository and the documentation to learn more\nabout the new version, and to start creating your own synthetic data\ngeneration pipelines.\n\nWe hope that this new version of distilabel will make it easier for the\ncommunity to create and share synthetic data generation pipelines, and that it\nwill help to democratize the use of synthetic data generation and AI feedback.\nWe are excited to see what the community will build with this new version of\ndistilabel, and we are looking forward to your feedback and contributions!\n\n## Stay in the loop!\n\nGet the latest Argilla news by email, including product updates, blogs, and\ncommunity events.\n\n  * ## RLHF and alternatives: ORPO\n\nApril 5, 2024\n\n\u25cf\n\nArgilla, MantisNLP\n\n  * ## RLHF and alternatives: KTO\n\nMarch 18, 2024\n\n\u25cf\n\nArgilla, MantisNLP\n\n  * ## RLHF and alternatives: IPO\n\nMarch 11, 2024\n\n\u25cf\n\nArgilla, MantisNLP\n\n  * Docs\n\n  * Blog\n\n  * Pricing\n\n  * Contact\n\n  * Careers\n\n  * About\n\nStar 3,091\n\nTerms of use\n\nPrivacy policy\n\nCopyright \u00a9 2024 Argilla.io All rights reserved\n\nWe use our own and third-party cookies to compile statistics on the use of the\nwebsite in order to identify faults and improve the content and configuration\nof the website. For more information, please visit the Privacy Policy\n\n", "frontpage": false}
