{"aid": "40069476", "title": "Marginalia's Query Parsing and Understanding", "url": "https://www.marginalia.nu/log/a_103_query_parsing/", "domain": "marginalia.nu", "votes": 2, "user": "marginalia_nu", "posted_at": "2024-04-17 20:03:17", "comments": 0, "source_title": "Query Parsing and Understanding", "source_text": "Query Parsing and Understanding @ marginalia.nu\n\n# Query Parsing and Understanding\n\nPosted: 2024-04-17\n\nBeen working on improving Marginalia Search query parsing and understanding.\nThis is going to be a pretty long update, as it\u2019s a few months\u2019 work.\n\nApart from cleaning up the somewhat messy query parsing code, a problem I\u2019m\ntrying to address is that the search engine is currently only good at dealing\nwith fairly focused queries, they don\u2019t need to be short, but if you try to\nqualify a search that is too broad by adding more terms, it often doesn\u2019t\nproduce anything useful.\n\n## Query Segmentation\n\nA reason for this is that the search engine currently has a pretty primitive\nquery understanding.\n\nIf you provide it with a search query, it will require that all terms are\nimportant to the document, weighted by BM25, or they all appear in proximity.\nThis has its flaws. If you consider a query like \u2018slackware linux package\nmanager\u2019, then I think a human would see that there are two concepts in this\nquery. We can break it down something like this:\n\n  * (slackware linux)\n  * (package manager)\n\nIf \u2018slackware\u2019 and \u2019linux\u2019; or \u2018package\u2019 and \u2018manager\u2019 appear close by each\nother, then that\u2019s probably indicative of a good hit, but whether \u2018slackware\u2019\nand \u2018manager\u2019; or \u2019linux\u2019 and \u2018package\u2019 appear close by each other is probably\nnot very informative.\n\nThis is a pretty well studied area, but many of the existing approaches\nrequire modelling based on query logs, which is something that Marginalia\nSearch does not retain as a matter of principle.\n\nSearch queries are very sensitive data and in my opinion, the only reasonable\nexpectation of privacy is that they must not be retained for analysis.\nOperating in this space it\u2019s also important to have a strategy for if a\ngovernment comes requesting such information. Simply not having the requested\ninformation allows full compliance with any legal requests without having to\nassume a risky adversarial position to the law.\n\nAnyway, since there are no query logs to base a model off, other approaches\nare required.\n\nI experimented with using existing word vector models, since they ostensibly\npredict the likelihood of sequences of words, but could not get this to work\nvery well. It may be an avenue for continued investigation, as at least on\npaper it should be well suited for the task, but in the meantime, an evergreen\nfont of concepts is Wikipedia. This is explored academically in e.g. M Hagen\n2010[1], which also offers a handy set of evaluation queries, the Webis-\nQSeC-10 dataset. This will be a loose inspiration for the new model.\n\nExtracting Wikipedia\u2019s article titles provides a manageable list of a few\nmillion word n-grams. This data is a bit noisy, but the quality is sufficient\nfor these purposes. This information is amended with frequency information,\nextracted by counting how many times each concept appears in link texts or\nitalics within the wikipedia corpus.\n\n## Fast Phrase Identification through modified Rabin-Karp\n\nThe next problem is that given a search query, how can we test its substrings\nagainst a large lexicon in a performant fashion. We want this to be extremely\nfast, because we want to not only be able to test the limited number of\nsubstrings in the query, but it\u2019s preferable to also be able to do this\nagainst the crawl data to lift important ngrams out of the text.\n\nWe might want e.g. \u201cmanage packages linux slackware\u201d to match \u201c[package\nmanager] [slackware linux]\u201d.\n\nWith this in mind, it\u2019s appealing to just store the hashes of the strings in a\nhash table. The benefit of this isn\u2019t obvious in a vacuum, but fixed width\nhashes has a desirable property that you can combine them, and depending on\nthe mathematical properties of the combining function, neat effects may fall\nout of the process.\n\nWhat we\u2019re looking to build is something like the the Rabin\u2013Karp algorithm, a\nsliding window that scans over the text, and identifies each word n-gram\nsegment of a specific length. Instead of building a single rolling hash like\nyou would with R-K, which does substring search, we\u2019ll use hashes of words to\nconstruct a new hash instead, that we can add and remove components from.\n\n(A hidden agenda behind this is to be able to re-use hashes to find substrings\nof different lengths without rehashing.)\n\nConsider if for example we combine hashes with the bit-wise XOR function, we\u2019d\nfind that we can undo the the combination of hashes by just xor:ing again.\n\nThis has a property that is desirable for testing whether it\u2019s worthwhile to\npermute a term, since XOR is commutative, but it\u2019s not sufficient for\ndetermining whether to segment an n-gram as we also get strange behaviors when\nthe same word appears multiple times in the same n-gram (undoing itself), and\nthere\u2019s a lot of false positives since we match any n-gram that has the same\nwords in any order:\n\n    \n    \n    F(africa) x F(south) x F(africa) = F(south) F(south) x F(africa) = F(africa) x F(south)\n\nTo determine whether to segment, we want a combining function that has an\ninverse function, but that is not self-inverse. To this end, combining hashes\nby rotating the bits of one of the components to the left and then bitwise\nXOR:ing them works better.\n\n    \n    \n    f(a,b) = rotate-bits-left(a, 1) XOR b F(a,b,n) = rotate-bits-left(a, n) XOR b\n\nThis pair of functions lets us undo earlier combination after successive\napplication, but is not commutative, and not self-inverse.\n\nIt\u2019s very fast, enough for the side-objective of keyword extraction in the\nhundreds of billions of tokens in the crawl data as well, to highlight noun\nwords and terms of interest for faster priority indexing. The search engine\nalready does something similar, but using hints such as capitalization and\ngrammar/POS tagging, but this misses a fair bit of keywords.\n\nIt isn\u2019t really feasible to index every arbitrary-length word-ngram, as the\nresulting token lexicon would be of a ridiculous cardinality, and it needs to\nbe less than 4 billion items for key statistical tricks to work our way: The\nsearch engine doesn\u2019t actually store the keyword lexicon, which would be\nenormous, but instead selects hash functions in such a way that collisions are\nrare.\n\nThis all cooks down to a fairly simple piece of code that can be run over a\nlist of terms that will output any n-grams that are present out of a lexicon\nconsisting only of hashes, while performing no string concatenation and only\ncalculating each word hash once.\n\nThe logic looks like this:\n\n    \n    \n    public List<SentenceSegment> findSegmentOffsets(int length, long[] hashes) { // Don't look for ngrams longer than the sentence if (hashes.length < length) return List.of(); List<SentenceSegment> positions = new ArrayList<>(); long hash = 0; int i = 0; // Prepare by combining up to length hashes for (; i < length; i++) { hash = orderedHasher.apply(hash, hashes[i]); } // Slide the window and look for matches for (;;) { int ct = counts.get(hash); if (ct > 0) { positions.add(new SentenceSegment(i - length, length, ct)); } if (i < hashes.length) { hash = orderedHasher.replace(hash, hashes[i], hashes[i - length], length); i++; } else { break; } } return positions; } /// ... class OrderedHasher implements HasherGroup { public long apply(long acc, long add) { return Long.rotateLeft(acc, 1) ^ add; } public long replace(long acc, long add, long rem, int n) { return Long.rotateLeft(acc, 1) ^ add ^ Long.rotateLeft(rem, n); } }\n\nThe code assumes the hash function is nearly perfect. The same assumption is\nused in other areas of the search engine, and the approximation holds up\nsurprisingly well. The noisiness of the internet itself helps plausibly mask\nany hash collisions that do appear: Even without this approximation, the\nnoisiness of the search results wouldn\u2019t be significantly decreased.\n\nSince a good while back Marginalia hashes its keywords with a modified version\nof Murmur 128 that make assumptions that the string is mostly 7 bit ASCII, but\nhedges its bets by seeding the Murmur hash with a basic polynomial rolling\nhash function. There\u2019s more to be said about the trade-offs in doing this, but\nin brief it benchmarks five times as fast as Guava\u2019s Murmur 128 hashes and\nnearly twice as fast as Common Codec\u2019s implementation, which it is based off,\nwhile doing a decent job balancing the interests of speed while still being a\nuseful hash function for the applications at hand. src\n\n## Query Construction\n\nThe next problem is one of representing the query during construction.\n\nA directed acyclic graph is a decent choice, especially with a predetermined\nstart and stop vertex. This permits the construction of rules that walk the\ngraph and add new paths based on some rule set, either through the\nsegmentation logic above or based on grammar or something else. The search\nengine has previously had rules for e.g. taking the word AC/DC and generating\nthe variants ACDC and AC DC. This isn\u2019t always the right move, sometimes\nseparators are significant, there\u2019s a big difference between \u201cfree sex-movies\u201d\nand \u201csex-free movies\u201d, for example, but in general it improves recall.\n\nAs a side-note, it\u2019s a popular sentiment that a search engine should only\nperform the exact search that is entered. Usually this is fuelled by some\nintuition from e.g. \u201cgrep\u201d, which works well on small bodies of documents, and\na frustration with the opaque query interpretation used by some larger search\nengines. This view isn\u2019t very well informed. An internet search engine needs\nto do more than grep because the Internet is a very large and heterogeneous\ncorpus of documents. A binary criteria like grep\u2019s would not make a useful\nsearch engine. That said, having a transparent and intuitive query execution\nand ranking model where it\u2019s obvious what the search engine is doing is\nimportant to avoid the frustrating case where users can\u2019t understand why they\ndon\u2019t seem to get results that match the query.\n\nWhile this DAG representation is very convenient when generating the queries,\nallowing very clear and succinct rules to be created without worrying much\nabout the structure of the data, it\u2019s less useful for evaluating the queries\nand sort of awkward to move around.\n\nReturning to \u201cslackware linux package manager\u201d, the equivalent DAG would looks\nsomething like this:\n\nDirected Acyclic Graph of the query \"slackware linux package manager\" courtesy\nof graphviz\n\nAn intermediate representation is needed. I decided on a simple LL(1) infix\ngrammar.\n\nThis was chosen in part because it\u2019s human-readable and trivial to tokenize and prase, to the point where recursive descent is almost overkill for the grammar. To give an brief idea of what the grammar looks like in practice the compiled query \u201c( marginalia | wiby ) search engine\u201d matches \u201cmarginalia search engine\u201d and \u201cwiby search engine\u201d.\n\nAir is added between the grammatically significant tokens to permit longer\nsearch keywords to contain the symbols \u2018(\u2019, \u2018)\u2019, and \u2018|\u2019. Escaping the\noperations would also have been an option.\n\nPreviously the search engine has dealt with variant queries by simply building\na list of alternative queries, both during construction but also during\nevaluation, basically running multiple queries in parallel. This is at first\nface good for performance, as these are trivial to run in parallel, but the\npossible complexity of the query is limited, and the total number of variants\nneeds to be relatively low.\n\nActually producing the new representation from the graph ended up being very\nsurprisingly difficult. I don\u2019t remember the last time I\u2019ve quite struggled\nthis much with what is ostensibly a simple coding problem. After several false\nstarts treating it as a graph traversal problem, eventually the solution I\narrived at was to expand all the paths through the graph into sets of\nvertexes, and using a sequence of intersection operations to greedily find a\nlocal minimum. Finding the global optimum was never needed, just something\nbetter than an enumeration of every possible combination. It should also be\nnoted that these graphs are small and typically have fairly low complexity,\nwith up to maybe a dozen vertexes and twenty or so edges at worst.\n\n## Query Evaluation\n\nQueries expressed in the new intermediate grammar are passed to the index\nservice. It has two needs for this information, it\u2019s needed when constructing\nthe queries against the index; and it\u2019s needed when evaluating the queries.\n\nThe representation chosen was the one that benefits the ranking code, as this\nis both the most expensive part and the part that could most do with increased\nclarity as it\u2019s generally pretty hard to benchmark the result ranking code so\nit needs to be easy to reason about.\n\nThe query is parsed into a simple grammar tree with an arbitrary number of\nchildren for each node. After some consideration, the choice was made to to\nseparate the topology of the tree from the data associated with the nodes, so\nthat it\u2019s possible to associate the same topology with different data sets,\nand operate on the data independently of the topology.\n\nIn one part of the code we may be dealing with a string representation of the\nkeywords, in another their term id:s, and in another positions and flag bit-\nmasks.\n\nThis makes much of the ranking code very simple, and it can be expressed\nthrough a mapping operation. We can define an operation where we traverse the\ntree combining values associated with each node using two operations, one\noperation is mapped to combining sibling values, and another is mapped to\ncombining child values.\n\nIf we for example want to answer if a path exists where a property is true for\neach element, say \u201cis the the keywords in the title\u201d, then we map \u201cchild\u201d to\nlogical AND and \u201csibling\u201d to logical OR. If we want to find the largest value\nof BM-25 in any path, we map \u201cchild\u201d to sum() and \u201csibling\u201d to max().\n\nThese types of operations are all over the ranking code, and the result feels\neasy to reason about. Not everything fits the tree-based approach to the\nquery, but since the data is separate it\u2019s still relatively manageable to work\nwith.\n\nOverall it can\u2019t be overstated how much thinking through data representation\npays off. There are things in code you can fuss about forever that doesn\u2019t\nmake a lick of difference, but fussing over how data is structured and\nmodelled almost always pays off in spades.\n\nEven though the new query parsing and evaluation models are definitely a fair\nbit more complex, much of the code actually feels simpler, almost as though\nlarge parts have been removed. And in a sense it has, what has been removed is\nall the gunk needed to deal with having data structured inappropriately. Often\nthis feels like complexity essential to the problem, and intuition is quick to\ntell us it can\u2019t be simplified any further than it is.\n\nAside from generating new search terms, the segmentation information is also a\nconstraint a limitation that the set of terms must appear in close proximity\nwithin the document.\n\nRegardless of segmentation, term proximity also feeds into two ranking\nparameters, one measuring the best position overlap of any path through the\nquery, which rewards results where all terms appear close by, and the other is\nthe average mutual jaccard index which rewards terms where some search terms\nare near by.\n\nOverall the ranking parameters are a bit poorly tuned right now as a result of\nthese large changes. This is expected and has historically always been the\ncase after a large change. My hunch is that some constraint is a bit too\nstrict, as the search engine tends to return a bit fewer results. Debugging\nthis sort of thing is a fairly time consuming process of identifying something\nthat\u2019s unexplainably higher or lower in the rankings than it should be, and\ntrying to figure out what is causing that.\n\n## A Brief Query Construction Trainwreck\n\nI initially attempted to use the new query structure to create fewer more\ncomplex queries against the indexes, basically using union and intersection\noperators to combine multiple keywords into one operation.\n\nThis resulted in a massive performance degradation with many queries taking\nupward of a second to complete.\n\nFlattening the variant subqueries and running them in parallel reversed the\nproblem, and instead lowered the average response time a fair bit.\n\nIt reinforces the lesson that a primary bottleneck in processing large amounts\nof data is single core memory throughput. Some outliers are always expected,\nbut during the performance degradation, all queries felt noticeably slow, with\n90th percentile values going from 150ms before, to 300ms during, to 110ms\nafter.\n\nHeatmap of request times, initial deployment marked as A, workaround marked\nwith B.\n\nDeploying several months worth of changes into production is inevitably a bit\nstressful.\n\n## References\n\n[1] Query Segmentation Revisited, 2011, M. Hagen et al. pdf\n\nmarginalia.nu \u00a9 2024 Viktor Lofgren <kontakt@marginalia.nu>\n\nConsider donating to support the development effort of Marginalia Search and\nthe other services!\n\n", "frontpage": false}
