{"aid": "40098001", "title": "Understand Positional Encoding for Self-Attention", "url": "https://swe-to-mle.pages.dev/posts/positional-encoding-for-self-attention/", "domain": "swe-to-mle.pages.dev", "votes": 3, "user": "peluche_", "posted_at": "2024-04-20 15:15:04", "comments": 0, "source_title": "Positional Encoding for Self Attention", "source_text": "Positional Encoding for Self Attention - SWE to ML Engineer\n\nSWE to ML Engineer\n\nPosts Tags Categories About Hire Me Grimoire\n\nSWE to ML Engineer\n\nPostsTagsCategoriesAboutHire MeGrimoire\n\n## Contents\n\n# Positional Encoding for Self Attention\n\npeluche included in bestiary\n\n2023-12-09 1782 words 9 minutes\n\nContents\n\nIn the dimly lit chambers of his ancient library, the wizard Eldron carefully\nweaves his spell over a complex array of arcane symbols. With each precise\ngesture, he transmutes these symbols, imbuing them with a hidden layer of\nmeaning: the magic of positional encoding. This enchantment allows the symbols\nto hold not just the essence of words, but also their place in the grand\ntapestry of language. Eldron\u2019s eyes gleam with satisfaction as the embeddings\nshimmer, now ready to reveal their secrets in perfect harmony and sequence.\n\nEldron's transmutation\n\n## The Quest\n\nEldron left many manuscripts of his work on positional encoding. Figure out\nthe intricacy of his spells and compare their efficacy.\n\n## Toy problem: Copy-Task\n\nTo illustrate the different positional encoding schemes we will work on a toy\nproblem.\n\nThe task is to replicate the input sequence before the special <copy> token,\nup to the end of the padding.\n\ne.g.:\n\n    \n    \n    1 7 2 <copy> _ _ _ _ _ _ \u2192 1 7 2 <copy> 1 7 2 _ _ _ 9 <copy> _ _ _ _ _ _ _ _ \u2192 9 <copy> 9 _ _ _ _ _ _ _ 2 2 4 3 <copy> _ _ _ _ _ \u2192 2 2 4 3 <copy> 2 2 4 3 _ 1 2 3 4 5 6 7 <copy> _ _ \u2192 1 2 3 4 5 6 7 <copy> 1 2\n\n### why this task?\n\nI chose this task because it requires attention and some awarness of position\nto be solvable.\n\n  * The offset of the <copy> is variable between examples so the model cannot hardwire inputs to outputs, so self-attention is important.\n  * The problem is not well suited for a bag of words approach because order is very important, so positional encoding is important.\n\n### why Encoder instead of Decoder?\n\nFor this task we chose to use an encoder-only transformer. Taking a sequence\nof tokens, a single copy token, and several padding tokens as input and\nproducing the entire resulting sequence in a single pass.\n\nThe reason we are not using an autoregressive decoder-only model for this is\nbecause the first half of the problem is unlearnable with a causal mask.\n\nlet\u2019s imagine what the equivalent training set would be for:\n\n    \n    \n    2 7 4 3 <copy> _ _ _ _ _ \u2192 2 7 4 3 <copy> 2 7 4 3 _\n\nUsing a causal mask with a decoder-only autoregressive transformer would be\nequivalent to ask the model to predict all these pairs of input / labels:\n\n    \n    \n    2 \u2192 7 2 7 \u2192 7 4 2 7 4 \u2192 7 4 3 2 7 4 3 \u2192 7 4 3 <copy> 2 7 4 3 <copy> \u2192 7 4 3 <copy> 2 2 7 4 3 <copy> _ \u2192 7 4 3 <copy> 2 7 2 7 4 3 <copy> _ _ \u2192 7 4 3 <copy> 2 7 2 7 4 3 <copy> _ _ _ \u2192 7 4 3 <copy> 2 7 4 2 7 4 3 <copy> _ _ _ _ \u2192 7 4 3 <copy> 2 7 4 3 2 7 4 3 <copy> _ _ _ _ _ \u2192 7 4 3 <copy> 2 7 4 3 _\n\nAnd the problem here is that the first 4 pairs are not possible to guess.\nThere is equal probability (10%) to chose the correct next token. Once the\n<copy> token appear in the input the output becomes totally inferable. But\nbefore that it\u2019s a coin toss. So decoder-only are not a good fit.\n\n## Sinusoidal\n\nThe first positional encoding proposed in the Attention Is All You Need paper\nis Sinusoidal. It consists of taking a bunch of offsets on a set of sin() and\ncos() waves with increasing frequencies as a representation of the position of\na given word.\n\nIntuitively it bothers me. I have the sentiment that it creates a failure mode\nwhere the network just \u201cduplicate\u201d tokens. Where token 1 at offset 0 is a\ndifferent token from 1 at offset 1 or 2 ..., conceptually duplicating the\nvocabulary for each possible offset in the context window.\n\nBut Eldron\u2019s manuscripts are littered with references to it, so let\u2019s cast the\nspell to get a better feel for it.\n\n    \n    \n    def get_sinusoidal_positional_encoding(context_size=CONTEXT_SIZE, embed_size=EMBED_SIZE): position = torch.arange(context_size).unsqueeze(1) div_term = torch.exp(torch.arange(0, embed_size, 2) * -(math.log(10000.0) / embed_size)) positional_encoding = torch.zeros(context_size, embed_size) positional_encoding[:, 0::2] = torch.sin(position * div_term) positional_encoding[:, 1::2] = torch.cos(position * div_term) return positional_encoding.to(device) class Net(nn.Module): def __init__(self): # <...> self.positional_embedding = get_sinusoidal_positional_encoding() # <...> def forward(self, x): x = self.token_embedding(x) # (batch_size, context_size, embedding_size) # sinusoidal positional encoding x = x + self.get_positional_embedding() # <...>  \n  \n---  \n  \n## Learned\n\nLearned positional encoding are similar in concept. But instead of hardcoding\nthe values we let the network learn whatever it prefers.\n\n    \n    \n    class Net(nn.Module): def __init__(self): # <...> self.positional_embedding = nn.Embedding(context_size, embed_size) # <...> def forward(self, x): x = self.token_embedding(x) # (batch_size, context_size, embedding_size) # learned positional encoding x = x + self.positional_embedding(torch.arange(0, self.context_size).to(device)) # <...>  \n  \n---  \n  \n### Visualize the Learned positional encodings\n\nIn our case what the network prefers is:\n\nPCA of the Learned Positional Encodings during training\n\n### Compare the 2 schemes\n\nLooking at the similarities (dot product and cosine similarity) for different\npositional offsets\n\nUninitialized Positional Encoding weightsTrained Positional Encoding\nweightsSinusoidal Positional Encoding Weights\n\n## RoPE\n\nRoPE is a scheme that plugs inside the Attention block and is based on\nrotation instead of translation.\n\nIntuitively I like the idea to not \u201cpollute\u201d the residual stream of\nembeddings. Only applying the transformation to the Query and Key inside the\nattention block.\n\n### Using a giant rotation matrix (Slow)\n\n    \n    \n    # slow (but intuitive?) using matrix multiplication for the rotations # e.g. of rotation matrix for embed_size=4 # [[cos(t), -sin(t), 0, 0], # [sin(t), cos(t), 0, 0], # [ 0, 0, cos(t2), -sin(t2)], # [ 0, 0, sin(t2), cos(t2)]] def get_rotation_matrix(m, embed_size=EMBED_SIZE): thetas = torch.tensor([10000 ** (-(2 * (i // 2)) / embed_size) for i in range(embed_size)]) thetas *= m rotation = torch.eye(embed_size) rotation *= thetas.cos() col_vals = torch.arange(0, embed_size, 2) row_vals = col_vals + 1 rotation[col_vals, row_vals] = -thetas.sin()[::2] rotation[row_vals, col_vals] = thetas.sin()[::2] return rotation.T def get_rotation_matrices(context_size=CONTEXT_SIZE, embed_size=EMBED_SIZE): rotations = [get_rotation_matrix(m, embed_size) for m in range(context_size)] return torch.stack(rotations).to(device)  \n  \n---  \n  \n### Using the fast way\n\n    \n    \n    # e.g. for embed_size=4 # | x1 | | cos(t) | | -x2 | | sin(t) | # | x2 | * | cos(t) | + | x1 | * | sin(t) | # | x3 | | cos(t2) | | -x4 | | sin(t2) | # | x3 | | cos(t2) | | x3 | | sin(t2) | def get_efficient_rotation_matrix(context_size, embed_size=EMBED_SIZE): thetas = torch.tensor([10000 ** (-(2 * (i // 2)) / embed_size) for i in range(embed_size)]) cos_ = [(m * thetas).cos() for m in range(context_size)] sin_ = [(m * thetas).sin() for m in range(context_size)] return torch.stack(cos_).to(device), torch.stack(sin_).to(device) def compute_efficient_rotation_matrices(x): B, C, E = x.shape cos_, sin_ = get_efficient_rotation_matrix(C, E) x1 = x[:, :, ::2] x2 = -x[:, :, 1::2] y = torch.stack((x2, x1), dim=-1).view(B, C, E) rotated = x * cos_ + y * sin_ return rotated def apply_rope(x, fast=True): if fast: return compute_efficient_rotation_matrices(x) B, C, E = x.shape return (x.view(B, C, 1, E) @ get_rotation_matrices(C, E)).view(B, C, E)  \n  \n---  \n  \n### Applying it in the Attention code\n\n    \n    \n    class MultiheadAttention(nn.Module): def __init__(self): # <...> def forward(self, x): B, C, E = x.shape # pre-layernorm x = self.ln(x) q, k, v = self.qkv(x).chunk(3, dim=-1) # --- RoPE --- q = apply_rope(q) k = apply_rope(k) # <...>  \n  \n---  \n  \n## ALiBi\n\nALiBi is also applied in the Attention block, but this time it\u2019s applied on\ntop of the Attention Scores instead of the Query and Key. It is meant to\nrepresent the relative distance between tokens instead of the absolute\nposition of a token in the sequence.\n\nIntuitively this is very seductive. Relative position means that it should not\n\u201cduplicate\u201d tokens, and in theory it should scale to arbitrary offsets (for\narbitrary context size). On the flip side it seems to me to be a very \u201cweak\u201d\nway to signal position. It influences how relevant a token is to the current\none based on distance. This seems pretty fuzzy, and not a robust way to encode\ndistance.\n\n    \n    \n    # e.g.: 4-dimensional mask for decoder-only # [[ 0., -0., -0., -0.], # [-1., 0., -0., -0.], # [-2., -1., 0., -0.], # [-3., -2., -1., 0.]] # # e.g.: 4dimensional mask for encoder-only # [[ 0., -1., -2., -3.], # [-1., 0., -1., -2.], # [-2., -1., 0., -1.], # [-3., -2., -1., 0.]] def alibi_distances(n_context, is_encoder=True): diag_dists = torch.cumsum(torch.triu(torch.ones(n_context, n_context).to(device)), dim=1) * -1 diag_dists[diag_dists != 0] += 1 dists = diag_dists.transpose(-2, -1) if is_encoder: dists = dists + diag_dists return dists def alibi_scalers(n_heads): # vector with M values in a geometric sequence (starting at 2^-(8/n)) where n == number of heads. m_vector = 2 ** -((8 / n_heads) * torch.arange(1, n_heads + 1).to(device)) return m_vector.view(-1, 1, 1) # reshape to broadcast correctly def apply_alibi(correlation, is_encoder=True): n_context = correlation.shape[-1] n_heads = correlation.shape[-3] scaled_dists = alibi_scalers(n_heads) * alibi_distances(n_context, is_encoder) return correlation + scaled_dists class MultiheadAttention(nn.Module): def __init__(self): # <...> def forward(self, x): # <...> correlation = q @ k.transpose(-2, -1) correlation = correlation / math.sqrt(k.shape[-1]) # --- ALiBi --- correlation = apply_alibi(correlation) # --- Optionally apply causal mask --- if self.positional == Positional.ALIBI_AND_CAUSAL: mask = torch.tril(torch.ones(C, C)).to(device) correlation = correlation.masked_fill(mask == 0, float('-inf')) # <...>  \n  \n---  \n  \n## Compare Schemes Accuracies\n\nOn the toy task for a sample of 5 runs each, we have:\n\nPositional Encoding Schemes Accuracies on the toy copy-task\n\nI couldn\u2019t get ALiBi to produce good results on my task. This could be due to:\n\n  * Some bug in my code?\n  * The model being too small.\n  * The task could be a very bad fit for ALiBi.\n\nI was advised to use a Causal mask in combination with ALiBi which helped a\nlot but didn\u2019t place it on a par with the other three.\n\nI also experimented with using only powers of 2 number of multi-headed\nattention but it didn\u2019t close the gap with other schemes either.\n\n## Attention Activation\n\nLooking at the attention activations for an example is pretty cool :)\n\n    \n    \n    7 1 8 2 <copy> _ _ _ _ _ \u2192\n\n## Bonus Note about Transformers\n\nI spent a lot of time getting no results until I got the godsent advice that:\nOne-layer models are such a special case \u2013 they can\u2019t form induction heads.\n\nAnd you can read more about it in this Anthropic paper.\n\n## The code\n\nYou can get the code at https://github.com/peluche/self-attention\n\n## Sources\n\n  * Sinusoidal / Learned Attention Is All You Need\n  * RoPE RoFormer: Enhanced Transformer with Rotary Position Embedding\n  * ALiBi Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation\n  * Ofir Press\u2019 github, especially this issue, blog, and YouTube\n  * One-Layer Models being weird A Mathematical Framework for Transformer Circuits\n  * Nelhage\u2019s blog\n\nUpdated on 2023-12-09\n\npositional encoding, sinusoidal positional encoding, learned positional encoding, RoPE, ALiBi, transformer, attention, encoder, decoder, copy-task, EldronBack | Home\n\nGAN, WGAN, and Instance Noise ViT - Vision Transformer\n\n2023 - 2024 peluche\n\n", "frontpage": false}
