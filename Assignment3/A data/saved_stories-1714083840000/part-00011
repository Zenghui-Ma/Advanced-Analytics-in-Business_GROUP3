{"aid": "40158945", "title": "Llamafile's Progress, Four Months In", "url": "https://hacks.mozilla.org/2024/04/llamafiles-progress-four-months-in/", "domain": "hacks.mozilla.org", "votes": 5, "user": "feross", "posted_at": "2024-04-25 15:47:59", "comments": 0, "source_title": "Llamafile\u2019s progress, four months in", "source_text": "Llamafile\u2019s progress, four months in - Mozilla Hacks - the Web developer blog\n\n# Hacks\n\n# Llamafile\u2019s progress, four months in\n\n### By Stephen Hood\n\nPosted on April 25, 2024 in Developer Tools, Featured Article, and Firefox\n\nWhen Mozilla\u2019s Innovation group first launched the llamafile project late last\nyear, we were thrilled by the immediate positive response from open source AI\ndevelopers. It\u2019s become one of Mozilla\u2019s top three most-favorited repositories\non GitHub, attracting a number of contributors, some excellent PRs, and a\ngrowing community on our Discord server.\n\nThrough it all, lead developer and project visionary Justine Tunney has\nremained hard at work on a wide variety of fundamental improvements to the\nproject. Just last night, Justine shipped the v0.8 release of llamafile, which\nincludes not only support for the very latest open models, but also a number\nof big performance improvements for CPU inference.\n\nAs a result of Justine\u2019s work, today llamafile is both the easiest and fastest\nway to run a wide range of open large language models on your own hardware.\nSee for yourself: with llamafile, you can run Meta\u2019s just-released LLaMA 3\nmodel\u2013which rivals the very best models available in its size class\u2013on an\neveryday Macbook.\n\nHow did we do it? To explain that, let\u2019s take a step back and tell you about\neverything that\u2019s changed since v0.1.\n\ntinyBLAS: democratizing GPU support for NVIDIA and AMD\n\nllamafile is built atop the now-legendary llama.cpp project. llama.cpp\nsupports GPU-accelerated inference for NVIDIA processors via the cuBLAS linear\nalgebra library, but that requires users to install NVIDIA\u2019s CUDA SDK. We felt\nuncomfortable with that fact, because it conflicts with our project goal of\nbuilding a fully open-source and transparent AI stack that anyone can run on\ncommodity hardware. And besides, getting CUDA set up correctly can be a bear\non some systems. There had to be a better way.\n\nWith the community\u2019s help (here\u2019s looking at you, @ahgamut and @mrdomino!), we\ncreated our own solution: it\u2019s called tinyBLAS, and it\u2019s llamafile\u2019s brand-new\nand highly efficient linear algebra library. tinyBLAS makes NVIDIA\nacceleration simple and seamless for llamafile users. On Windows, you don\u2019t\neven need to install CUDA at all; all you need is the display driver you\u2019ve\nprobably already installed.\n\nBut tinyBLAS is about more than just NVIDIA: it supports AMD GPUs, as well.\nThis is no small feat. While AMD commands a respectable 20% of today\u2019s GPU\nmarket, poor software and driver support have historically made them a\nsecondary player in the machine learning space. That\u2019s a shame, given that\nAMD\u2019s GPUs offer high performance, are price competitive, and are widely\navailable.\n\nOne of llamafile\u2019s goals is to democratize access to open source AI\ntechnology, and that means getting AMD a seat at the table. That\u2019s exactly\nwhat we\u2019ve done: with llamafile\u2019s tinyBLAS, you can now easily make full use\nof your AMD GPU to accelerate local inference. And, as with CUDA, if you\u2019re a\nWindows user you don\u2019t even have to install AMD\u2019s ROCm SDK.\n\nAll of this means that, for many users, llamafile will automatically use your\nGPU right out of the box, with little to no effort on your part.\n\nCPU performance gains for faster local AI\n\nHere at Mozilla, we are keenly interested in the promise of \u201clocal AI,\u201d in\nwhich AI models and applications run directly on end-user hardware instead of\nin the cloud. Local AI is exciting because it opens up the possibility of more\nuser control over these systems and greater privacy and security for users.\n\nBut many consumer devices lack the high-end GPUs that are often required for\ninference tasks. llama.cpp has been a game-changer in this regard because it\nmakes local inference both possible and usably performant on CPUs instead of\njust GPUs.\n\nJustine\u2019s recent work on llamafile has now pushed the state of the art even\nfurther. As documented in her detailed blog post on the subject, by writing 84\nnew matrix multiplication kernels she was able to increase llamafile\u2019s prompt\nevaluation performance by an astonishing 10x compared to our previous release.\nThis is a substantial and impactful step forward in the quest to make local AI\nviable on consumer hardware.\n\nThis work is also a great example of our commitment to the open source AI\ncommunity. After completing this work we immediately submitted a PR to\nupstream these performance improvements to llama.cpp. This was just the latest\nof a number of enhancements we\u2019ve contributed back to llama.cpp, a practice we\nplan to continue.\n\nRaspberry Pi performance gains\n\nSpeaking of consumer hardware, there are few examples that are both more\ninteresting and more humble than the beloved Raspberry Pi. For a bargain\nbasement price, you get a full-featured computer running Linux with plenty of\ncomputing power for typical desktop uses. It\u2019s an impressive package, but\nhistorically it hasn\u2019t been considered a viable platform for AI applications.\n\nNot any more. llamafile has now been optimized for the latest model (the\nRaspberry Pi 5), and the result is that a number of small LLMs\u2013such as\nRocket-3B (download), TinyLLaMA-1.5B (download), and Phi-2 (download)\u2013run at\nusable speeds on one of the least expensive computers available today. We\u2019ve\nseen prompt evaluation speeds of up to 80 tokens/sec in some cases!\n\nKeeping up with the latest models\n\nThe pace of progress in the open model space has been stunningly fast. Over\nthe past few months, hundreds of models have been released or updated via\nfine-tuning. Along the way, there has been a clear trend of ever-increasing\nmodel performance and ever-smaller model sizes.\n\nThe llama.cpp project has been doing an excellent job of keeping up with all\nof these new models, frequently rolling-out support for new architectures and\nmodel features within days of their release.\n\nFor our part we\u2019ve been keeping llamafile closely synced with llama.cpp so\nthat we can support all the same models. Given the complexity of both\nprojects, this has been no small feat, so we\u2019re lucky to have Justine on the\ncase.\n\nToday, you can today use the very latest and most capable open models with\nllamafile thanks to her hard work. For example, we were able to roll-out\nllamafiles for Meta\u2019s newest LLaMA 3 models\u20138B-Instruct and\n70B-Instruct\u2013within a day of their release. With yesterday\u2019s 0.8 release,\nllamafile can also run Grok, Mixtral 8x22B, and Command-R.\n\nCreating your own llamafiles\n\nSince the day that llamafile shipped people have wanted to create their own\nllamafiles. Previously, this required a number of steps, but today you can do\nit with a single command, e.g.:\n\nllamafile-convert [model.gguf]\n\nIn just moments, this will produce a \u201cmodel.llamafile\u201d file that is ready for\nimmediate use. Our thanks to community member @chan1012 for contributing this\nhelpful improvement.\n\nIn a related development, Hugging Face recently added official support for\nllamafile within their model hub. This means you can now search and filter\nHugging Face specifically for llamafiles created and distributed by other\npeople in the open source community.\n\nOpenAI-compatible API server\n\nSince it\u2019s built on top of llama.cpp, llamafile inherits that project\u2019s server\ncomponent, which provides OpenAI-compatible API endpoints. This enables\ndevelopers who are building on top of OpenAI to switch to using open models\ninstead. At Mozilla we very much want to support this kind of future: one\nwhere open-source AI is a viable alternative to centralized, closed,\ncommercial offerings.\n\nWhile open models do not yet fully rival the capabilities of closed models,\nthey\u2019re making rapid progress. We believe that making it easier to pivot\nexisting code over to executing against open models will increase demand and\nfurther fuel this progress.\n\nOver the past few months, we\u2019ve invested effort in extending these endpoints,\nboth to increase functionality and improve compatibility. Today, llamafile can\nserve as a drop-in replacement for OpenAI in a wide variety of use cases.\n\nWe want to further extend our API server\u2019s capabilities, and we\u2019re eager to\nhear what developers want and need. What\u2019s holding you back from using open\nmodels? What features, capabilities, or tools do you need? Let us know!\n\nIntegrations with other open source AI projects\n\nFinally, it\u2019s been a delight to see llamafile adopted by independent\ndevelopers and integrated into leading open source AI projects (like Open\nInterpreter). Kudos in particular to our own Kate Silverstein who landed PRs\nthat add llamafile support to LangChain and LlamaIndex (with AutoGPT coming\nsoon).\n\nIf you\u2019re a maintainer or contributor to an open source AI project that you\nfeel would benefit from llamafile integration, let us know how we can help.\n\nJoin us!\n\nThe llamafile project is just getting started, and it\u2019s also only the first\nstep in a major new initiative on Mozilla\u2019s part to contribute to and\nparticipate in the open source AI community. We\u2019ll have more to share about\nthat soon, but for now: I invite you to join us on the llamafile project!\n\nThe best place to connect with both the llamafile team at Mozilla and the\noverall llamafile community is over at our Discord server, which has a\ndedicated channel just for llamafile. And of course, your enhancement\nrequests, issues, and PRs are always welcome over at our GitHub repo.\n\nI hope you\u2019ll join us. The next few months are going to be even more\ninteresting and unexpected than the last, both for llamafile and for open\nsource AI itself.\n\n## About Stephen Hood\n\nStephen leads open source AI projects (including llamafile) in Mozilla's\nInnovation group. He previously managed social bookmarking pioneer\ndel.icio.us; co-founded Storium, Blockboard, and FairSpin; and worked on Yahoo\nSearch and BEA WebLogic.\n\n  * https://stephenhood.com\n\nMore articles by Stephen Hood...\n\n## Thanks! Please check your inbox to confirm your subscription.\n\nIf you haven\u2019t previously confirmed a subscription to a Mozilla-related\nnewsletter you may have to do so. Please check your inbox or your spam filter\nfor an email from us.\n\n### No comments yet\n\nExcept where otherwise noted, content on this site is licensed under the\nCreative Commons Attribution Share-Alike License v3.0 or any later version.\n\n", "frontpage": true}
