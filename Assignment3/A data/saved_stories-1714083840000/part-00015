{"aid": "40159018", "title": "How to Run Llama 3 with AIME API to Deploy Conversational AI Solutions", "url": "https://www.aime.info/blog/en/how-to-run-llama-3-with-aime-api-to-deploy-conversational-ai-solutions/", "domain": "aime.info", "votes": 1, "user": "headkit", "posted_at": "2024-04-25 15:52:38", "comments": 0, "source_title": "How to run Llama 3 with AIME API to Deploy Conversational AI Solutions", "source_text": "How to run Llama 3 with AIME API to Deploy Conversational AI Solutions\n\n# How to run Llama 3 with AIME API to Deploy Conversational AI Solutions\n\nWith LLaMa 3 you get a collection of pretrained state-of-the-art large\nlanguage models for free. Unlike the well-known ChatGPT, LLaMa models are\ndownloadable and compatible with existing hardware for execution.\n\nTable of contents\n\n  1. What is new with Llama 3\n  2. Hardware Requirements\n  3. Our Llama 3 Implementation\n  4. Try out Llama 3 Chat with AIME API Demonstrator\n  5. Getting Started: How to Deploy Llama 3 Chat\n\n    1. Create an AIME ML Container\n    2. Clone the Llama3-Chat Repository\n    3. Download Llama 3 Model Checkpoints\n    4. Convert the Checkpoints to your GPU Configuration\n    5. Run Llama 3 as Interactive Chat in the Terminal\n    6. Run Llama 3 Chat as Service with the AIME API Server\n  6. Llama 3 Inference GPU Benchmarks\n\n    1. Results\n  7. Conclusion\n\nIn our previous articles about the Llama models (Llama 2, Llama 1), we delved\ninto the details of running Llama as console application as well as deploying\nit via the AIME API Server. Embracing the next step with Llama 3, the most\nrecent advancement in the realm of open large language models, we will now\nprovide a comprehensive guide on how to set up Llama 3 and operate it with the\nAIME API server, opening up new possibilities for delivering AI conversational\nexperiences.\n\nAt AIME, we understand the importance of providing AI tools as services to\nharness the full potential of technologies like Llama. That's why we're\nexcited to announce the integration of Llama 3 with our new AIME-API as one of\nthe first providers, offering developers a streamlined solution for delivering\nits capabilities as a scalable HTTP/HTTPS service for easy integration with\nclient applications. Whether you're building chatbots, virtual assistants, or\ninteractive customer support systems, this integration offers flexibility to\nmake modifications and adjustments to the model and the scalability to deploy\nsuch solutions.\n\n## What is new with Llama 3\n\nAs you may already know, there are two previous versions of the Llama model -\nso what is the advantage of Llama 3 over the older versions?\n\nThe main improvement is the 8K context length, which doubles the previous 4K\ncontext length of Llama 2 and quadruples the 2K of the original Llama models.\n8K context length equals about 6.000 words which represents about 12 standard\nA4 (letter size) pages of text. This is a good length to operate a chat bot\nwith a detailed instructions manual or playbook to cover lengthy\nconversations, remembering every detail of the conversation.\n\nAlthough the model sizes did not increase dramaticly due to an improved and\nbetter curated training data set - a training dataset 7x larger than that used\nfor Llama 2 - all Llama 3 models show a huge improvement in nearly all LLM\nbenchmarks, putting it on par with most closed source state-of-the-art models.\n\nChart taken from Meta Blog article\n\nAlso chatting with Llama 3 one notices a far more friendly, helpful and\nenganging large language model than the previous versions.\n\nThe 7B model was tuned to an 8B model to make better use of 24 GB GPU memory\nwhich has established as the standard on entry deep learning GPUs. The 13B\nmodel was dropped for the improved 70B model.\n\nThere are rumours that Meta is still computing on an even larger model to be\nreleased later in the year. Let's wait and see.\n\n## Hardware Requirements\n\nAlthough the Llama 3 models were trained on a cluster of H100 80GB GPUs it is\npossible to run the models on different and smaller multi-GPU hardware for\ninference.\n\nIn table 1 we list a summary of the minimum GPU requirements and recommended\nAIME systems to run a specific Llama 3 model with realtime reading\nperformance:\n\nModel| Size| Minimum GPU Configuration| Recommended AIME Server| Recommended\nAIME Cloud Instance  \n---|---|---|---|---  \n8B| 15GB| 1x NVIDIA RTX A5000 24GB or 1x NVIDIA RTX 4090 24GB| AIME G400\nWorkstation or AIME A4000 Server| V10-1XA5000-M6  \n70B| 132GB| 2x NVIDIA H100/A100 80GB, 4x NVIDIA RTX A6000/6000 Ada 48GB or 8x\nNVIDIA RTX A5000 24GB| AIME A4004 Server| V28-2XA180-M6, C24-4X6000ADA-Y1,\nC32-8XA5000-Y1  \n  \nTable 1: Summary of the minimum GPU requirements and recommended AIME systems\nto run a specific Llama 3 model with at least realtime reading performance\n\nA detailed performance analysis of different hardware configurations can be\nfound in the section \"Llama 3 Inference GPU Benchmarks\" of this article.\n\n## Our Llama 3 Implementation\n\nOur Llama 3 implementation is a fork of the original Llama 3 repository for\nPytorch supporting all current Llama 3 model sizes: 8B and 70B and the\nInstruct fine tuned versions of the models.\n\nOur fork provides the possibility to convert the weights to be able to run the\nmodel on different GPU configurations than the official model weights provided\nby Meta for Llama 3 (see table 2).\n\nWith the integration in our AIME-API and its batch aggregation feature, it is\npossible to leverage the full possible inference performance of different GPU\nconfigurations by aggregating requests to be processed in parallel as batch\njobs by the GPUs. This dramaticly increases the achievable throughput of chat\nrequests.\n\nWe also benchmarked the different GPU configurations with this technology to\nguide the decision which hardware gives best throughput performance to process\nLlama 3 requests.\n\n## Try out Llama 3 Chat with AIME API Demonstrator\n\nThe AIME-API-Server offers client interfaces for various programming\nlanguages. To demonstrate the power of Llama 3 with the AIME API Server, we\nprovide a fully working Llama 3 demonstrator using our Java Script client\ninterface.\n\nAIME API Llama 3 Demonstrator\n\n## Getting Started: How to Deploy Llama 3 Chat\n\nIn the following we show how to setup, get the source, download the Llama 3\nmodels, and how to run Llama 3 as console application and as worker for\nserving HTTPS/HTTP requests.\n\n### Create an AIME ML Container\n\nHere are instructions for setting up the environment using the AIME ML\ncontainer management. Similar results can be achieved by using Conda or alike.\n\nTo create a PyTorch 2.1.2 environment for installation, we use the AIME ML\ncontainer, as described in aime-ml-containers using the following command:\n\n    \n    \n    > mlc-create llama3 Pytorch 2.1.2-aime -w=/path/to/your/workspace -d=/destination/of/model/checkpoints\n\nThe -d parameter is only necessary if you don\u2019t want to store the checkpoints\nin your workspace but in a specific data directory. It is mounting the folder\n/destination/of/model/checkpoints to /data in the container. This folder\nrequires at least 250 GB of free storage to store (all) the Llama 3 models.\n\nOnce the container is created, open it with:\n\n    \n    \n    > mlc-open llama3\n\nYou are now working in the ML container being able to install all requirements\nand necessary pip and apt packages without interfering with the host system.\n\n### Clone the Llama3-Chat Repository\n\nLlama3-Chat is a forked version of the original Llama 3 reference\nimplementation by AIME, with the following added features:\n\n  * Tool for converting the original model checkpoints to different GPU configurations\n  * Improved text sampling\n  * Implemented token-wise text output\n  * Interactive console chat\n  * AIME API Server integration\n\nClone our Llama3-Chat repository with:\n\n    \n    \n    [llama3] user@client:/workspace$ > git clone https://github.com/aime-labs/llama3_chat\n\nNow install the required pip packages:\n\n    \n    \n    [llama3] user@client:/workspace$ > pip install -r /workspace/llama3_chat/requirements.txt\n\n### Download Llama 3 Model Checkpoints\n\nIn order to download the model weights and tokenizer, you have to apply for\naccess by meta and accept their license. Once your request is approved, you\nwill receive a signed URL via e-mail. Make sure you have wget and md5sum\ninstalled in the container:\n\n    \n    \n    [llama3] user@client:/workspace$ > sudo apt-get install wget md5sum\n\nThen run the download.sh script with:\n\n    \n    \n    [llama3] user@client:/workspace$ > /workspace/download.sh\n\nPass the URL provided when prompted to start the download. Keep in mind that\nthe links expire after 24 hours and a certain amount of downloads. If you\nstart seeing errors such as 403: Forbidden , you can always re-request a link.\n\nThe download process might take a while depending on your internet connection\nspeed. For the 70B model 129GB are to be downloaded.\n\n### Convert the Checkpoints to your GPU Configuration\n\nThe downloaded checkpoints are designed to only work with certain GPU\nconfigurations: 8B with 1 GPU and 70B with 8 GPUs. To run the model on a\ndifferent GPU configuration, we provide a tool to convert the weights\nrespectively. Table 2 shows all supported GPU configs.\n\nModel size| Num GPUs 24GB| Num GPUs 40GB| Num GPUs 48GB| Num GPUs 80GB  \n---|---|---|---|---  \n8B| 1| 1| 1| 1  \n70B| 8| 4| 4| 2  \n  \nTable 2: This table shows the required amount of GPUs to run the desired Llama\n3 model size depending on the available GPU memory.\n\nThe converting can be started with:\n\n    \n    \n    [llama3] user@client:/workspace$ > python3 /workspace/llama3_chat/convert_weights.py --input_dir /data/models/Meta-Llama-3-70B-Instruct/ --model_size 70B --num_gpus <num_gpus>\n\nThe converting will take some minutes depending on CPU and storage speed and\nmodel size.\n\n### Run Llama 3 as Interactive Chat in the Terminal\n\nTo try the Llama 3 models as a chat bot in the terminal, use the following\nPyTorch script and specify the desired model size to use:\n\n    \n    \n    [llama3] user@client:/workspace$ > torchrun --nproc_per_node <num_gpus> /workspace/llama3_chat/chat.py --ckpt_dir /data/models/Meta-Llama-3-70B-Instruct\n\nThe chat mode is simply initiated by giving the following context as the\nstarting prompt. It sets the environment so that the language model tries to\ncomplete the text as a chat dialog:\n\n> A dialog, where User interacts with a helpful, kind, obedient, honest and\n> very reasonable assistant called Steve. User: Hello, Steve Steve: How can I\n> assist you today?\n\nNow the model acts as a simple chat bot. The starting prompt influences the\nmood of the chat answers. In this case, he credibly fills the role of a\nhelpful assistant and does not leave it again without further ado.\nInteresting, funny to useful answers emerge - depending on the input texts.\n\n### Run Llama 3 Chat as Service with the AIME API Server\n\nSetup AIME API Server To read about how to setup and start the AIME API Server\nplease see the setup section in the documentation.\n\nStart Llama 3 as API Worker To connect the LLama3-chat model with the AIME-\nAPI-Server ready to receive chat requests through the JSON HTTPS/HTTP\ninterface, simply add the flag --api_server followed by the address of the\nAIME API Server to connect to:\n\n    \n    \n    [llama2] user@client:/workspace$ > torchrun --nproc_per_node <num_gpus> /workspace/llama3_chat/chat.py --ckpt_dir /data/models/Meta-Llama-3-70B-Instruct/ --api_server https://api.aime.info/\n\nNow the LLama 3 model acts as a worker instance for the AIME API Server ready\nto process jobs ordered by clients of the AIME API Server.\n\nFor a documentation on how to send client requests to the AIME API Server\nplease read on here.\n\n## Llama 3 Inference GPU Benchmarks\n\nTo measure the performance of your LLaMA 3 worker connected to the AIME API\nServer, we developed a benchmark tool as part of our AIME API Server to\nsimulate and stress the server with the desired amount of chat requests. First\ninstall the requirements with:\n\n    \n    \n    user@client:/workspace/aime-api-server/$ > pip install -r requirements_api_benchmark.txt\n\nThen the benchmark tool can be started with:\n\n    \n    \n    user@client:/workspace/aime-api-server/$ > python3 run_api_benchmark.py --api_server https://api.aime.info/ --total_requests <number_of_requests>\n\nrun_api_benchmark.py will send as many requests as possible in parallel and up\nto <number_of_requests> chat requests to the API server. Each requests starts\nwith the initial context \"Once upon a time\". Llama 2 will generate, depending\non the used model size, a story of about 400 to 1000 tokens length. The\nprocessed tokens per second are measured and averaged over all processed\nrequests.\n\n### Results\n\nWe show the results of the different Llama 3 model sizes and GPU\nconfigurations. The model is loaded in such a way that it can use the GPUs in\nbatch mode. In the bar charts the maximum possible batch size, which equals\nthe parallel processable chat sessions, is stated below the GPU model and is\ndirectly related to the available GPU memory.\n\nThe results are shown as total possible throughput in tokens per second and\nthe smaler bar shows the tokens per second for an individual chat session.\n\nThe human (silent) reading speed is about 5 to 8 words per second. With a\nratio of 0.75 words per token, it is comparable to a required text generation\nspeed of about 6 to 11 tokens per second to be experienced as not to slow.\n\nLlama 3 8B GPU Performance\n\nLlama 3 70B GPU Performance\n\n## Conclusion\n\nIntegrating Llama 3 with AIME API and its straightforward setup offers\ndevelopers a scalable solution for deploying conversational AI solutions. By\nutilizing Llama 3 alongside the AIME API Server, developers can create\nadvanced applications like chatbots, virtual assistants, and interactive\ncustomer support systems.\n\nThe hardware requirements for running Llama 3 models are flexible, allowing\nfor deployment on various distributed GPU configurations and extendable setups\nor infrastructure to serve thousands of requests. Even the minimum GPU\nrequirements or smallest recommended AIME systems for each model size ensure a\nmore than real-time reading response performance for a seamless multi-user\nchat experience.\n\n## Spread the word\n\n  * Share\n  * Tweet\n  * Share\n  * Email\n\npublic\n\nNext article\n\n## AIME API - The Scalable AI Model Inference Solution\n\npublic\n\nPrevious article\n\n## Llama 3 als Conversational-AI mittels AIME-API-Server betreiben\n\n## Keep reading...\n\npublic\n\n### AIME API - The Scalable AI Model Inference Solution\n\n7 min read\n\npublic\n\n### Deep Learning GPU Benchmarks\n\n10 min read\n\npublic\n\n### Deploy LLaMa 2 with AIME API Server for Operation of Conversational AI\nSolutions\n\n8 min read\n\n  * AIME Blog\n\n  * Imprint\n\n  * Terms\n\n  * Privacy Policy\n\n  * Right of Withdrawal\n\n  * Mastodon\n\n  * Bluesky\n\n  * LinkedIn\n\n  * GitHub\n\n  * Jobs\n\n\u00a9 AIME Website 2024. All Rights Reserved.\n\n  * English\n  * Deutsch\n\n", "frontpage": false}
