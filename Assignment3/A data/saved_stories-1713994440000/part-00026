{"aid": "40145156", "title": "OpenELM: Efficient Language Model Family with Open-Source Training and Inference", "url": "https://huggingface.co/papers/2404.14619", "domain": "huggingface.co", "votes": 1, "user": "gorbypark", "posted_at": "2024-04-24 14:58:55", "comments": 0, "source_title": "Paper page - OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework", "source_text": "Paper page - OpenELM: An Efficient Language Model Family with Open-source\nTraining and Inference Framework\n\nHugging Face\n\nPapers\n\narxiv:2404.14619\n\n# OpenELM: An Efficient Language Model Family with Open-source Training and\nInference Framework\n\nPublished on Apr 22\n\n\u00b7 Featured in Daily Papers on Apr 24\n\nUpvote\n\n49\n\nAuthors:\n\nSachin Mehta ,\n\nMohammad Hossein Sekhavat ,\n\nQingqing Cao ,\n\nMaxwell Horton ,\n\nYanzi Jin ,\n\n,\n\nIman Mirzadeh ,\n\nMahyar Najibi ,\n\nDmitry Belenko ,\n\n,\n\nMohammad Rastegari\n\n## Abstract\n\nThe reproducibility and transparency of large language models are crucial for\nadvancing open research, ensuring the trustworthiness of results, and enabling\ninvestigations into data and model biases, as well as potential risks. To this\nend, we release OpenELM, a state-of-the-art open language model. OpenELM uses\na layer-wise scaling strategy to efficiently allocate parameters within each\nlayer of the transformer model, leading to enhanced accuracy. For example,\nwith a parameter budget of approximately one billion parameters, OpenELM\nexhibits a 2.36% improvement in accuracy compared to OLMo while requiring\n2times fewer pre-training tokens. Diverging from prior practices that only\nprovide model weights and inference code, and pre-train on private datasets,\nour release includes the complete framework for training and evaluation of the\nlanguage model on publicly available datasets, including training logs,\nmultiple checkpoints, and pre-training configurations. We also release code to\nconvert models to MLX library for inference and fine-tuning on Apple devices.\nThis comprehensive release aims to empower and strengthen the open research\ncommunity, paving the way for future open research endeavors. Our source code\nalong with pre-trained model weights and training recipes is available at\nhttps://github.com/apple/corenet. Additionally, \\model models can be found on\nHuggingFace at: https://huggingface.co/apple/OpenELM.\n\nView arXiv page View PDF Add to collection\n\n### Community\n\nbhimrazy\n\nabout 11 hours ago\n\nGreat to see developments in the small language model family.\n\nMichaelBarryUK\n\nabout 6 hours ago\n\n\u201cI sense a great disturbance in the source, as if millions of developers\nsuddenly cried out in excitement and were suddenly empowered. I fear something\nremarkable has happened. The \u2018Apples\u2019 and \u2018Metas\u2019 of the tech empire have\nopened their vaults, joining the open-source Resistance. This is the beginning\nof a new collaboration, a new hope for innovation.\u201d \ud83c\udf0c\ud83c\udf4f\ud83d\udcbb May the source be with\nyou\n\nKatastropika\n\nabout 2 hours ago\n\nApple realized their golden age of being considered the king of innovation is\nlong over. Now they're lagging far behind in the generative AI race.\n\n\u00b7 Sign up or log in to comment\n\nUpvote\n\n49\n\n## Models citing this paper 10\n\nBrowse 10 models citing this paper\n\n## Datasets citing this paper 0\n\nNo dataset linking this paper\n\nCite arxiv.org/abs/2404.14619 in a dataset README.md to link it from this\npage.\n\n### Spaces citing this paper 0\n\nNo Space linking this paper\n\nCite arxiv.org/abs/2404.14619 in a Space README.md to link it from this page.\n\n## Collections including this paper 22\n\n", "frontpage": false}
