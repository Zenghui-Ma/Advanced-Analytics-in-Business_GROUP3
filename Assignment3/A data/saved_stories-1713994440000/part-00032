{"aid": "40145224", "title": "Fixing Serverless with a $5 VPS", "url": "https://www.unkey.com/blog/fixing-serverless-with-a-vps", "domain": "unkey.com", "votes": 2, "user": "jamesperkins", "posted_at": "2024-04-24 15:05:00", "comments": 0, "source_title": "Fixing serverless with a $5 VPS | Unkey", "source_text": "Fixing serverless with a $5 VPS | Unkey\n\nBlog/engineering\n\n# Fixing serverless with a $5 VPS\n\nHow we are supporting our serverless API with servers to scale beyond the\nlimits of serverless.\n\nWritten by\n\nAndreas Thomas\n\nPublished on\n\nApr 24, 2024\n\nServerless solved many issues and created many more. It's a great way to build\napplications that scale, but it can only scale as far as your other services\ncan. For example, if you're using a serverless function to process data, and\nthat data is coming from a database that can only handle 100 requests per\nsecond, then your serverless function is effectively capped at 100 requests\nper second as well.\n\nSo why do we use serverless if it's limited by servers? Well in our case, we\nuse Cloudflare workers for our API for their global low latency. That's it,\neverything else we build around it, is just to make it work.\n\n## The problem\n\nOur API emits multiple different events, such as the outcome of a\nverification, or a rate-limit being hit. We use Tinybird for all of our\nanalytics, and we were sending each event individually to Tinybird. This is\neasy on the worker's side because you just fire and forget.\n\n    \n    \n    1executionContext.waitUntil(tinybird.ingestKeyVerification({ ... }))\n\nThe problem is that Tinybird has a 1k requests per second ingest limit, and we\nwere hitting that limit regularly. The obvious solution is to batch the events\nand send them in a single request\n\nThat's where the rabbit hole started...\n\n### Batching in Cloudflare workers\n\nCloudflare workers are great, but they are not designed for batching. You\ncan't just wait for a few seconds and then send a batch of events to Tinybird.\nYou have to send the events as they come in, or you risk losing the event as\nthe underlying worker can be recycled at any time after the execution ended.\n\nLet's look at the options I explored:\n\n  * Cloudflare Queues, D1 or Durable Objects\n  * Cloudflare Logpush\n  * Cloudflare Analytics Engine\n  * Clickhouse Cloud\n  * Kafka\n  * Tinybird Enterprise\n  * VPS\n\n### Cloudflare Queues, D1 or Durable Objects\n\nI group these 3, cause they all solve the problem pretty well, but are just\ntoo expensive for our use case. Here's some napkin math to illustrate my\npoint:\n\nWorker invocations are charged at $0,30 per million, there's also a charge for\nCPU time, so let's just say we're paying around 40 cents per million requests.\nCloudflare queues are $1.20 per million messages (assuming you don't need to\nretry), D1 and Durable Objects are similarly priced.\n\nEach worker invocation produces on average 2 events, but inside the worker, we\ncan batch them and send a single message through the queue. So for 1 million\nAPI requests, we'd be paying $0.40 for the worker invocations, and $1.20 for\nthe queue, for a total of $1.60. Queues are just too expensive relative to the\ncost of the worker invocations.\n\nNow $1.60 per million requests doesn't sound like a lot, but we need to have\ngood margins and then our customers would have to add more margin on top of\nthat. We want to make building APIs more accessible to everyone, so we need to\nkeep our costs low.\n\n### Cloudflare Logpush\n\nCloudflare Logpush is pretty nice for sending logs to your sink. The problem\nis that it's pretty slow, we're seeing around 2-3 min of delay between logging\nsomething and it appearing in our sink. This is not acceptable for our real-\ntime analytics.\n\nThere are also some other limitations that make it hard to use:\n\n  * Message size: Maximum of 2056 characters per log line (That's not really that much data if you encode it in JSON)\n  * Array limit: 20 elements (I still don't know what this refers to)\n  * Log message array: A nested array with a limit of three elements\n\nSource\n\nWe do use Logpush for other internal metrics to aggregate and send them to\nAxiom.co, where the latency doesn't matter as much and we'd be fine to drop\nsome events if we exceed some limits. Let us know if you'd like to see a blog\npost about that implementation.\n\n### Clickhouse Cloud\n\nI was surprised how good clickhouse cloud was, I expected an AWS-like\nexperience but they made it pretty nice and easy to use. The problem for us is\nthat it didn't work so well with thousands of small requests per second.\nClickhouse really likes receiving large batches of 10k-100k rows in a single\nrequest, so it can optimize the writes to disk, and we need to do the\nopposite. Source\n\n### Cloudflare Analytics Engine\n\nWhen I first heard about the Analytics Engine, I thought it was the solution\nto all of our problems. Clickhouse natively on Cloudflare sounds like a dream\ncome true. The reality is far from that though. At Cloudflare's scale both in\nterms of usage but also in terms of the number of different customer needs,\nit's hard to build a product that fits everyone. They made some harsh\ntradeoffs to make it work for everyone, and in doing that made it only work\nfor a few. At least in its current state. I'm happy to look past the API\ndesign and just write a wrapper around it, but I was just bouncing from one\nundocumented problem to the next.\n\nAnyways, Analytics Engine would have been a pretty nice solution due to its\npricing. It's $0.25 per million rows ingested and $1 per million queries\n(regardless of the number of rows returned). This is a pretty good deal, and\nmy idea was to use it as a buffer. I'd ingest the events into the Analytics\nEngine and then run a SELECT * query every few seconds to get the events and\nsend them to Tinybird. The problem with that is: how do I know which events\nI've already processed without having to keep track of every single row? I\ntried a few different approaches, but they all came down to the fact that\nthere is no guarantee of how quickly Cloudflare commits rows to the Analytics\nEngine. It could take a few seconds, or even a minute, making it hard to build\na reliable system to get the events out of it.\n\n### Kafka\n\nKafka would be pretty good for this use case, but sending many small messages\nto Kafka over HTTP is not really what it's designed for. We're back to the\nsame problem of not being able to batch the events in the worker. I've heard\nfrom teams who run Cloudflare workers + Kafka in production right now, and\nthey're not happy with the solution as they scale up. I have not done my own\nresearch on this but instead trusted their judgment.\n\n### Tinybird Enterprise\n\nPerhaps the most obvious solution would be to just ask Tinybird to raise the\nlimit for us. So we did. They were very helpful and offered a custom solution,\nalbeit at a price point that is not commercially viable for us yet. We don't\nneed the full Tinybird enterprise experience, we just need a higher request\nlimit. All other metrics are well within the current limits.\n\n## The solution\n\nIf we can't batch the events in a serverless function, then let's do it on a\nserver instead. We've created a simple Go application that is api-compatible\nwith Tinybird's /v0/events API. It accepts NDJSON events, buffers them, and\nthen sends a batch to Tinybird in a single request. Because it's compatible,\nwe don't need to change anything in our worker code, we just use a different\nTINYBIRD_URL and TINYBIRD_TOKEN and we're good to go.\n\n### Deployment\n\nWe chose to deploy this proxy on Koyeb because it's super easy and they have\nautoscaling. I know others do too, but we were already familiar with Koyeb. We\njust push our code to GitHub, and Koyeb takes care of the rest.\n\nIf you want to run this yourself, just choose a VPS provider you're\ncomfortable with, and run the docker container.\n\n### Quickstart\n\nAll you need is docker and your Tinybird token. You can run it like this:\n\n    \n    \n    1docker run -p 8000:8000 -e TINYBIRD_TOKEN=\"abc\" ghcr.io/unkeyed/tinybird-proxy:latest\n\n### Config\n\nAll configuration is done via environment variables.\n\nSee the readme for all available options.\n\nBy default, the proxy will try to send batches of max 100k rows or every\nsecond, whichever happens first. The buffer size is 1 million events, to\nminimize data loss in case of a machine failure. If the buffer is full, the\nproxy will not accept more data until it's flushed.\n\n### Performance\n\nWe've tested this with over 8k RPS and it barely broke a sweat. We're\ncurrently running this on 1-4 autoscaling instances with 2 vCPUs and 2GB of\nRAM.\n\n## Conclusion\n\nUsing a proxy solved all of our problems:\n\n  * Supports our current and expected load for the foreseeable future\n  * Low latency for our real-time analytics\n  * Cheap (less than $50 per month)\n  * Easy to scale\n\nWe're still using serverless where it makes sense and Cloudflare workers play\na critical role in ensuring our API is fast and reliable, but some of the\nsupporting infrastructure can be run on servers for a fraction of the cost.\n\nWritten by\n\nAndreas Thomas\n\nPublished on\n\nApr 24, 2024\n\nContents\n\n  * The problem\n  * Batching in Cloudflare workers\n  * Cloudflare Queues, D1 or Durable Objects\n  * Cloudflare Logpush\n  * Clickhouse Cloud\n  * Cloudflare Analytics Engine\n  * Kafka\n  * Tinybird Enterprise\n  * The solution\n  * Deployment\n  * Quickstart\n  * Config\n  * Performance\n  * Conclusion\n\nSuggested\n\nHow to build authentic communication in your team\n\nFeb 23, 2024\n\nDecoding CLI Authentication\n\nFeb 07, 2024\n\nHigh frequency real-time usage based billing\n\nMar 01, 2024\n\n## Protect your API. Start today.\n\nChat with us\n\nStart Now\n\n2500 verifications and 100K successful rate\u2010limited requests per month. No CC\nrequired.\n\nBuild better APIs faster.\n\nUnkeyed, Inc. 2024\n\nCompany\n\n  * About\n  * Blog\n  * Changelog\n  * Templates\n  * Analytics\n  * Source Code\n  * Docs\n\nConnect\n\n  * X (Twitter)\n  * Discord\n  * GitHub\n  * OSS Friends\n  * Book a Call\n\nLegal\n\n  * Terms of Service\n  * Privacy Policy\n\n", "frontpage": false}
