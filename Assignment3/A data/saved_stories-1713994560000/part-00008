{"aid": "40145306", "title": "Snowflake Arctic Instruct Open LLM", "url": "https://huggingface.co/Snowflake/snowflake-arctic-instruct", "domain": "huggingface.co", "votes": 1, "user": "rgbrgb", "posted_at": "2024-04-24 15:11:42", "comments": 0, "source_title": "Snowflake/snowflake-arctic-instruct \u00b7 Hugging Face", "source_text": "Snowflake/snowflake-arctic-instruct \u00b7 Hugging Face\n\nHugging Face\n\n#\n\nSnowflake\n\n/\n\nsnowflake-arctic-instruct\n\nText Generation Transformers Safetensors arctic snowflake Mixture of Experts\nconversational Inference Endpoints\n\nModel card Files Files and versions Community\n\n4\n\nEdit model card\n\n## Model Details\n\nArctic is a dense-MoE Hybrid transformer architecture pre-trained from scratch\nby the Snowflake AI Research Team. We are releasing model checkpoints for both\nthe base and instruct-tuned versions of Arctic under an Apache-2.0 license.\nThis means you can use them freely in your own research, prototypes, and\nproducts. Please see our blog Snowflake Arctic: The Best LLM for Enterprise AI\n\u2014 Efficiently Intelligent, Truly Open for more information on Arctic and links\nto other relevant resources such as our series of cookbooks covering topics\naround training your own custom MoE models, how to produce high-quality\ntraining data, and much more.\n\n  * Arctic-Base\n  * Arctic-Instruct\n\nFor the latest details about Snowflake Arctic including tutorials, etc. please\nrefer to our github repo:\n\n  * https://github.com/Snowflake-Labs/snowflake-arctic\n\nModel developers Snowflake AI Research Team\n\nLicense Apache-2.0\n\nInput Models input text only.\n\nOutput Models generate text and code only.\n\nModel Release Date April, 24th 2024.\n\n## Model Architecture\n\nArctic combines a 10B dense transformer model with a residual 128x3.66B MoE\nMLP resulting in 480B total and 17B active parameters chosen using a top-2\ngating. For more details about Arctic's model Architecture, training process,\ndata, etc. see our series of cookbooks.\n\n## Usage\n\nAs of 4/24/2024 we are actively working with the maintainers of transformers\nto include the Arctic model implementation. Until this support is released\nplease follow these instructions to get the required dependencies for using\nArctic:\n\n    \n    \n    pip install git+https://github.com/Snowflake-Labs/transformers.git@arctic\n\nArctic leverages several features from DeepSpeed, you will need to install the\nlatest version of DeepSpeed to get all of these required features:\n\n    \n    \n    pip install \"deepspeed>=0.14.2\"\n\n### Inference examples\n\nDue to the model size we recommend using a single 8xH100 instance from your\nfavorite cloud provider such as: AWS p5.48xlarge, Azure ND96isr_H100_v5, etc.\n\nIn this example we are using FP8 quantization provided by DeepSpeed in the\nbackend, we can also use FP6 quantization by specifying q_bits=6 in the\nArcticQuantizationConfig config. The \"150GiB\" setting for max_memory is\nrequired until we can get DeepSpeed's FP quantization supported natively as a\nHFQuantizer which we are actively working on.\n\n    \n    \n    import os # enable hf_transfer for faster ckpt download os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\" import torch from transformers import AutoModelForCausalLM, AutoTokenizer from transformers.models.arctic.configuration_arctic import ArcticQuantizationConfig tokenizer = AutoTokenizer.from_pretrained(\"Snowflake/snowflake-arctic-instruct\") quant_config = ArcticQuantizationConfig(q_bits=8) model = AutoModelForCausalLM.from_pretrained( \"Snowflake/snowflake-arctic-instruct\", low_cpu_mem_usage=True, device_map=\"auto\", ds_quantization_config=quant_config, max_memory={i: \"150GiB\" for i in range(8)}, torch_dtype=torch.bfloat16) messages = [{\"role\": \"user\", \"content\": \"What is 1 + 1 \"}] input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\") outputs = model.generate(input_ids=input_ids, max_new_tokens=20) print(tokenizer.decode(outputs[0]))\n\nThe Arctic github page has additional code snippets and examples around\nrunning inference:\n\n  * Example with pure-HF: https://github.com/Snowflake-Labs/snowflake-arctic/blob/main/inference\n  * Tutorial using vLLM: https://github.com/Snowflake-Labs/snowflake-arctic/tree/main/inference/vllm\n\nDownloads last month\n\n    0\n\nSafetensors\n\nModel size\n\n482B params\n\nTensor type\n\nBF16\n\n\u00b7\n\n## Space using Snowflake/snowflake-arctic-instruct 1\n\n## Collection including Snowflake/snowflake-arctic-instruct\n\n#### Arctic\n\nCollection\n\nA collection of pre-trained dense-MoE Hybrid transformer models \u2022 2 items \u2022\nUpdated about 3 hours ago \u2022 7\n\n", "frontpage": false}
