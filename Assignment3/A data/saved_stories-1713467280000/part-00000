{"aid": "40075788", "title": "PEP 744 \u2013 JIT Compilation", "url": "https://peps.python.org/pep-0744/", "domain": "python.org", "votes": 2, "user": "PaulHoule", "posted_at": "2024-04-18 13:05:47", "comments": 0, "source_title": "PEP 744 \u2013 JIT Compilation | peps.python.org", "source_text": "PEP 744 \u2013 JIT Compilation | peps.python.org\n\n# Python Enhancement Proposals\n\n  * Python \u00bb\n  * PEP Index \u00bb\n  * PEP 744\n\n# PEP 744 \u2013 JIT Compilation\n\nAuthor:\n\n    Brandt Bucher <brandt at python.org>\nDiscussions-To:\n\n    Discourse thread\nStatus:\n\n    Draft\nType:\n\n    Informational\nCreated:\n\n    11-Apr-2024\nPython-Version:\n\n    3.13\nPost-History:\n\n    11-Apr-2024\n\n## Abstract\n\nEarlier this year, an experimental \u201cjust-in-time\u201d compiler was merged into\nCPython\u2019s main development branch. While recent CPython releases have included\nother substantial internal changes, this addition represents a particularly\nsignificant departure from the way CPython has traditionally executed Python\ncode. As such, it deserves wider discussion.\n\nThis PEP aims to summarize the design decisions behind this addition, the\ncurrent state of the implementation, and future plans for making the JIT a\npermanent, non-experimental part of CPython. It does not seek to provide a\ncomprehensive overview of how the JIT works, instead focusing on the\nparticular advantages and disadvantages of the chosen approach, as well as\nanswering many questions that have been asked about the JIT since its\nintroduction.\n\nReaders interested in learning more about the new JIT are encouraged to\nconsult the following resources:\n\n  * The presentation which first introduced the JIT at the 2023 CPython Core Developer Sprint. It includes relevant background, a light technical introduction to the \u201ccopy-and-patch\u201d technique used, and an open discussion of its design amongst the core developers present.\n  * The open access paper originally describing copy-and-patch.\n  * The blog post by the paper\u2019s author detailing the implementation of a copy-and-patch JIT compiler for Lua. While this is a great low-level explanation of the approach, note that it also incorporates other techniques and makes implementation decisions that are not particularly relevant to CPython\u2019s JIT.\n  * The implementation itself.\n\n## Motivation\n\nUntil this point, CPython has always executed Python code by compiling it to\nbytecode, which is interpreted at runtime. This bytecode is a more-or-less\ndirect translation of the source code: it is untyped, and largely unoptimized.\n\nSince the Python 3.11 release, CPython has used a \u201cspecializing adaptive\ninterpreter\u201d (PEP 659), which rewrites these bytecode instructions in-place\nwith type-specialized versions as they run. This new interpreter delivers\nsignificant performance improvements, despite the fact that its optimization\npotential is limited by the boundaries of individual bytecode instructions. It\nalso collects a wealth of new profiling information: the types flowing though\na program, the memory layout of particular objects, and what paths through the\nprogram are being executed the most. In other words, what to optimize, and how\nto optimize it.\n\nSince the Python 3.12 release, CPython has generated this interpreter from a\nC-like domain-specific language (DSL). In addition to taming some of the\ncomplexity of the new adaptive interpreter, the DSL also allows CPython\u2019s\nmaintainers to avoid hand-writing tedious boilerplate code in many parts of\nthe interpreter, compiler, and standard library that must be kept in sync with\nthe instruction definitions. This ability to generate large amounts of runtime\ninfrastructure from a single source of truth is not only convenient for\nmaintenance; it also unlocks many possibilities for expanding CPython\u2019s\nexecution in new ways. For instance, it makes it feasible to automatically\ngenerate tables for translating a sequence of instructions into an equivalent\nsequence of smaller \u201cmicro-ops\u201d, generate an optimizer for sequences of these\nmicro-ops, and even generate an entire second interpreter for executing them.\n\nIn fact, since early in the Python 3.13 release cycle, all CPython builds have\nincluded this exact micro-op translation, optimization, and execution\nmachinery. However, it is disabled by default; the overhead of interpreting\neven optimized traces of micro-ops is just too large for most code. Heavier\noptimization probably won\u2019t improve the situation much either, since any\nefficiency gains made by new optimizations will likely be offset by the\ninterpretive overhead of even smaller, more complex micro-ops.\n\nThe most obvious strategy to overcome this new bottleneck is to statically\ncompile these optimized traces. This presents opportunities to avoid several\nsources of indirection and overhead introduced by interpretation. In\nparticular, it allows the removal of dispatch overhead between micro-ops (by\nreplacing a generic interpreter with a straight-line sequence of hot code),\ninstruction decoding overhead for individual micro-ops (by \u201cburning\u201d the\nvalues or addresses of arguments, constants, and cached values directly into\nmachine instructions), and memory traffic (by moving data off of heap-\nallocated Python frames and into physical hardware registers).\n\nSince much of this data varies even between identical runs of a program and\nthe existing optimization pipeline makes heavy use of runtime profiling\ninformation, it doesn\u2019t make much sense to compile these traces ahead of time.\nAs has been demonstrated for many other dynamic languages (and even Python\nitself), the most promising approach is to compile the optimized micro-ops\n\u201cjust in time\u201d for execution.\n\n## Rationale\n\nDespite their reputation, JIT compilers are not magic \u201cgo faster\u201d machines.\nDeveloping and maintaining any sort of optimizing compiler for even a single\nplatform, let alone all of CPython\u2019s most popular supported platforms, is an\nincredibly complicated, expensive task. Using an existing compiler framework\nlike LLVM can make this task simpler, but only at the cost of introducing\nheavy runtime dependencies and significantly higher JIT compilation overhead.\n\nIt\u2019s clear that successfully compiling Python code at runtime requires not\nonly high-quality Python-specific optimizations for the code being run, but\nalso quick generation of efficient machine code for the optimized program. The\nPython core development team has the necessary skills and experience for the\nformer (a middle-end tightly coupled to the interpreter), and copy-and-patch\ncompilation provides an attractive solution for the latter.\n\nIn a nutshell, copy-and-patch allows a high-quality template JIT compiler to\nbe generated from the same DSL used to generate the rest of the interpreter.\nFor a widely-used, volunteer-driven project like CPython, this benefit cannot\nbe overstated: CPython\u2019s maintainers, by merely editing the bytecode\ndefinitions, will also get the JIT backend updated \u201cfor free\u201d, for all JIT-\nsupported platforms, at once. This is equally true whether instructions are\nbeing added, modified, or removed.\n\nLike the rest of the interpreter, the JIT compiler is generated at build time,\nand has no runtime dependencies. It supports a wide range of platforms (see\nthe Support section below), and has comparatively low maintenance burden. In\nall, the current implementation is made up of about 900 lines of build-time\nPython code and 500 lines of runtime C code.\n\n## Specification\n\nThe JIT will become non-experimental once all of the following conditions are\nmet:\n\n  1. It provides a meaningful performance improvement for at least one popular platform (realistically, on the order of 5%).\n  2. It can be built, distributed, and deployed with minimal disruption.\n  3. The Steering Council, upon request, has determined that it would provide more value to the community if enabled than if disabled (considering tradeoffs such as maintenance burden, memory usage, or the feasibility of alternate designs).\n\nThese criteria should be considered a starting point, and may be expanded over\ntime. For example, discussion of this PEP may reveal that additional\nrequirements (such as multiple committed maintainers, a security audit,\ndocumentation in the devguide, support for out-of-process debugging, or a\nruntime option to disable the JIT) should be added to this list.\n\nUntil the JIT is non-experimental, it should not be used in production, and\nmay be broken or removed at any time without warning.\n\nOnce the JIT is no longer experimental, it should be treated in much the same\nway as other build options such as --enable-optimizations or --with-lto. It\nmay be a recommended (or even default) option for some platforms, and release\nmanagers may choose to enable it in official releases.\n\n### Support\n\nThe JIT has been developed for all of PEP 11\u2019s current tier one platforms,\nmost of its tier two platforms, and one of its tier three platforms.\nSpecifically, CPython\u2019s main branch has CI building and testing the JIT for\nboth release and debug builds on:\n\n  * aarch64-apple-darwin/clang\n  * aarch64-pc-windows/msvc [1]\n  * aarch64-unknown-linux-gnu/clang [2]\n  * aarch64-unknown-linux-gnu/gcc [2]\n  * i686-pc-windows-msvc/msvc\n  * x86_64-apple-darwin/clang\n  * x86_64-pc-windows-msvc/msvc\n  * x86_64-unknown-linux-gnu/clang\n  * x86_64-unknown-linux-gnu/gcc\n\nIt\u2019s worth noting that some platforms, even future tier one platforms, may\nnever gain JIT support. This can be for a variety of reasons, including\ninsufficient LLVM support (powerpc64le-unknown-linux-gnu/gcc), inherent\nlimitations of the platform (wasm32-unknown-wasi/clang), or lack of developer\ninterest (x86_64-unknown-freebsd/clang).\n\nOnce JIT support for a platform is added (meaning, the JIT builds successfully\nwithout displaying warnings to the user), it should be treated in much the\nsame way as PEP 11 prescribes: it should have reliable CI/buildbots, and JIT\nfailures on tier one and tier two platforms should block releases. Though it\u2019s\nnot necessary to update PEP 11 to specify JIT support, it may be helpful to do\nso anyway. Otherwise, a list of supported platforms should be maintained in\nthe JIT\u2019s README.\n\nSince it should always be possible to build CPython without the JIT, removing\nJIT support for a platform should not be considered a backwards-incompatible\nchange. However, if it is reasonable to do so, the normal deprecation process\nshould be followed as outlined in PEP 387.\n\nThe JIT\u2019s build-time dependencies may be changed between releases, within\nreason.\n\n## Backwards Compatibility\n\nDue to the fact that the current interpreter and the JIT backend are both\ngenerated from the same specification, the behavior of Python code should be\ncompletely unchanged. In practice, observable differences that have been found\nand fixed during testing have tended to be bugs in the existing micro-op\ntranslation and optimization stages, rather than bugs in the copy-and-patch\nstep.\n\n### Debugging\n\nTools that profile and debug Python code will continue to work fine. This\nincludes in-process tools that use Python-provided functionality (like\nsys.monitoring, sys.settrace, or sys.setprofile), as well as out-of-process\ntools that walk Python frames from the interpreter state.\n\nHowever, it appears that profilers and debuggers for C code are currently\nunable to trace back through JIT frames. Working with leaf frames is possible\n(this is how the JIT itself is debugged), though it is of limited utility due\nto the absence of proper debugging information for JIT frames.\n\nSince the code templates emitted by the JIT are compiled by Clang, it may be\npossible to allow JIT frames to be traced through by simply modifying the\ncompiler flags to use frame pointers more carefully. It may also be possible\nto harvest and emit the debugging information produced by Clang. Neither of\nthese ideas have been explored very deeply.\n\nWhile this is an issue that should be fixed, fixing it is not a particularly\nhigh priority at this time. This is probably a problem best explored by\nsomebody with more domain expertise in collaboration with those maintaining\nthe JIT, who have little experience with the inner workings of these tools.\n\n## Security Implications\n\nThis JIT, like any JIT, produces large amounts of executable data at runtime.\nThis introduces a potential new attack surface to CPython, since a malicious\nactor capable of influencing the contents of this data is therefore capable of\nexecuting arbitrary code. This is a well-known vulnerability of JIT compilers.\n\nIn order to mitigate this risk, the JIT has been written with best practices\nin mind. In particular, the data in question is not exposed by the JIT\ncompiler to other parts of the program while it remains writable, and at no\npoint is the data both writable and executable.\n\nThe nature of template-based JITs also seriously limits the kinds of code that\ncan be generated, further reducing the likelihood of a successful exploit. As\nan additional precaution, the templates themselves are stored in static, read-\nonly memory.\n\nHowever, it would be naive to assume that no possible vulnerabilities exist in\nthe JIT, especially at this early stage. The author is not a security expert,\nbut is available to join or work closely with the Python Security Response\nTeam to triage and fix security issues as they arise.\n\n### Apple Silicon\n\nThough difficult to test without actually signing and packaging a macOS\nrelease, it appears that macOS releases should enable the JIT Entitlement for\nthe Hardened Runtime.\n\nThis shouldn\u2019t make installing Python any harder, but may add additional steps\nfor release managers to perform.\n\n## How to Teach This\n\nChoose the sections that best describe you:\n\n  * If you are a Python programmer or end user...\n\n    * ...nothing changes for you. Nobody should be distributing JIT-enabled CPython interpreters to you while it is still an experimental feature. Once it is non-experimental, you will probably notice slightly better performance and slightly higher memory usage. You shouldn\u2019t be able to observe any other changes.\n  * If you maintain third-party packages...\n\n    * ...nothing changes for you. There are no API or ABI changes, and the JIT is not exposed to third-party code. You shouldn\u2019t need to change your CI matrix, and you shouldn\u2019t be able to observe differences in the way your packages work when the JIT is enabled.\n  * If you profile or debug Python code...\n\n    * ...nothing changes for you. All Python profiling and tracing functionality remains.\n  * If you profile or debug C code...\n\n    * ...currently, the ability to trace through JIT frames is limited. This may cause issues if you need to observe the entire C call stack, rather than just \u201cleaf\u201d frames. See the Debugging section above for more information.\n  * If you compile your own Python interpreter....\n\n    * ...if you don\u2019t wish to build the JIT, you can simply ignore it. Otherwise, you will need to install a compatible version of LLVM, and pass the appropriate flag to the build scripts. Your build may take up to a minute longer. Note that the JIT should not be distributed to end users or used in production while it is still in the experimental phase.\n  * If you\u2019re a maintainer of CPython (or a fork of CPython)...\n\n    * ...and you change the bytecode definitions or the main interpreter loop...\n\n      * ...in general, the JIT shouldn\u2019t be much of an inconvenience to you (depending on what you\u2019re trying to do). The micro-op interpreter isn\u2019t going anywhere, and still offers a debugging experience similer to what the main bytecode interpreter provides today. There is moderate likelihood that larger changes to the interpreter (such as adding new local variables, changing error handling and deoptimization logic, or changing the micro-op format) will require changes to the C template used to generate the JIT, which is meant to mimic the main interpreter loop. You may also occasionally just get unlucky and break JIT code generation, which will require you to either modify the Python build scripts yourself, or solicit the help of somebody more familiar with them (see below).\n    * ...and you work on the JIT itself...\n\n      * ...you hopefully already have a decent idea of what you\u2019re getting yourself into. You will be regularly modifying the Python build scripts, the C template used to generate the JIT, and the C code that actually makes up the runtime portion of the JIT. You will also be dealing with all sorts of crashes, stepping over machine code in a debugger, staring at COFF/ELF/Mach-O dumps, developing on a wide range of platforms, and generally being the point of contact for the people changing the bytecode when CI starts failing on their PRs (see above). Ideally, you\u2019re at least familiar with assembly, have taken a couple of courses with \u201ccompilers\u201d in their name, and have read a blog post or two about linkers.\n    * ...and you maintain other parts of CPython...\n\n      * ...nothing changes for you. You shouldn\u2019t need to develop locally with JIT builds. If you choose to do so (for example, to help reproduce and triage JIT issues), your builds may take up to a minute longer each time the relevant files are modified.\n\n## Reference Implementation\n\nKey parts of the implementation include:\n\n  * Tools/jit/README.md: Instructions for how to build the JIT.\n  * Python/jit.c: The entire runtime portion of the JIT compiler.\n  * jit_stencils.h: An example of the JIT\u2019s generated templates.\n  * Tools/jit/template.c: The code which is compiled to produce the JIT\u2019s templates.\n  * Tools/jit/_targets.py: The code to compile and parse the templates at build time.\n\n## Rejected Ideas\n\n### Maintain it outside of CPython\n\nWhile it is probably possible to maintain the JIT outside of CPython, its\nimplementation is tied tightly enough to the rest of the interpreter that\nkeeping it up-to-date would probably be more difficult than actually\ndeveloping the JIT itself. Additionally, contributors working on the existing\nmicro-op definitions and optimizations would need to modify and build two\nseparate projects to measure the effects of their changes under the JIT\n(whereas today, infrastructure exists to do this automatically for any\nproposed change).\n\nReleases of the separate \u201cJIT\u201d project would probably also need to correspond\nto specific CPython pre-releases and patch releases, depending on exactly what\nchanges are present. Individual CPython commits between releases likely\nwouldn\u2019t have corresponding JIT releases at all, further complicating\ndebugging efforts (such as bisection to find breaking changes upstream).\n\nSince the JIT is already quite stable, and the ultimate goal is for it to be a\nnon-experimental part of CPython, keeping it in main seems to be the best path\nforward. With that said, the relevant code is organized in such a way that the\nJIT can be easily \u201cdeleted\u201d if it does not end up meeting its goals.\n\n### Turn it on by default\n\nOn the other hand, some have suggested that the JIT should be enabled by\ndefault in its current form.\n\nAgain, it is important to remember that a JIT is not a magic \u201cgo faster\u201d\nmachine; currently, the JIT is about as fast as the existing specializing\ninterpreter. This may sound underwhelming, but it is actually a fairly\nsignificant achievement, and it\u2019s the main reason why this approach was\nconsidered viable enough to be merged into main for further development.\n\nWhile the JIT provides significant gains over the existing micro-op\ninterpreter, it isn\u2019t yet a clear win when always enabled (especially\nconsidering its increased memory consumption and additional build-time\ndependencies). That\u2019s the purpose of this PEP: to clarify expectations about\nthe objective criteria that should be met in order to \u201cflip the switch\u201d.\n\nAt least for now, having this in main, but off by default, seems to be a good\ncompromise between always turning it on and not having it available at all.\n\n### Support multiple compiler toolchains\n\nClang is specifically needed because it\u2019s the only C compiler with support for\nguaranteed tail calls (musttail), which are required by CPython\u2019s\ncontinuation-passing-style approach to JIT compilation. Without it, the tail-\nrecursive calls between templates could result in unbounded C stack growth\n(and eventual overflow).\n\nSince LLVM also includes other functionalities required by the JIT build\nprocess (namely, utilities for object file parsing and disassembly), and\nadditional toolchains introduce additional testing and maintenance burden,\nit\u2019s convenient to only support one major version of one toolchain at this\ntime.\n\n### Compile the base interpreter\u2019s bytecode\n\nMost of the prior art for copy-and-patch uses it as a fast baseline JIT,\nwhereas CPython\u2019s JIT is using the technique to compile optimized micro-op\ntraces.\n\nIn practice, the new JIT currently sits somewhere between the \u201cbaseline\u201d and\n\u201coptimizing\u201d compiler tiers of other dynamic language runtimes. This is\nbecause CPython uses its specializing adaptive interpreter to collect runtime\nprofiling information, which is used to detect and optimize \u201chot\u201d paths\nthrough the code. This step is carried out using self-modifying code, a\ntechnique which is much more difficult to implement with a JIT compiler.\n\nWhile it\u2019s possible to compile normal bytecode using copy-and-patch (in fact,\nearly prototypes predated the micro-op interpreter and did exactly this), it\njust doesn\u2019t seem to provide enough optimization potential as the more\ngranular micro-op format.\n\n### Add GPU support\n\nThe JIT is currently CPU-only. It does not, for example, offload NumPy array\ncomputations to CUDA GPUs, as JITs like Numba do.\n\nThere is already a rich ecosystem of tools for accelerating these sorts of\nspecialized tasks, and CPython\u2019s JIT is not intended to replace them. Instead,\nit is meant to improve the performance of general-purpose Python code, which\nis less likely to benefit from deeper GPU integration.\n\n## Open Issues\n\n### Speed\n\nCurrently, the JIT is about as fast as the existing specializing interpreter\non most platforms. Improving this is obviously a top priority at this point,\nsince providing a significant performance gain is the entire motivation for\nhaving a JIT at all. A number of proposed improvements are already underway,\nand this ongoing work is being tracked in GH-115802.\n\n### Memory\n\nBecause it allocates additional memory for executable machine code, the JIT\ndoes use more memory than the existing interpreter at runtime. According to\nthe official benchmarks, the JIT currently uses about 10-20% more memory than\nthe base interpreter. The upper end of this range is due to aarch64-apple-\ndarwin, which has larger page sizes (and thus, a larger minimum allocation\ngranularity).\n\nHowever, these numbers should be taken with a grain of salt, as the benchmarks\nthemselves don\u2019t actually have a very high baseline of memory usage. Since\nthey have a higher ratio of code to data, the JIT\u2019s memory overhead is more\npronounced than it would be in a typical workload where memory pressure is\nmore likely to be a real concern.\n\nNot much effort has been put into optimizing the JIT\u2019s memory usage yet, so\nthese numbers likely represent a maximum that will be reduced over time.\nImproving this is a medium priority, and is being tracked in GH-116017.\n\nEarlier versions of the JIT had a more complicated memory allocation scheme\nwhich imposed a number of fragile limitations on the size and layout of the\nemitted code, and significantly bloated the memory footprint of Python\nexecutable. These issues are no longer present in the current design.\n\n### Dependencies\n\nBuilding the JIT adds between 3 and 60 seconds to the build process, depending\non platform. It is only rebuilt whenever the generated files become out-of-\ndate, so only those who are actively developing the main interpreter loop will\nbe rebuilding it with any frequency.\n\nUnlike many other generated files in CPython, the JIT\u2019s generated files are\nnot tracked by Git. This is because they contain compiled binary code\ntemplates specific to not only the host platform, but also the current build\nconfiguration for that platform. As such, hosting them would require a\nsignificant engineering effort in order to build and host dozens of large\nbinary files for each commit that changes the generated code. While perhaps\nfeasible, this is not a priority, since installing the required tools is not\nprohibitively difficult for most people building CPython, and the build step\nis not particularly time-consuming.\n\nSince some still remain interested in this possibility, discussion is being\ntracked in GH-115869.\n\n## Footnotes\n\n[1]\n\n    Due to lack of available hardware, the JIT is built, but not tested, for this platform.\n[2] (1, 2)\n\n    Due to lack of available hardware, the JIT is built using cross-compilation and tested using hardware emulation for this platform. Some tests are skipped because emulation causes them to fail. However, the JIT has been successfully built and tested for this platform on non-emulated hardware.\n\n## Copyright\n\nThis document is placed in the public domain or under the CC0-1.0-Universal\nlicense, whichever is more permissive.\n\nSource: https://github.com/python/peps/blob/main/peps/pep-0744.rst\n\nLast modified: 2024-04-12 01:08:14 GMT\n\n", "frontpage": false}
