{"aid": "40046438", "title": "npm i ollama", "url": "https://github.com/ollama/ollama-js", "domain": "github.com/ollama", "votes": 2, "user": "bschmidt1", "posted_at": "2024-04-15 22:29:33", "comments": 0, "source_title": "GitHub - ollama/ollama-js: Ollama JavaScript library", "source_text": "GitHub - ollama/ollama-js: Ollama JavaScript library\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nollama / ollama-js Public\n\n  * Notifications\n  * Fork 64\n  * Star 986\n\nOllama JavaScript library\n\nollama.ai\n\n### License\n\nMIT license\n\n986 stars 64 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# ollama/ollama-js\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n7 Branches\n\n15 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nda-zfix: errors in browser.ts (#58)dfc7988 \u00b7\n\n## History\n\n122 Commits  \n  \n### .github/workflows\n\n|\n\n### .github/workflows\n\n| fix version setting in ci  \n  \n### examples\n\n|\n\n### examples\n\n| formatting  \n  \n### src\n\n|\n\n### src\n\n| fix: add model details field and interface (#57)  \n  \n### test\n\n|\n\n### test\n\n| match host formatting  \n  \n### .eslintignore\n\n|\n\n### .eslintignore\n\n| Add basic project files.  \n  \n### .eslintrc.cjs\n\n|\n\n### .eslintrc.cjs\n\n| formatting  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| update gitignore  \n  \n### .npmignore\n\n|\n\n### .npmignore\n\n| Add basic project files.  \n  \n### .prettierrc.json\n\n|\n\n### .prettierrc.json\n\n| add prettier config  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| Change license to MIT.  \n  \n### README.md\n\n|\n\n### README.md\n\n| Update README.md  \n  \n### jest.config.cjs\n\n|\n\n### jest.config.cjs\n\n| add prettier config  \n  \n### package-lock.json\n\n|\n\n### package-lock.json\n\n| build: add cjs compatibility (#61)  \n  \n### package.json\n\n|\n\n### package.json\n\n| build: add cjs compatibility (#61)  \n  \n### tsconfig.json\n\n|\n\n### tsconfig.json\n\n| fix: errors in browser.ts (#58)  \n  \n## Repository files navigation\n\n# Ollama JavaScript Library\n\nThe Ollama JavaScript library provides the easiest way to integrate your\nJavaScript project with Ollama.\n\n## Getting Started\n\n    \n    \n    npm i ollama\n\n## Usage\n\n    \n    \n    import ollama from 'ollama' const response = await ollama.chat({ model: 'llama2', messages: [{ role: 'user', content: 'Why is the sky blue?' }], }) console.log(response.message.content)\n\n### Browser Usage\n\nTo use the library without node, import the browser module.\n\n    \n    \n    import ollama from 'ollama/browser'\n\n## Streaming responses\n\nResponse streaming can be enabled by setting stream: true, modifying function\ncalls to return an AsyncGenerator where each part is an object in the stream.\n\n    \n    \n    import ollama from 'ollama' const message = { role: 'user', content: 'Why is the sky blue?' } const response = await ollama.chat({ model: 'llama2', messages: [message], stream: true }) for await (const part of response) { process.stdout.write(part.message.content) }\n\n## Create\n\n    \n    \n    import ollama from 'ollama' const modelfile = ` FROM llama2 SYSTEM \"You are mario from super mario bros.\" ` await ollama.create({ model: 'example', modelfile: modelfile })\n\n## API\n\nThe Ollama JavaScript library's API is designed around the Ollama REST API\n\n### chat\n\n    \n    \n    ollama.chat(request)\n\n  * request <Object>: The request object containing chat parameters.\n\n    * model <string> The name of the model to use for the chat.\n    * messages <Message[]>: Array of message objects representing the chat history.\n\n      * role <string>: The role of the message sender ('user', 'system', or 'assistant').\n      * content <string>: The content of the message.\n      * images <Uint8Array[] | string[]>: (Optional) Images to be included in the message, either as Uint8Array or base64 encoded strings.\n    * format <string>: (Optional) Set the expected format of the response (json).\n    * stream <boolean>: (Optional) When true an AsyncGenerator is returned.\n    * keep_alive <string | number>: (Optional) How long to keep the model loaded.\n    * options <Options>: (Optional) Options to configure the runtime.\n  * Returns: <ChatResponse>\n\n### generate\n\n    \n    \n    ollama.generate(request)\n\n  * request <Object>: The request object containing generate parameters.\n\n    * model <string> The name of the model to use for the chat.\n    * prompt <string>: The prompt to send to the model.\n    * system <string>: (Optional) Override the model system prompt.\n    * template <string>: (Optional) Override the model template.\n    * raw <boolean>: (Optional) Bypass the prompt template and pass the prompt directly to the model.\n    * images <Uint8Array[] | string[]>: (Optional) Images to be included, either as Uint8Array or base64 encoded strings.\n    * format <string>: (Optional) Set the expected format of the response (json).\n    * stream <boolean>: (Optional) When true an AsyncGenerator is returned.\n    * keep_alive <string | number>: (Optional) How long to keep the model loaded.\n    * options <Options>: (Optional) Options to configure the runtime.\n  * Returns: <GenerateResponse>\n\n### pull\n\n    \n    \n    ollama.pull(request)\n\n  * request <Object>: The request object containing pull parameters.\n\n    * model <string> The name of the model to pull.\n    * insecure <boolean>: (Optional) Pull from servers whose identity cannot be verified.\n    * stream <boolean>: (Optional) When true an AsyncGenerator is returned.\n  * Returns: <ProgressResponse>\n\n### push\n\n    \n    \n    ollama.push(request)\n\n  * request <Object>: The request object containing push parameters.\n\n    * model <string> The name of the model to push.\n    * insecure <boolean>: (Optional) Push to servers whose identity cannot be verified.\n    * stream <boolean>: (Optional) When true an AsyncGenerator is returned.\n  * Returns: <ProgressResponse>\n\n### create\n\n    \n    \n    ollama.create(request)\n\n  * request <Object>: The request object containing create parameters.\n\n    * model <string> The name of the model to create.\n    * path <string>: (Optional) The path to the Modelfile of the model to create.\n    * modelfile <string>: (Optional) The content of the Modelfile to create.\n    * stream <boolean>: (Optional) When true an AsyncGenerator is returned.\n  * Returns: <ProgressResponse>\n\n### delete\n\n    \n    \n    ollama.delete(request)\n\n  * request <Object>: The request object containing delete parameters.\n\n    * model <string> The name of the model to delete.\n  * Returns: <StatusResponse>\n\n### copy\n\n    \n    \n    ollama.copy(request)\n\n  * request <Object>: The request object containing copy parameters.\n\n    * source <string> The name of the model to copy from.\n    * destination <string> The name of the model to copy to.\n  * Returns: <StatusResponse>\n\n### list\n\n    \n    \n    ollama.list()\n\n  * Returns: <ListResponse>\n\n### show\n\n    \n    \n    ollama.show(request)\n\n  * request <Object>: The request object containing show parameters.\n\n    * model <string> The name of the model to show.\n    * system <string>: (Optional) Override the model system prompt returned.\n    * template <string>: (Optional) Override the model template returned.\n    * options <Options>: (Optional) Options to configure the runtime.\n  * Returns: <ShowResponse>\n\n### embeddings\n\n    \n    \n    ollama.embeddings(request)\n\n  * request <Object>: The request object containing embedding parameters.\n\n    * model <string> The name of the model used to generate the embeddings.\n    * prompt <string>: The prompt used to generate the embedding.\n    * keep_alive <string | number>: (Optional) How long to keep the model loaded.\n    * options <Options>: (Optional) Options to configure the runtime.\n  * Returns: <EmbeddingsResponse>\n\n## Custom client\n\nA custom client can be created with the following fields:\n\n  * host <string>: (Optional) The Ollama host address. Default: \"http://127.0.0.1:11434\".\n  * fetch <Object>: (Optional) The fetch library used to make requests to the Ollama host.\n\n    \n    \n    import { Ollama } from 'ollama' const ollama = new Ollama({ host: 'http://localhost:11434' }) const response = await ollama.chat({ model: 'llama2', messages: [{ role: 'user', content: 'Why is the sky blue?' }], })\n\n## Building\n\nTo build the project files run:\n\n    \n    \n    npm run build\n\n## About\n\nOllama JavaScript library\n\nollama.ai\n\n### Topics\n\njavascript js ollama\n\n### Resources\n\nReadme\n\n### License\n\nMIT license\n\nActivity\n\nCustom properties\n\n### Stars\n\n986 stars\n\n### Watchers\n\n15 watching\n\n### Forks\n\n64 forks\n\nReport repository\n\n## Releases 10\n\nv0.5.0 Latest\n\nMar 14, 2024\n\n\\+ 9 releases\n\n## Packages 0\n\nNo packages published\n\n## Used by 434\n\n\\+ 426\n\n## Contributors 11\n\n## Languages\n\n  * TypeScript 94.9%\n  * JavaScript 5.1%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
