{"aid": "40159208", "title": "Overcoming Hallucinations with the Trustworthy Language Model", "url": "https://cleanlab.ai/blog/trustworthy-language-model/", "domain": "cleanlab.ai", "votes": 1, "user": "anishathalye", "posted_at": "2024-04-25 16:03:21", "comments": 0, "source_title": "Overcoming Hallucinations with the Trustworthy Language Model", "source_text": "Overcoming Hallucinations with the Trustworthy Language Model\n\nNew Research\n\nNew Feature\n\nGenerative AI\n\n# Overcoming Hallucinations with the Trustworthy Language Model\n\n04/25/2024\n\n  * Anish Athalye\n  * Jonas Mueller\n  * Curtis Northcutt\n  * Hui Wen Goh\n  * Ulyana Tkachenko\n\nWe\u2019re excited to launch Cleanlab\u2019s Trustworthy Language Model (TLM), which\novercomes the biggest barrier to enterprise adoption of LLMs: hallucinations\nand reliability. By adding a trust score to every LLM response, TLM lets you\ncontain and manage bogus LLM outputs, enabling you to deploy generative AI for\nnew use cases previously unsuitable for LLMs. Through rigorous benchmarking,\nwe\u2019ve shown that TLM both produces more accurate outputs than existing LLMs\nand has better-calibrated trustworthiness scores (enabling greater cost/time\nsavings) than other common approaches to taming LLM uncertainty.\n\nCreate an account to get free access to the TLM API, or experiment with TLM in\nthe playground.\n\n# LLMs\u2019 biggest challenge: hallucinations\n\nA recent Gartner poll shows that while 55% of organizations are experimenting\nwith generative AI, only 10% have put generative AI into production. One of\nthe biggest barriers to productionizing LLMs is dealing with their tendency to\nproduce bogus outputs known as hallucinations, which precludes their use in\napplications where correct outputs are necessary (i.e., most applications)!\n\nDespite this, some organizations have deployed these unreliable LLMs,\nsometimes with catastrophic results. Air Canada\u2019s chatbot hallucinated refund\npolicies, eventually resulting in the airline being held responsible for the\nmisinformation and being ordered by a tribunal to refund a customer; the\nchatbot has since been taken down. A federal judge fined a law firm after\ntheir lawyers used ChatGPT to draft a brief full of fabricated citations. New\nYork City\u2019s \u201cMyCity\u201d chatbot has been hallucinating wrong answers to business\nowners\u2019 questions about local laws.\n\n# Overcoming hallucinations with trustworthiness scores\n\nLLMs will always have some hallucinations, but by providing a trustworthiness\nscore with every output, Cleanlab TLM lets you identify when the LLM is\nhallucinating. TLM is optimized for minimizing false negatives \u2014 when the LLM\nhallucinates, we want to make sure the trustworthiness score is low \u2014 to\nenable reliable deployment of LLM-based applications.\n\nThe TLM API can serve as:\n\n  * A drop-in replacement for your LLM. Much like existing LLM APIs, TLM provides a .prompt() method, and TLM will return a response along with a trustworthiness score, enabling new use cases.\n\n    * Even the responses themselves are more accurate than the baseline model, because TLM internally produces many responses and returns the one with the highest trustworthiness score.\n  * A layer of trust for your existing LLM outputs or human-generated data. TLM provides a .get_trustworthiness_score() method that can score any prompt/response pair.\n\nTLM works by augmenting existing LLMs with a layer of trust. The generally-\navailable version of TLM lets you choose between a number of popular base\nmodels, including GPT-3.5 and GPT-4, but TLM can augment any LLM with only\nblack-box access to the LLM API. For enterprise use cases, such as adding\ntrustworthiness to your custom fine-tuned LLM, contact us.\n\nBerkeley Research Group (BRG) has already seen significant cost savings from\nleveraging TLM. According to Steven Gawthorpe, PhD, Associate Director and\nSenior Data Scientist at BRG:\n\n> While there are always other tools out there, Cleanlab\u2019s TLM is the first\n> viable answer to LLM hallucinations that I\u2019ve seen. Several of our human-in-\n> the-loop LLM workflows can now be 80% automated with Cleanlab\u2019s\n> trustworthiness scores on every LLM output. Doing this manually for the\n> entire dataset is often impossible, but Cleanlab gives us the power of 1000s\n> of data scientists to enrich data and strengthen LLM outputs. The downstream\n> cost savings of using TLM for accurate data are substantial, providing\n> significant financial benefits with 10x to 100x ROI for many of our clients.\n> Other tools on the market aren\u2019t even on the same playing field compared to\n> what Cleanlab is doing.\n\n# Use cases enabled by TLM\n\nTrustworthiness scores unlock new production use cases of LLMs, and any\nexisting application of LLMs can also benefit by taking into account these\nscores.\n\n## Customer service chatbot\n\nTLM can power trustworthy chatbots that answer the 80% of questions where they\nare confident, but where they escalate to a human if they\u2019re unsure about a\nresponse rather than hallucinating one (like in the Air Canada case). This can\nbe done simply by routing the question to a human when the trustworthiness\nscore falls below a chosen threshold.\n\n## Auto-labeling\n\nLLMs are commonly used for auto-labeling data. With TLM, you can confidently\nauto-label a large fraction of your data and only have humans review a portion\nof the data where the LLM does not return trustworthy results.\n\n    \n    \n    template = ''' What type of compliance issue is most likely present in the following document? Please restrict your answer to a one word answer and nothing else. Your answer should be selected from the following options: HIPAA, FERPA, GDPR, none. Document below here: {document} ''' def classify(document) -> Tuple[str, float]: answer = tlm.prompt(template.format(document=document)) return answer['response'], answer['trustworthiness_score']\n\nIf you were to use this prompt to classify a large number of legal documents,\nyou\u2019d find that the documents with high trustworthiness scores were labeled\ncorrectly, while the documents with low scores had labels that needed to be\ndouble-checked:\n\ndocument| response| trustworthiness  \n---|---|---  \nAll medical health records will be accessed one way only. The patient\u2019s\nmedical data will be stored on unencrypted public servers at the discretion of\nthe enterprise customer.| HIPAA| 0.984  \n\u22ee| \u22ee| \u22ee  \nTechTarget\u2019s Cookies Policy includes the following terminology: \u201cBy continuing\nto use the site, you agree to the use of cookies.\u201d| FERPA| 0.426  \n  \nFor more on this use case, see the TLM auto-labeling tutorial.\n\n## Data extraction\n\nTLM can also be used for open-domain data extraction. Our TLM information\nextraction tutorial walks through an example use case of extracting key\ninformation from electronics parts datasheet PDFs like the following:\n\nIf you were populating a parts catalog, you might be interested in extracting\ninformation like operating voltage from such documents, where TLM\u2019s\ntrustworthiness scores can separate good outputs from bad:\n\npart| operating voltage| trustworthiness  \n---|---|---  \nATtiny44A| 1.8 - 5.5V| 0.937  \n\u22ee| \u22ee| \u22ee  \nZRE200GE| 1V - 15V DC| 0.567  \n  \n## ... and more\n\nThe examples above just scratch the surface of reliable AI applications that\nbecome possible with TLM. We\u2019re continually adding hands on tutorials for new\napplications of TLM, such as:\n\n  * Trustworthy retrieval-augmented generation (RAG)\n  * Detecting bad instruction-tuning data\n  * Turning your own LLM into a TLM (Llama-3 example)\n\n# Evaluating TLM Performance\n\nWe evaluate TLM\u2019s ability to add trust to arbitrary LLMs by benchmarking TLM\nagainst OpenAI\u2019s state-of-the-art GPT-4 LLM. Our comprehensive benchmarks\ninvestigate two questions to evaluate the reliability of TLM\u2019s (1) responses,\nand (2) trustworthiness scores:\n\n  1. How accurate are TLM responses compared to the baseline LLM?\n  2. To meet a required error rate by flagging low-scoring LLM responses for human review, how much costs/time does a team save by scoring responses via TLM vs. existing confidence estimation approaches?\n\nThe second item can be rephrased as: How many wrong LLM responses can we catch\nunder a limited review budget by prioritizing via trustworthiness scores? When\ninvestigating this, we compare against two popular approaches to estimate the\nconfidence of the baseline LLM:\n\n  * Self-Eval: Asking the LLM to evaluate its own output and rate its confidence on a scale of 1-5. This is done in a subsequent request to the model (details in Appendix).\n  * Probability: Relying on the probability of the generated output given by the language model, as recommended by OpenAI. This is called the perplexity in AI research, and is the average log probability of tokens in the LLM response, obtained from the raw output of the underlying autoregressive neural network.\n\nBoth of these confidence measures merely quantify the aleatoric uncertainty\n(known unknowns) in model predictions. This is uncertainty the model is aware\nof due to a known challenging prompt (e.g., incomplete/vague request). TLM\u2019s\ntrustworthiness score additionally quantifies epistemic uncertainty (unknown\nunknowns), which arises when the model was not previously trained on data\nsimilar to a given request.\n\n## Benchmark datasets\n\nOur study focuses on Q&A settings. Unlike other LLM benchmarks, we never\nmeasure benchmark performance using LLM-based evaluations. All of our\nbenchmarks involve questions with a single correct answer, and benchmark\nperformance is based on whether or not the LLM response matches this known\nground-truth answer. We consider these popular Q&A datasets:\n\n  * TriviaQA: Open-domain trivia questions.\n  * ARC: Grade school multiple-choice questions (we consider the \u201cChallenge Test\u201d subset).\n  * SVAMP: Elementary-level math word problems.\n  * GSM8k: Grade school math problems.\n  * Diagnosis: Diagnosing medical conditions based on symptom descriptions from the patient.\n\nThe next sections show some benchmark examples and the corresponding TLM\noutputs.\n\n## Examples from benchmark where TLM responded correctly\n\nPrompt: If 6 potatoes makes 36 hash browns, how many hash browns can you make\nout of 96 potatoes?\n\nTLM Output: 576 Trustworthiness Score: 0.993\n\nPrompt: You are a doctor looking at a patient\u2019s symptoms. Classify the\nSymptoms into a single Diagnosis that best represents them. The list of\navailable Diagnosis is: [cervical spondylosis, impetigo, urinary tract\ninfection, arthritis, dengue, common cold, drug reaction, fungal infection,\nmalaria, allergy, bronchial asthma, varicose veins, migraine, hypertension,\ngastroesophageal reflux disease, pneumonia, psoriasis, diabetes, jaundice,\nchicken pox, typhoid, peptic ulcer disease]. Symptoms: I have a feeling of\nfood or acid backing up into my throat. I have chest pain which gets worse if\nI lie down. I get frequent heartburn or indigestion, after eating food and\nvomit it out.\n\nTLM Output: gastroesophageal reflux disease Trustworthiness Score: 0.994\n\n## Examples from benchmark where TLM responded incorrectly\n\nPrompt: Emil is 19 years old now. When he turns 24, he will be half the age of\nhis dad but twice as old as his brother. What is the sum of the ages of his\ndad and his brother now?\n\nTLM Output: 65 Trustworthiness Score: 0.123 (Ground-Truth Answer: 50)\n\nPrompt: On a standard dartboard, which number lies opposite number 4?\n\nTLM Output: 18 Trustworthiness Score: 0.379 (Ground-Truth Answer: 16)\n\nPrompt: You are a doctor looking at a patient\u2019s symptoms. Classify the\nSymptoms into a single Diagnosis that best represents them. The list of\navailable Diagnosis is: [cervical spondylosis, impetigo, urinary tract\ninfection, arthritis, dengue, common cold, drug reaction, fungal infection,\nmalaria, allergy, bronchial asthma, varicose veins, migraine, hypertension,\ngastroesophageal reflux disease, pneumonia, psoriasis, diabetes, jaundice,\nchicken pox, typhoid, peptic ulcer disease]. Symptoms: I have a severe\nheadache that feels like pressure in my head. I also have a mild fever and\nsmall red spots on my back.\n\nTLM Output: migraine Trustworthiness Score: 0.221 (Ground-Truth Answer:\ndengue)\n\n## Benchmark Results\n\nThe following table reports the accuracy of responses from TLM and GPT-4\nacross each benchmark dataset:\n\nDataset| OpenAI GPT-4 API| Cleanlab TLM API  \n---|---|---  \nTriviaQA| 84.7%| 84.8%  \nARC| 94.6%| 94.9%  \nSVAMP| 90.7%| 91.7%  \nGSM8k| 46.5%| 55.6%  \nDiagnosis| 67.4%| 68.0%  \n  \nTLM consistently improves the accuracy of the baseline GPT-4 LLM across all\ndatasets.\n\nNext, we evaluate the three aforementioned approaches to estimate\ntrustworthiness scores for each LLM response (again using GPT-4 as the\nbaseline LLM): TLM, Self-Eval, Probability. The following plot reports the\nerror rate of LLM responses amongst the top-K% of responses with the highest\ntrustworthiness scores in each dataset:\n\nAcross all datasets, TLM trustworthiness scores allow us to more reliably\ndetect bad LLM responses than the Self-Eval or Probability scores. If a team\nhas to ensure a max-acceptable error rate by manually reviewing the low-\nscoring LLM responses, enormous reviewing costs/time can be saved by adopting\nTLM scores. For instance, a team could achieve near-zero error rates for the\nSVAMP dataset by only inspecting ~20% of the LLM responses when relying on TLM\ntrustworthiness, but would have to inspect nearly 40% or 90% of the data when\nrelying on Probability or Self-Eval scores.\n\nWe additionally evaluate the utility of these trustworthiness scores via: the\nprobability that LLM response #1 receives a higher trustworthiness score than\nLLM response #2, where the former is randomly selected from the subset of\nmodel responses that were correct, and the latter from the subset of model\nresponses that were incorrect. Widely used to assess diagnostic scores, this\nevaluation metric is known as the Area under the Receiver Operating\nCharacteristic Curve (AUROC). The following table reports the AUROC achieved\nby each trustworthiness scoring method in each dataset:\n\nDataset| Probability| Self-Eval| TLM  \n---|---|---|---  \nTriviaQA| 0.704| 0.623| 0.812  \nARC| 0.755| 0.659| 0.861  \nSVAMP| 0.943| 0.793| 0.973  \nGSM8k| 0.883| 0.868| 0.994  \nDiagnosis| 0.614| 0.654| 0.711  \n  \nAdditional benchmarks are presented in the Appendix, in particular with\nanother version of TLM built around GPT-3.5 instead of GPT-4. Our studies\nreveal that TLM can reduce the error rate (incorrect answers) of GPT-4 by up\nto 10% and the error rate of GPT-3.5 by up to 22%. The trustworthiness\nestimates output by TLM are significantly more effective for catching bad\nanswers, across different evaluation metrics, datasets, and LLMs.\n\nYour subscription could not be saved. Please try again.\n\nYour subscription has been successful.\n\n# Conclusion\n\nThis article shows how the TLM technology can boost the reliability of any LLM\napplication via trustworthiness scores and more accurate responses. You can\nuse Cleanlab\u2019s TLM built on top of popular LLMs, or contact us to convert your\nown LLM into a TLM (requires no additional training of the LLM or access to\nits training data or model weights).\n\nOf course, there\u2019s no free lunch. TLM requires extra computation in order to\nprovide these benefits. It internally calls the underlying LLM multiple times\nto self-reflect on candidate responses and assess the consistency between\ncandidate responses. Learn more via the documentation. TLM is thus most useful\nfor higher-stakes AI applications that require reliability and no unchecked\nhallucinations.\n\nWe will soon launch a web interface to run TLM over big datasets, which\ninevitably contain edge-cases that cause bad LLM outputs. You\u2019ll be able to\ncatch and remediate these with the trustworthiness score, all with a few\nclicks.\n\n# Resources\n\n  * Play with Cleanlab\u2019s TLM in our interactive demo.\n  * Try the actual TLM API for free, and run various tutorial use-cases.\n  * Read about TLM in today\u2019s News.\n  * Join our Slack community to discuss reliable AI + follow us on Twitter & LinkedIn.\n\n# Appendix\n\nExpand each collapsible section below to learn more.\n\nBrowse all\n\nNext\n\nRelated Blogs\n\nHow to Generate Better Synthetic Image Datasets with Stable Diffusion\n\nSystematically evaluate synthetic datasets via quantitative scores. Use these\nscores to guide prompt engineering and other synthetic data generator\noptimizations.\n\nRead more\n\nBeware of Unreliable Data in Model Evaluation: A LLM Prompt Selection case\nstudy with Flan-T5\n\nYou may choose suboptimal prompts for your LLM (or make other suboptimal\nchoices via model evaluation) unless you clean your test data.\n\nRead more\n\nAutomatically Find and Fix Issues in Image/Document Tags and other Multi-Label\nDatasets\n\nIn this tutorial, learn how to use Cleanlab Studio to automatically correct\nmulti-label classification data for image and document tagging, content\ncuration, NLP, and more!\n\nRead more\n\nGet started today\n\nTry Cleanlab Studio for free and automatically improve your dataset \u2014 no code\nrequired.\n\nTry for freeContact sales\n\nMore resources\n\nExplore applications of Cleanlab Studio via blogs, tutorials, videos, and read\nthe research that powers this next-generation platform.\n\nJoin us on Slack\n\nJoin the Cleanlab Community to ask questions and see how scientists and\nengineers are practicing Data-Centric AI.\n\nTry for freeCase studiesJoin our community\n\nSOLUTIONS\n\nIndustries\n\nData and Tech ConsultingLawFinancial Services and InsuranceE-Commerce and\nRetailManufacturing and AgricultureHealthcare\n\nApplications\n\nData Entry, Management, and CurationFoundation and Large Language\nModelsBusiness Intelligence and AnalyticsData Annotation and\nCrowdsourcingCustomer ServiceContent Moderation\n\nLearn\n\nBlogExamplesTutorialsPlaygroundResearch\n\nCompany\n\nAbout UsCareersContactCulture\n\nOpen Source\n\nGithubDocumentationExamples\n\nTerms and ConditionsPrivacy Policy\u00a9 2024 Cleanlab Inc.\n\n", "frontpage": false}
