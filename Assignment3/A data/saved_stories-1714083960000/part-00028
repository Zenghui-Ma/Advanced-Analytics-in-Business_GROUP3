{"aid": "40159415", "title": "OpenELM: Efficient LLM Family with Open-Source Training and Inference Framework", "url": "https://machinelearning.apple.com/research/openelm", "domain": "machinelearning.apple.com", "votes": 1, "user": "akyuu", "posted_at": "2024-04-25 16:16:09", "comments": 0, "source_title": "OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework", "source_text": "OpenELM: An Efficient Language Model Family with Open-source Training and\nInference Framework - Apple Machine Learning Research\n\nresearch areaSpeech and Natural Language Processing\n\ncontent type paper | published April 2024\n\n# OpenELM: An Efficient Language Model Family with Open-source Training and\nInference Framework\n\nAuthorsSachin Mehta, Mohammad Sekhavat, Qingqing Cao, Max Horton, Yanzi Jin,\nFrank Sun, Iman Mirzadeh, Mahyar Najibikohnehshahri, Dmitry Belenko, Peter\nZatloukal, Mohammad Rastegari\n\nView publication\n\nView source code (GitHub)\n\nThe reproducibility and transparency of large language models are crucial for\nadvancing open research, ensuring the trustworthiness of results, and enabling\ninvestigations into data and model biases, as well as potential risks. To this\nend, we release OpenELM, a state-of-the-art open language model. OpenELM uses\na layer-wise scaling strategy to efficiently allocate parameters within each\nlayer of the transformer model, leading to enhanced accuracy. For example,\nwith a parameter budget of approximately one billion parameters, OpenELM\nexhibits a 2.36% improvement in accuracy compared to OLMo while requiring 2\ntimes fewer pre-training tokens.\n\nDiverging from prior practices that only provide model weights and inference\ncode, and pre-train on private datasets, our release includes the complete\nframework for training and evaluation of the language model on publicly\navailable datasets, including training logs, multiple checkpoints, and pre-\ntraining configurations. We also release code to convert models to MLX library\nfor inference and fine-tuning on Apple devices. This comprehensive release\naims to empower and strengthen the open research community, paving the way for\nfuture open research endeavors.\n\n## Related readings and updates.\n\n### Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image\nGeneration\n\nThis paper was accepted at the workshop I Can\u2019t Believe It\u2019s Not Better!\n(ICBINB) at NeurIPS 2023. Recent advances in image tokenizers, such as VQ-VAE,\nhave enabled text-to-image generation using auto-regressive methods, similar\nto language modeling. However, these methods have yet to leverage pre-trained\nlanguage models, despite their adaptability to various downstream tasks. In\nthis work, we explore this gap, and find that pre-trained language...\n\nSee paper details\n\n### Reverse Transfer Learning: Can Word Embeddings Trained for Different NLP\nTasks Improve Neural Language Models?\n\nNatural language processing (NLP) tasks tend to suffer from a paucity of\nsuitably annotated training data, hence the recent success of transfer\nlearning across a wide variety of them. The typical recipe involves: (i)\ntraining a deep, possibly bidirectional, neural network with an objective\nrelated to language modeling, for which training data is plentiful; and (ii)\nusing the trained network to derive contextual representations that are far\nricher...\n\nSee paper details\n\n## Discover opportunities in Machine Learning.\n\nOur research in machine learning breaks new ground every day.\n\nWork with us\n\nPrivacy Policy Terms of Use Legal\n\nCopyright \u00a9 2024 Apple Inc. All rights reserved.\n\n", "frontpage": false}
