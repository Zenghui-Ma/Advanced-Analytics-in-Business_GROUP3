{"aid": "40075813", "title": "The Illustrated Word2Vec", "url": "https://jalammar.github.io/illustrated-word2vec/", "domain": "jalammar.github.io", "votes": 2, "user": "wcedmisten", "posted_at": "2024-04-18 13:08:17", "comments": 0, "source_title": "The Illustrated Word2vec", "source_text": "The Illustrated Word2vec \u2013 Jay Alammar \u2013 Visualizing machine learning one\nconcept at a time.\n\n# Jay Alammar\n\nVisualizing machine learning one concept at a time. @JayAlammar on Twitter.\nYouTube Channel\n\n# The Illustrated Word2vec\n\nDiscussions: Hacker News (347 points, 37 comments), Reddit r/MachineLearning\n(151 points, 19 comments) Translations: Chinese (Simplified), French, Korean,\nPortuguese, Russian\n\n> \u201cThere is in all things a pattern that is part of our universe. It has\n> symmetry, elegance, and grace - those qualities you find always in that\n> which the true artist captures. You can find it in the turning of the\n> seasons, in the way sand trails along a ridge, in the branch clusters of the\n> creosote bush or the pattern of its leaves.\n>\n> We try to copy these patterns in our lives and our society, seeking the\n> rhythms, the dances, the forms that comfort. Yet, it is possible to see\n> peril in the finding of ultimate perfection. It is clear that the ultimate\n> pattern contains it own fixity. In such perfection, all things move toward\n> death.\u201d ~ Dune (1965)\n\nI find the concept of embeddings to be one of the most fascinating ideas in\nmachine learning. If you\u2019ve ever used Siri, Google Assistant, Alexa, Google\nTranslate, or even smartphone keyboard with next-word prediction, then chances\nare you\u2019ve benefitted from this idea that has become central to Natural\nLanguage Processing models. There has been quite a development over the last\ncouple of decades in using embeddings for neural models (Recent developments\ninclude contextualized word embeddings leading to cutting-edge models like\nBERT and GPT2).\n\nWord2vec is a method to efficiently create word embeddings and has been around\nsince 2013. But in addition to its utility as a word-embedding method, some of\nits concepts have been shown to be effective in creating recommendation\nengines and making sense of sequential data even in commercial, non-language\ntasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all\nbenefitted from carving out this brilliant piece of machinery from the world\nof NLP and using it in production to empower a new breed of recommendation\nengines.\n\nIn this post, we\u2019ll go over the concept of embedding, and the mechanics of\ngenerating embeddings with word2vec. But let\u2019s start with an example to get\nfamiliar with using vectors to represent things. Did you know that a list of\nfive numbers (a vector) can represent so much about your personality?\n\n# Personality Embeddings: What are you like?\n\n> \u201cI give you the desert chameleon, whose ability to blend itself into the\n> background tells you all you need to know about the roots of ecology and the\n> foundations of a personal identity\u201d ~Children of Dune\n\nOn a scale of 0 to 100, how introverted/extraverted are you (where 0 is the\nmost introverted, and 100 is the most extraverted)? Have you ever taken a\npersonality test like MBTI \u2013 or even better, the Big Five Personality Traits\ntest? If you haven\u2019t, these are tests that ask you a list of questions, then\nscore you on a number of axes, introversion/extraversion being one of them.\n\nExample of the result of a Big Five Personality Trait test. It can really tell\nyou a lot about yourself and is shown to have predictive ability in academic,\npersonal, and professional success. This is one place to find your results.\n\nImagine I\u2019ve scored 38/100 as my introversion/extraversion score. we can plot\nthat in this way:\n\nLet\u2019s switch the range to be from -1 to 1:\n\nHow well do you feel you know a person knowing only this one piece of\ninformation about them? Not much. People are complex. So let\u2019s add another\ndimension \u2013 the score of one other trait from the test.\n\nWe can represent the two dimensions as a point on the graph, or better yet, as\na vector from the origin to that point. We have incredible tools to deal with\nvectors that will come in handy very shortly.\n\nI\u2019ve hidden which traits we\u2019re plotting just so you get used to not knowing\nwhat each dimension represents \u2013 but still getting a lot of value from the\nvector representation of a person\u2019s personality.\n\nWe can now say that this vector partially represents my personality. The\nusefulness of such representation comes when you want to compare two other\npeople to me. Say I get hit by a bus and I need to be replaced by someone with\na similar personality. In the following figure, which of the two people is\nmore similar to me?\n\nWhen dealing with vectors, a common way to calculate a similarity score is\ncosine_similarity:\n\nPerson #1 is more similar to me in personality. Vectors pointing at the same\ndirection (length plays a role as well) have a higher cosine similarity score.\n\nYet again, two dimensions aren\u2019t enough to capture enough information about\nhow different people are. Decades of psychology research have led to five\nmajor traits (and plenty of sub-traits). So let\u2019s use all five dimensions in\nour comparison:\n\nThe problem with five dimensions is that we lose the ability to draw neat\nlittle arrows in two dimensions. This is a common challenge in machine\nlearning where we often have to think in higher-dimensional space. The good\nthing is, though, that cosine_similarity still works. It works with any number\nof dimensions:\n\ncosine_similarity works for any number of dimensions. These are much better\nscores because they're calculated based on a higher resolution representation\nof the things being compared.\n\nAt the end of this section, I want us to come out with two central ideas:\n\n  1. We can represent people (and things) as vectors of numbers (which is great for machines!).\n  2. We can easily calculate how similar vectors are to each other.\n\n# Word Embeddings\n\n> \u201cThe gift of words is the gift of deception and illusion\u201d ~Children of Dune\n\nWith this understanding, we can proceed to look at trained word-vector\nexamples (also called word embeddings) and start looking at some of their\ninteresting properties.\n\nThis is a word embedding for the word \u201cking\u201d (GloVe vector trained on\nWikipedia):\n\n[ 0.50451 , 0.68607 , -0.59517 , -0.022801, 0.60046 , -0.13498 , -0.08813 ,\n0.47377 , -0.61798 , -0.31012 , -0.076666, 1.493 , -0.034189, -0.98173 ,\n0.68229 , 0.81722 , -0.51874 , -0.31503 , -0.55809 , 0.66421 , 0.1961 ,\n-0.13495 , -0.11476 , -0.30344 , 0.41177 , -2.223 , -1.0756 , -1.0783 ,\n-0.34354 , 0.33505 , 1.9927 , -0.04234 , -0.64319 , 0.71125 , 0.49159 ,\n0.16754 , 0.34344 , -0.25663 , -0.8523 , 0.1661 , 0.40102 , 1.1685 , -1.0137 ,\n-0.21585 , -0.15155 , 0.78321 , -0.91241 , -1.6106 , -0.64426 , -0.51042 ]\n\nIt\u2019s a list of 50 numbers. We can\u2019t tell much by looking at the values. But\nlet\u2019s visualize it a bit so we can compare it other word vectors. Let\u2019s put\nall these numbers in one row:\n\nLet\u2019s color code the cells based on their values (red if they\u2019re close to 2,\nwhite if they\u2019re close to 0, blue if they\u2019re close to -2):\n\nWe\u2019ll proceed by ignoring the numbers and only looking at the colors to\nindicate the values of the cells. Let\u2019s now contrast \u201cKing\u201d against other\nwords:\n\nSee how \u201cMan\u201d and \u201cWoman\u201d are much more similar to each other than either of\nthem is to \u201cking\u201d? This tells you something. These vector representations\ncapture quite a bit of the information/meaning/associations of these words.\n\nHere\u2019s another list of examples (compare by vertically scanning the columns\nlooking for columns with similar colors):\n\nA few things to point out:\n\n  1. There\u2019s a straight red column through all of these different words. They\u2019re similar along that dimension (and we don\u2019t know what each dimensions codes for)\n  2. You can see how \u201cwoman\u201d and \u201cgirl\u201d are similar to each other in a lot of places. The same with \u201cman\u201d and \u201cboy\u201d\n  3. \u201cboy\u201d and \u201cgirl\u201d also have places where they are similar to each other, but different from \u201cwoman\u201d or \u201cman\u201d. Could these be coding for a vague conception of youth? possible.\n  4. All but the last word are words representing people. I added an object (water) to show the differences between categories. You can, for example, see that blue column going all the way down and stopping before the embedding for \u201cwater\u201d.\n  5. There are clear places where \u201cking\u201d and \u201cqueen\u201d are similar to each other and distinct from all the others. Could these be coding for a vague concept of royalty?\n\n## Analogies\n\n> \"Words can carry any burden we wish. All that's required is agreement and a\n> tradition upon which to build.\" ~God Emperor of Dune\n\nThe famous examples that show an incredible property of embeddings is the\nconcept of analogies. We can add and subtract word embeddings and arrive at\ninteresting results. The most famous example is the formula: \u201cking\u201d - \u201cman\u201d +\n\u201cwoman\u201d:\n\nUsing the Gensim library in python, we can add and subtract word vectors, and\nit would find the most similar words to the resulting vector. The image shows\na list of the most similar words, each with its cosine similarity.\n\nWe can visualize this analogy as we did previously:\n\nThe resulting vector from \"king-man+woman\" doesn't exactly equal \"queen\", but\n\"queen\" is the closest word to it from the 400,000 word embeddings we have in\nthis collection.\n\nNow that we\u2019ve looked at trained word embeddings, let\u2019s learn more about the\ntraining process. But before we get to word2vec, we need to look at a\nconceptual parent of word embeddings: the neural language model.\n\n# Language Modeling\n\n> \u201cThe prophet is not diverted by illusions of past, present and future. The\n> fixity of language determines such linear distinctions. Prophets hold a key\n> to the lock in a language.\n>\n> This is not a mechanical universe. The linear progression of events is\n> imposed by the observer. Cause and effect? That's not it at all. The prophet\n> utters fateful words. You glimpse a thing \"destined to occur.\" But the\n> prophetic instant releases something of infinite portent and power. The\n> universe undergoes a ghostly shift.\u201d ~God Emperor of Dune\n\nIf one wanted to give an example of an NLP application, one of the best\nexamples would be the next-word prediction feature of a smartphone keyboard.\nIt\u2019s a feature that billions of people use hundreds of times every day.\n\nNext-word prediction is a task that can be addressed by a language model. A\nlanguage model can take a list of words (let\u2019s say two words), and attempt to\npredict the word that follows them.\n\nIn the screenshot above, we can think of the model as one that took in these\ntwo green words (thou shalt) and returned a list of suggestions (\u201cnot\u201d being\nthe one with the highest probability):\n\nWe can think of the model as looking like this black box:\n\nBut in practice, the model doesn\u2019t output only one word. It actually outputs a\nprobability score for all the words it knows (the model\u2019s \u201cvocabulary\u201d, which\ncan range from a few thousand to over a million words). The keyboard\napplication then has to find the words with the highest scores, and present\nthose to the user.\n\nThe output of the neural language model is a probability score for all the\nwords the model knows. We're referring to the probability as a percentage\nhere, but 40% would actually be represented as 0.4 in the output vector.\n\nAfter being trained, early neural language models (Bengio 2003) would\ncalculate a prediction in three steps:\n\nThe first step is the most relevant for us as we discuss embeddings. One of\nthe results of the training process was this matrix that contains an embedding\nfor each word in our vocabulary. During prediction time, we just look up the\nembeddings of the input word, and use them to calculate the prediction:\n\nLet\u2019s now turn to the training process to learn more about how this embedding\nmatrix was developed.\n\n# Language Model Training\n\n> \u201cA process cannot be understood by stopping it. Understanding must move with\n> the flow of the process, must join it and flow with it.\u201d ~Dune\n\nLanguage models have a huge advantage over most other machine learning models.\nThat advantage is that we are able to train them on running text \u2013 which we\nhave an abundance of. Think of all the books, articles, Wikipedia content, and\nother forms of text data we have lying around. Contrast this with a lot of\nother machine learning models which need hand-crafted features and specially-\ncollected data.\n\n> \u201cYou shall know a word by the company it keeps\u201d J.R. Firth\n\nWords get their embeddings by us looking at which other words they tend to\nappear next to. The mechanics of that is that\n\n  1. We get a lot of text data (say, all Wikipedia articles, for example). then\n  2. We have a window (say, of three words) that we slide against all of that text.\n  3. The sliding window generates training samples for our model\n\nAs this window slides against the text, we (virtually) generate a dataset that\nwe use to train a model. To look exactly at how that\u2019s done, let\u2019s see how the\nsliding window processes this phrase:\n\n> \u201cThou shalt not make a machine in the likeness of a human mind\u201d ~Dune\n\nWhen we start, the window is on the first three words of the sentence:\n\nWe take the first two words to be features, and the third word to be a label:\n\nWe now have generated the first sample in the dataset we can later use to\ntrain a language model.\n\nWe then slide our window to the next position and create a second sample:\n\nAn the second example is now generated.\n\nAnd pretty soon we have a larger dataset of which words tend to appear after\ndifferent pairs of words:\n\nIn practice, models tend to be trained while we\u2019re sliding the window. But I\nfind it clearer to logically separate the \u201cdataset generation\u201d phase from the\ntraining phase. Aside from neural-network-based approaches to language\nmodeling, a technique called N-grams was commonly used to train language\nmodels (see: Chapter 3 of Speech and Language Processing). To see how this\nswitch from N-grams to neural models reflects on real-world products, here\u2019s a\n2015 blog post from Swiftkey, my favorite Android keyboard, introducing their\nneural language model and comparing it with their previous N-gram model. I\nlike this example because it shows you how the algorithmic properties of\nembeddings can be described in marketing speech.\n\n## Look both ways\n\n> \"Paradox is a pointer telling you to look beyond it. If paradoxes bother\n> you, that betrays your deep desire for absolutes. The relativist treats a\n> paradox merely as interesting, perhaps amusing or even, dreadful thought,\n> educational.\" ~God Emperor of Dune\n\nKnowing what you know from earlier in the post, fill in the blank:\n\nThe context I gave you here is five words before the blank word (and an\nearlier mention of \u201cbus\u201d). I\u2019m sure most people would guess the word bus goes\ninto the blank. But what if I gave you one more piece of information \u2013 a word\nafter the blank, would that change your answer?\n\nThis completely changes what should go in the blank. the word red is now the\nmost likely to go into the blank. What we learn from this is the words both\nbefore and after a specific word carry informational value. It turns out that\naccounting for both directions (words to the left and to the right of the word\nwe\u2019re guessing) leads to better word embeddings. Let\u2019s see how we can adjust\nthe way we\u2019re training the model to account for this.\n\n# Skipgram\n\n> \u201cIntelligence takes chance with limited data in an arena where mistakes are\n> not only possible but also necessary.\u201d ~Chapterhouse: Dune\n\nInstead of only looking two words before the target word, we can also look at\ntwo words after it.\n\nIf we do this, the dataset we\u2019re virtually building and training the model\nagainst would look like this:\n\nThis is called a Continuous Bag of Words architecture and is described in one\nof the word2vec papers [pdf]. Another architecture that also tended to show\ngreat results does things a little differently.\n\nInstead of guessing a word based on its context (the words before and after\nit), this other architecture tries to guess neighboring words using the\ncurrent word. We can think of the window it slides against the training text\nas looking like this:\n\nThe word in the green slot would be the input word, each pink box would be a\npossible output.\n\nThe pink boxes are in different shades because this sliding window actually\ncreates four separate samples in our training dataset:\n\nThis method is called the skipgram architecture. We can visualize the sliding\nwindow as doing the following:\n\nThis would add these four samples to our training dataset:\n\nWe then slide our window to the next position:\n\nWhich generates our next four examples:\n\nA couple of positions later, we have a lot more examples:\n\n## Revisiting the training process\n\n> \"Muad'Dib learned rapidly because his first training was in how to learn.\n> And the first lesson of all was the basic trust that he could learn. It's\n> shocking to find how many people do not believe they can learn, and how many\n> more believe learning to be difficult.\" ~Dune\n\nNow that we have our skipgram training dataset that we extracted from existing\nrunning text, let\u2019s glance at how we use it to train a basic neural language\nmodel that predicts the neighboring word.\n\nWe start with the first sample in our dataset. We grab the feature and feed to\nthe untrained model asking it to predict an appropriate neighboring word.\n\nThe model conducts the three steps and outputs a prediction vector (with a\nprobability assigned to each word in its vocabulary). Since the model is\nuntrained, it\u2019s prediction is sure to be wrong at this stage. But that\u2019s okay.\nWe know what word it should have guessed \u2013 the label/output cell in the row\nwe\u2019re currently using to train the model:\n\nThe 'target vector' is one where the target word has the probability 1, and\nall other words have the probability 0.\n\nHow far off was the model? We subtract the two vectors resulting in an error\nvector:\n\nThis error vector can now be used to update the model so the next time, it\u2019s a\nlittle more likely to guess thou when it gets not as input.\n\nAnd that concludes the first step of the training. We proceed to do the same\nprocess with the next sample in our dataset, and then the next, until we\u2019ve\ncovered all the samples in the dataset. That concludes one epoch of training.\nWe do it over again for a number of epochs, and then we\u2019d have our trained\nmodel and we can extract the embedding matrix from it and use it for any other\napplication.\n\nWhile this extends our understanding of the process, it\u2019s still not how\nword2vec is actually trained. We\u2019re missing a couple of key ideas.\n\n# Negative Sampling\n\n> \u201cTo attempt an understanding of Muad'Dib without understanding his mortal\n> enemies, the Harkonnens, is to attempt seeing Truth without knowing\n> Falsehood. It is the attempt to see the Light without knowing Darkness. It\n> cannot be.\u201d ~Dune\n\nRecall the three steps of how this neural language model calculates its\nprediction:\n\nThe third step is very expensive from a computational point of view \u2013\nespecially knowing that we will do it once for every training sample in our\ndataset (easily tens of millions of times). We need to do something to improve\nperformance.\n\nOne way is to split our target into two steps:\n\n  1. Generate high-quality word embeddings (Don\u2019t worry about next-word prediction).\n  2. Use these high-quality embeddings to train a language model (to do next-word prediction).\n\nWe\u2019ll focus on step 1. in this post as we\u2019re focusing on embeddings. To\ngenerate high-quality embeddings using a high-performance model, we can switch\nthe model\u2019s task from predicting a neighboring word:\n\nAnd switch it to a model that takes the input and output word, and outputs a\nscore indicating if they\u2019re neighbors or not (0 for \u201cnot neighbors\u201d, 1 for\n\u201cneighbors\u201d).\n\nThis simple switch changes the model we need from a neural network, to a\nlogistic regression model \u2013 thus it becomes much simpler and much faster to\ncalculate.\n\nThis switch requires we switch the structure of our dataset \u2013 the label is now\na new column with values 0 or 1. They will be all 1 since all the words we\nadded are neighbors.\n\nThis can now be computed at blazing speed \u2013 processing millions of examples in\nminutes. But there\u2019s one loophole we need to close. If all of our examples are\npositive (target: 1), we open ourself to the possibility of a smartass model\nthat always returns 1 \u2013 achieving 100% accuracy, but learning nothing and\ngenerating garbage embeddings.\n\nTo address this, we need to introduce negative samples to our dataset \u2013\nsamples of words that are not neighbors. Our model needs to return 0 for those\nsamples. Now that\u2019s a challenge that the model has to work hard to solve \u2013 but\nstill at blazing fast speed.\n\nFor each sample in our dataset, we add negative examples. Those have the same\ninput word, and a 0 label.\n\nBut what do we fill in as output words? We randomly sample words from our\nvocabulary\n\nThis idea is inspired by Noise-contrastive estimation [pdf]. We are\ncontrasting the actual signal (positive examples of neighboring words) with\nnoise (randomly selected words that are not neighbors). This leads to a great\ntradeoff of computational and statistical efficiency.\n\n# Skipgram with Negative Sampling (SGNS)\n\nWe have now covered two of the central ideas in word2vec: as a pair, they\u2019re\ncalled skipgram with negative sampling.\n\n# Word2vec Training Process\n\n> \"The machine cannot anticipate every problem of importance to humans. It is\n> the difference between serial bits and an unbroken continuum. We have the\n> one; machines are confined to the other.\" ~God Emperor of Dune\n\nNow that we\u2019ve established the two central ideas of skipgram and negative\nsampling, we can proceed to look closer at the actual word2vec training\nprocess.\n\nBefore the training process starts, we pre-process the text we\u2019re training the\nmodel against. In this step, we determine the size of our vocabulary (we\u2019ll\ncall this vocab_size, think of it as, say, 10,000) and which words belong to\nit.\n\nAt the start of the training phase, we create two matrices \u2013 an Embedding\nmatrix and a Context matrix. These two matrices have an embedding for each\nword in our vocabulary (So vocab_size is one of their dimensions). The second\ndimension is how long we want each embedding to be (embedding_size \u2013 300 is a\ncommon value, but we\u2019ve looked at an example of 50 earlier in this post).\n\nAt the start of the training process, we initialize these matrices with random\nvalues. Then we start the training process. In each training step, we take one\npositive example and its associated negative examples. Let\u2019s take our first\ngroup:\n\nNow we have four words: the input word not and output/context words: thou (the\nactual neighbor), aaron, and taco (the negative examples). We proceed to look\nup their embeddings \u2013 for the input word, we look in the Embedding matrix. For\nthe context words, we look in the Context matrix (even though both matrices\nhave an embedding for every word in our vocabulary).\n\nThen, we take the dot product of the input embedding with each of the context\nembeddings. In each case, that would result in a number, that number indicates\nthe similarity of the input and context embeddings\n\nNow we need a way to turn these scores into something that looks like\nprobabilities \u2013 we need them to all be positive and have values between zero\nand one. This is a great task for sigmoid, the logistic operation.\n\nAnd we can now treat the output of the sigmoid operations as the model\u2019s\noutput for these examples. You can see that taco has the highest score and\naaron still has the lowest score both before and after the sigmoid operations.\n\nNow that the untrained model has made a prediction, and seeing as though we\nhave an actual target label to compare against, let\u2019s calculate how much error\nis in the model\u2019s prediction. To do that, we just subtract the sigmoid scores\nfrom the target labels.\n\nerror = target - sigmoid_scores\n\nHere comes the \u201clearning\u201d part of \u201cmachine learning\u201d. We can now use this\nerror score to adjust the embeddings of not, thou, aaron, and taco so that the\nnext time we make this calculation, the result would be closer to the target\nscores.\n\nThis concludes the training step. We emerge from it with slightly better\nembeddings for the words involved in this step (not, thou, aaron, and taco).\nWe now proceed to our next step (the next positive sample and its associated\nnegative samples) and do the same process again.\n\nThe embeddings continue to be improved while we cycle through our entire\ndataset for a number of times. We can then stop the training process, discard\nthe Context matrix, and use the Embeddings matrix as our pre-trained\nembeddings for the next task.\n\n# Window Size and Number of Negative Samples\n\nTwo key hyperparameters in the word2vec training process are the window size\nand the number of negative samples.\n\nDifferent tasks are served better by different window sizes. One heuristic is\nthat smaller window sizes (2-15) lead to embeddings where high similarity\nscores between two embeddings indicates that the words are interchangeable\n(notice that antonyms are often interchangable if we\u2019re only looking at their\nsurrounding words \u2013 e.g. good and bad often appear in similar contexts).\nLarger window sizes (15-50, or even more) lead to embeddings where similarity\nis more indicative of relatedness of the words. In practice, you\u2019ll often have\nto provide annotations that guide the embedding process leading to a useful\nsimilarity sense for your task. The Gensim default window size is 5 (five\nwords before and five words after the input word, in addition to the input\nword itself).\n\nThe number of negative samples is another factor of the training process. The\noriginal paper prescribes 5-20 as being a good number of negative samples. It\nalso states that 2-5 seems to be enough when you have a large enough dataset.\nThe Gensim default is 5 negative samples.\n\n# Conclusion\n\n> \u201cIf it falls outside your yardsticks, then you are engaged with\n> intelligence, not with automation\u201d ~God Emperor of Dune\n\nI hope that you now have a sense for word embeddings and the word2vec\nalgorithm. I also hope that now when you read a paper mentioning \u201cskip gram\nwith negative sampling\u201d (SGNS) (like the recommendation system papers at the\ntop), that you have a better sense for these concepts. As always, all feedback\nis appreciated @JayAlammar.\n\n# References & Further Readings\n\n  * Distributed Representations of Words and Phrases and their Compositionality [pdf]\n  * Efficient Estimation of Word Representations in Vector Space [pdf]\n  * A Neural Probabilistic Language Model [pdf]\n  * Speech and Language Processing by Dan Jurafsky and James H. Martin is a leading resource for NLP. Word2vec is tackled in Chapter 6.\n  * Neural Network Methods in Natural Language Processing by Yoav Goldberg is a great read for neural NLP topics.\n  * Chris McCormick has written some great blog posts about Word2vec. He also just released The Inner Workings of word2vec, an E-book focused on the internals of word2vec.\n  * Want to read the code? Here are two options:\n\n    * Gensim\u2019s python implementation of word2vec\n    * Mikolov\u2019s original implementation in C \u2013 better yet, this version with detailed comments from Chris McCormick.\n  * Evaluating distributional models of compositional semantics\n  * On word embeddings, part 2\n  * Dune\n\nWritten on March 27, 2019\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-\nShareAlike 4.0 International License. Attribution example: Alammar, J (2018).\nThe Illustrated Transformer [Blog post]. Retrieved from\nhttps://jalammar.github.io/illustrated-transformer/\n\nNote: If you translate any of the posts, let me know so I can link your\ntranslation to the original post. My email is in the about page.\n\n", "frontpage": false}
