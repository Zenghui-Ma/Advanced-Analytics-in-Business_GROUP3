{"aid": "40098405", "title": "Stop Doing Cloud", "url": "https://grski.pl/self-host", "domain": "grski.pl", "votes": 42, "user": "dvfjsdhgfv", "posted_at": "2024-04-20 16:12:28", "comments": 13, "source_title": "Stop going to the cloud and getting scammed. $200 infra to serve your startup till 100k monthly users in 15 minutes. Self-hosted Postgres, caddyserver and docker-compose FTW.", "source_text": "Stop going to the cloud and getting scammed. $200 infra to serve your startup\ntill 100k monthly users in 15 minutes. Self-hosted Postgres, caddyserver and\ndocker-compose FTW. - Olaf G\u00f3rski\n\n### 2023-10-20\n\n# Stop going to the cloud and getting scammed. $200 infra to serve your\nstartup till 100k monthly users in 15 minutes. Self-hosted Postgres,\ncaddyserver and docker-compose FTW.\n\n## STOP DOING CLOUD\n\nThis will be a feisty juicy article, a bit controversial. I think more than a\nhalf of the users of the cloud/kubernetes would be better off without it. AWS\nshould stand for how to have people pay for our infra we need once per year\nduring black friday and actually make money out of it. Declouding is a nice\ntrend I'm seeing now. 37signals have some good stuff on it. I'll ad my share\nas to why what has once been something cool has evolved into an abomination\nthat often adds more complexity and problems than it brings, at least for some\npeople.\n\nSure it has it's uses at a certain scale and so on. The problem is almost no\none is at such a scale and never will be, but we are blindly following a\ntrend, pretending it's not the reality.\n\nWhy not try... Simplicty? Boring old stuff that just works, is easily\ndebuggable and that even one person can grasp?\n\nNo your startup with 100k monthly users probably doesn't need all the stuff\nAWS excells at. To be honest most of you will be fine running a single\ndedicated bare-metal server.\n\nCloud pricing is unclear often, performance is not that dependable, especially\non shared resources. To bring down costs you need to often sign up for long-\nterm plans. So on so forth. Layers of abstractions upon abstractions.\n\nI still do use the cloud in some of my work, but there's an alternative people\nhave forgotten about - actually hosting your shit. Owning your data. Your\narchitecture. Everything. Today I'll show you an example how we can do that.\nIn this article we will go through setting up a self-hosted postgres instance,\nreplicated/scalable API, load balancer, automatic ssl management, simple\ndeployment that can be automated in 10 minutes and lastly, we will do that for\nunder $200 per month and within 15 minutes. With this setup in some cases I'd\nargue you can handle up to 1M monthly active users without a hitch. See why\nthe cloud providers and gurus have...\n\n## They have played us for absolute fools.\n\nIn the article I assume you have a server running somewhere. Preferable a\ndedicated one. In my case it's 80 core 128 gb hetzner, that I got for $200 per\nmonth.\n\nBefore starting let's install some utils we will need and update our server.\n\n    \n    \n    sudo apt update && sudo apt upgrade sudo apt install gnupg2 wget vim ca-certificates curl gnupg lsb-release\n\n## Installing postgres\n\n### Installing needed packages\n\n    \n    \n    sudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.list.d/pgdg.list' curl -fsSL https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/postgresql.gpg sudo apt-get --purge remove postgresql postgresql-* # IF YOU HAD POSTGRES PREVIOUSLY sudo apt update sudo apt install postgresql-16 postgresql-contrib-16 sudo systemctl start postgresql sudo systemctl enable postgresql\n\nLet's quickly walk through what we did here. We've added newest postgres repos\nso that our server knows what and where to install from. In case of ubuntu\n22.04, the default postgres version that the distro repos come with is\npostgres 14. We want the new fancy shiny stuff, so we had to make that extra\nstep.\n\nThen, we optionally uninstall previous postgres versions. I doubt you had any,\nbut adding this step as it might be helpful for some of you. Be careful\nthough. That --purge thing will purge a lot of stuff. Your data from databases\nincluded. If you want to ugprade from existing postgres installation, this\nguide is not for you.\n\nAfter that we update our sources and install postgres + some needed packages.\n\nLastly we start the postgresql service and make it enabled - so it boots after\nstartup.\n\nNow, we have to... Well, actually that's it. AWS marketers have done a good\njob in making you think installing and running a database was hard and a\ncomplex task. In some cases it is, indeed. But for the average IT Joe/startup?\nI wouldn't say so.\n\n### Creating new database and user\n\nNow we need to do a little bit of setup on our database. In order to do that,\nlet's connect to it using psql. How?\n\n    \n    \n    sudo -u postgres psql\n\nnow you should see something like:\n\n    \n    \n    psql (16.0 (Ubuntu 16.0-1.pgdg22.04+1)) Type \"help\" for help. postgres=#\n\nBoom. There we are. To test if our efforts in installing the newest postgres\nversion have not failed, type:\n\n    \n    \n    psql (16.0 (Ubuntu 16.0-1.pgdg22.04+1)) Type \"help\" for help. postgres=# SELECT version(); version ----------------------------------------------------------------------------------------------------------------------------------- PostgreSQL 16.0 (Ubuntu 16.0-1.pgdg22.04+1) on x86_64-pc-linux-gnu, compiled by gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0, 64-bit (1 row)\n\nAs you can see, PostgreSQL 16.0. Congrats, we made it brahs.\n\nCurrently you are inside your postgres, running as the default allmighty\npostgres user on postgres database. Now - we DO NOT EVER want to run our apps\non this database. Don't be a lazy bum. It's a big security breach potentailly.\nSo what do we do instead one might ask? That is a trivial question - we need\nto create a seprate database and a separate user for that database. Usually\nyou have separate db (or multiple dbs actually) for an app/service couple with\nuser just for that db.\n\nThat way if someone ever manages to break into your DB, in case you are\nhosting multiple dbs with data from multiple apps, they only get access to\nthat one particular db. Is it hard? Nope. Check this out:\n\n    \n    \n    postgres=# CREATE DATABASE yourdbname; CREATE DATABASE postgres=# CREATE USER youruser WITH ENCRYPTED PASSWORD 'yourpass'; CREATE ROLE postgres=# GRANT ALL PRIVILEGES ON DATABASE yourdbname TO youruser; GRANT postgres=# ALTER DATABASE yourdbname OWNER TO youruser; ALTER DATABASE\n\nWe created a new user with a particular password, created a new database. Then\nwe assigned the user privilages to perform all operations on said database,\nbut only on that database.\n\nThe last line is needed because we created the database as the postgres user.\nWhich means that while the user can perform actions on the database, he can\nonly perform actions on the database objects that are his own. Because we\ncreated the database as the postgres user, and during db creation it gets\ncreated with some default schemas/tables, by default the owner of these is the\nuser that created it. In our case - postgres. So other than allowing our new\nuser to perform any action on the said database, we need to now make him an\nowner of the stuff that's already existing in the database so he can modify it\ntoo if needed, and it will be.\n\nBy the way interesting concept right? Even when you create an empty database,\nit's already populated with some stuff, so it ain't empty. IT, right?\n\nThat's pretty much it. Or is it?\n\n### Connecting to postgres from outside of localhost\n\nPostgres, by default, only allows you to connect to itself from\nlocalhost/local machine to simplify. Meaning - any connections from other ips,\nnetworks etc. will be rejected. It's a very needed security measure that\nprevents random people from the internet to try and brute force their way into\nyour database. That is the last thing you want.\n\nHowever we live in a world where everything is running in a container.\nContainers have their own networks (usually) and when we make requests from\ninside of the container, the network we are in will be a bit different,\nmeaning we won't be 'marked' as localhost, which in turn currently will make\npostgres reject our connection, even if we specify correct credentials.\n\nI know it may sound tricky - how come, we are on the same machine, local\nmachine. Why is our request treated as it isn't? This relats to how docker,\ncontainers and their networking works. Docker has it's own private network for\nall the stuff it does, sometimes sharing it with the host (in the host network\nmode) or having a 'bridge' that acts as a, well, bridge, between the local\nnetwork and docker network, which allows you to, for example, call services\nhosted on the host machine, from within docker container.\n\nThis way you can have multiple docker containers or docker-composes running,\nsome of which internally are using the same ports, without conflicts and so\non. They are usually put in other networks created eg. per docker-compose\n(unless you specify otherwise).\n\nIt's a great thing, however in this case it complicates stuff for us, but not\nby much. What do we have to do?\n\nWell, first of all, install docker! It'll come in handy, right?\n\n## Installing docker on ubuntu 22.04\n\nFirst off, again - if you tried to install something before hand you might\nwant to do this to purge everything and have a clean slate. Again - be\ncareful.\n\n    \n    \n    sudo apt remove docker-desktop rm -r $HOME/.docker/desktop sudo rm /usr/local/bin/com.docker.cli sudo apt purge docker-desktop\n\nOnce we have that, we will add new sources to our repos, this time for docker,\nsimilarly as we did for our postgres.\n\n    \n    \n    curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null sudo apt update\n\nNow, let's see what versions are available to us:\n\n    \n    \n    apt-cache madison docker-ce | awk '{ print $3 }' 5:24.0.6-1~ubuntu.22.04~jammy 5:24.0.5-1~ubuntu.22.04~jammy 5:24.0.4-1~ubuntu.22.04~jammy 5:24.0.3-1~ubuntu.22.04~jammy 5:24.0.2-1~ubuntu.22.04~jammy 5:24.0.1-1~ubuntu.22.04~jammy 5:24.0.0-1~ubuntu.22.04~jammy 5:23.0.6-1~ubuntu.22.04~jammy 5:23.0.5-1~ubuntu.22.04~jammy 5:23.0.4-1~ubuntu.22.04~jammy 5:23.0.3-1~ubuntu.22.04~jammy 5:23.0.2-1~ubuntu.22.04~jammy 5:23.0.1-1~ubuntu.22.04~jammy (...)\n\nI decided to go with the newest one, if you for some reason want to install\nanother, feel free.\n\n    \n    \n    VERSION_STRING=5:24.0.6-1~ubuntu.22.04~jammy sudo apt install docker-ce=$VERSION_STRING docker-ce-cli=$VERSION_STRING containerd.io docker-compose-plugin\n\naaand done. Let's test our docker installation.\n\n    \n    \n    docker run hello-world Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world 719385e32844: Pull complete Digest: sha256:88ec0acaa3ec199d3b7eaf73588f4518c25f9d34f58ce9a0df68429c5af48e8d Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/\n\nSeems to be working. How about docker-compose?\n\n    \n    \n    docker-compose Command 'docker-compose' not found, but can be installed with: apt install docker-compose\n\nNot there, weird? Nope. Some of you might be still used to the old docker-\ncompose thingy, however some time ago it got moved to be a part of the docker\nitself, which means that now instead of docker-compose you do:\n\n    \n    \n    docker compose Usage: docker compose [OPTIONS] COMMAND Define and run multi-container applications with Docker.\n\nAlright! We set. Almost.\n\nCurrently, if you sshed on a clean server, which I assume you did, you are\nrunning as the root user. You can check this by typing:\n\n    \n    \n    whoami root\n\nThe problem with that is similar to the situation with our postgres and the\nalmighty postgres user.\n\nIdeally, we do not want to run our containers as root, to prevent attackers\nfrom being able to do bad stuff to the whole server. Let's create a new user\nwhere we will be running our containers. You can have user per app or service,\nbut not sure if you need that. Just not running on root is usually good\nenough.\n\nHow?\n\n### Running docker on non-user or rootless docker\n\nThat's all quite simple.\n\nWe need to create a new user, add it to the sudoers grup, set a password for\nit and lastly add it to the docker group. In our case we will create a prod\nuser and then add it to the sudo group and docker group.\n\n    \n    \n    sudo useradd prod sudo usermod -aG sudo prod sudo passwd prod sudo usermod -aG docker docker\n\nThat's quite much it for now.\n\n### Enabling docker containers to connect to host postgres\n\nThe sane way. Some people deal with the issue described before, the one\nregarding connections from outside of localhost, by allowing * which means any\nand all networks/ips. This is a NO GO for production, really. The sane way is,\nas mentioned, to only allow specific networks. In our case docker network. How\nto do that?\n\nWe have to find out what is the local ip address that our docker network got\nassigned and simply allow traffic from that network to access. Sounds tricky,\nbut ain't.\n\nNow, before we proceed, I'm not that proficient in networking to be frank.\nWhich means that my solution, while working, might not be the ideal one. Happy\nfor feedback from someone more knowledgeable in the topic.\n\nWe want to add our docker network to those permitted inside our postgres. This\nimplies we need to know the docker network address. How to get it?\n\n    \n    \n    ip addr | grep docker 3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0\n\n172.17.0.1 in this case is the network address we need. It'll probably be the\nsame in your case, but doesn't have to be. Now that we have it, let's move on.\nHow to edit postgres config?\n\nFirst, my young padawan, we will need to find the location of our\npostgresql.conf file - which, surprisingly, is the file used to configure\npostgres.\n\nWe can do that with:\n\n    \n    \n    sudo find / -type f -name postgresql.conf /etc/postgresql/16/main/postgresql.conf\n\nIn my case it's in /etc/postgresql/16/main/postgresql.conf. The probability of\nit being the same for you, if you are running ubuntu 22.04 and followed this\nguide, as the probability of us living in a simulation or being at the\nbeginning of an AI bubble. Get back to the topic Olaf. Gosh.\n\nOkay, we know where the file is, we need to edit it. Type in:\n\n    \n    \n    sudo vim /etc/postgresql/16/main/postgresql.conf\n\nand look for listen_addresses part. In vim you can do a search by typing\n/{phrase} so /listen_addresses should navigate you to the proper line. In my\ncase it looks like this:\n\n    \n    \n    #listen_addresses = 'localhost' # what IP address(es) to listen on;\n\nwe need to uncomment the line and edit it so it allows connections from our\ndocker network ip. So:\n\n    \n    \n    listen_addresses = \"localhost,172.17.0.1\"\n\nthen :wq and done.\n\nNow we also need to edit pg_hba.conf to also allow this particular network to\nacces our database while authenticating with a password.\n\nFirst let's find it:\n\n    \n    \n    sudo find / -type f -name pg_hba.conf /etc/postgresql/16/main/pg_hba.conf\n\nand edit it with:\n\n    \n    \n    sudo vim /etc/postgresql/16/main/pg_hba.conf\n\nnow again, navigate to a section containing \"IPv4 local connections\":\n\n    \n    \n    # IPv4 local connections: host all all 127.0.0.1/32 scram-sha-256 # IPv6 local connections: host all all ::1/128 scram-sha-256\n\nwe need to edit the IPv4 section to look like this:\n\n    \n    \n    # IPv4 local connections: host all all 127.0.0.1/32 scram-sha-256 host all all 172.17.0.0/16 scram-sha-256\n\nWhat does the /16 after the netowrk address mean? Match the first 16 bytes of\nthe address, so the 2 first. numbers, rest can change.\n\nNow, let's restart our postgres.\n\n    \n    \n    sudo /etc/init.d/postgresql restart\n\nImportant note. You might need to add something like this to your docker-\ncompose:\n\n    \n    \n    extra_hosts: - \"host.docker.internal:172.17.0.1\"\n\nFor each service that will access it, and edit the connection string for\npostgres host to be: host.docker.internal. Either that or just use 172.17.0.1\nvalue directly.\n\nWith docker and postgres set up, the world is yours to take. But is this it? I\nwouldn't be myself if i just ended with this. Let's take it up a notch. I mean\nwe usually want to have something in front of our API, some reverse proxy,\nmaybe capability to scale, have multiple replicas and so on. Performance and\nscaling stuff. Simple solution for that too.\n\n## BUT MUH SCALABILITY, LOAD BALANCING, SSL and whatnot\n\nYe I hear you, all the folks with 1k monthly active users, serving up to 2\nrequests per second, usually screaming about scalability the loudest. WHERE\"S\nKUBERNETES? WHERE\"S MY CLOUD SCALING STUFF, REEE!!one! AND THE DEVOPS TEAM?\nARE YOU A FOOL?\n\nI'll give you some love too, fret not.\n\nFor the database, the case is simple. With a dedicated bare metal server that\nI recommend you get, unless you do some horrendeous things in the schema or\nquery, well, you can handle TONS of data & traffic on a single machine.With\njust one instance of $200 ARM Hetzner with 80 dedicated cores, 128 GB of RAM,\n2TB NVME PCIe SSD, how much more do you need in most cases?\n\nYeah, availability zones and so on, but let's take a step back. How many of\nyou are truly running multi region & multi availability zones DB deployments?\nHMMM? Thought so. Sorry to break it to you, but hype/conference driven\ndevelopment isn't the only way to go. I'd argue that at least 90-95% of\ncurrent startups could probably run just fine with this single instance only.\nOkay, maybe some of you would need something like S3 (Cloudflare R2 maybe?).\nOutgrowing this setup will probably mean you already got enough tracton,\ncustomers and money to actually start your own colocation thingy with a\ndedicated team. Backups? Survavibility? We'll talk about that part later.\n\nSo, in this post, we won't cover how to horizontally scale the db, as I think\nit's simply not needed for the audience i target this too. What is needed\nthough, is probably replication of the apis/scaling them and then reverse\nproxy/managing ssl/load balancing. That's what we will do. Let's start with\nreplicating our api to an arbitrary size. How can we do that?\n\nDocker-compose lol.\n\n## Ditch Kubernetes, docker compose for the win\n\nThis part will be quite short, sweet and simple. Probably not many of you\nknow, but docker compose supports replication out of the box. Why wouldn't it.\nHow do we go about it?\n\n    \n    \n    api: build: context: . depends_on: database: condition: service_healthy restart: always deploy: replicas: 4 ports: - \"8000-8003:8000\"\n\nthe key part here being:\n\n    \n    \n    deploy: replicas: 4 ports: - \"8000-8003:8000\"\n\nafter that, just do docker compose up. And then boom. You done. Multiple\nreplicas of your api service up and running. With 4 lines of code, 2 of them\nyou already probably have in your code.\n\nRemember what we said - we do not want to run docker as root, so ssh/login\ninto the user we created prod for this purpose.\n\nonce you there, just clone the repo and docker compose up.\n\nYou'll be amazed how fast the deployments can happen. Also about the secrets.\n1password offers some nice options here for such use cases, or in fact, you\ncan even just create .env file, specify it in the docker-compose and be done\nwith it.\n\nLogs can be easily checked with a simple docker compose logs + docker saves\nthem to a file iether way.\n\nBut, what about load balancing, reverse proxy and ssl stuff?\n\n## Load Balancing & automatic ssl with Caddyserver\n\nWe will use caddyserver to act as a reverse proxy, load balancer and to\nautomatically take care of the certificates for us. It's a bit less performant\nthan nginx, true, but the ease of use and convenience it provides is well\nworth it. That plus usually it's not the proxy that will die first. Quite the\nopposite.\n\nSo how do we go about this? Probably complicated? Nope.\n\nWe will let ansible handle all the work for us. Ansible? Yes, you read that\nright. Not terraform.\n\nIn order to do that we will need to create a new user for our ansible to run\non, enable ssh access and do a bit of ansible dev. Let's go. You already know\nthe drill.\n\n    \n    \n    sudo useradd ingres sudo passwd ingres sudo usermod -aG sudo ingres\n\nNow, a small change to what we did before.\n\nWe make sure our user can do sudo operations without password. How?\n\n    \n    \n    visudo\n\nthen find this piece:\n\n    \n    \n    # User privilege specification root ALL=(ALL:ALL) ALL\n\nand add this below (or at the bottom of the file):\n\n    \n    \n    ingres ALL=(ALL) NOPASSWD: ALL\n\nWe could be more granular about permissions here and what it has access to,\nbut that could come in a 2nd iteration, I consider this good enough.\n\nLet's enable SSH access now. This might not be needed on your machine, depends\non the server. I had to do it on my hetzner.\n\nWe need to edit the sshd_config file. How to find where it is? You should know\nby now ;)\n\n    \n    \n    vim /etc/ssh/sshd_config\n\nand find something like this:\n\n    \n    \n    #AuthorizedKeysFile .ssh/authorized_keys\n\nturn it into:\n\n    \n    \n    AuthorizedKeysFile .ssh/authorized_keys AllowUsers root prod dev\n\nadd your ssh key to /home/ingres/.ssh/authorized_keys in order to do that and\neg. add the same ssh key you use for your root account (not ideal):\n\n```bashand lastly: su ingres # we switch the user to make it the owner of the\ndirectory we create mkdir -p /home/ingres/.ssh cat ~/.ssh/authorized_keys >\n/home/ingres/.ssh/authorized_keys\n\n    \n    \n    aaand lastly: ```bash service sshd restart\n\nSetup done. Time to install caddy with ansible. But before that, we need to\nsetup ansible.\n\nI'll assume you have pyenv installed and set up running on your local machine.\nYou can read about that here in my article, or here.\n\nWith that we can:\n\n    \n    \n    pyenv virtualenv 3.11 infrastructure-deployment-3-11 mkdir infrastructure-deployment cd infrastructure-deployment pyenv local infrastructure-deployment-3-11 python -m pip install ansible\n\nPyenv set up. Ansible set up. We will need one more thing - install custom\nrole from ansible galaxy.\n\n    \n    \n    python -m ansible galaxy role install caddy_ansible.caddy_ansible\n\nNow inside our infrastructure-deployment directory on our local machine create\na file called inventory.yml\n\n    \n    \n    all: hosts: bare-metal-hetzner: ansible_host: \"your-host-ip\" ansible_user: \"ingres\" ansible_port: 22\n\naaand caddy_install.yml:\n\n    \n    \n    --- - name: Install Caddy Server hosts: all become: true roles: - role: caddy_ansible.caddy_ansible caddy_conf_filename: Caddyfile caddy_update: true caddy_systemd_capabilities_enabled: true caddy_systemd_capabilities: \"CAP_NET_BIND_SERVICE\" caddy_config: | your-fancy-startup-domain.com { # Compress responses according to Accept-Encoding headers encode gzip zstd # Send API requests to backend reverse_proxy 127.0.0.1:8000 127.0.0.1:8301 127.0.0.1:8302 127.0.0.1:8303 }\n\nrun\n\n    \n    \n    python -m ansible playbook -i inventory.yml caddy_install.yml\n\naaand done.\n\nNow if you go to your-fancy-startup-domain.com, given that proper docker\ncontainers are running, you'll get them.\n\nAutomatic SSL. Automatic load balancing. EVERYTHING WORKS.\n\n## BACKUPS, SURVAVIBILITY\n\nYou can have hourly backups with BorgBackup. How? Brilliant tutorial can be\nfound in hetzner docs. Go read them.\n\nOn top of that add $4 1 TB Hetzner Storage Box linked to your server. Boom.\nDone. You might want to think about adding pg_dump, but IMO just the\nBorgBackup for starters is ok.\n\nMy borg-backup script looks more or less like this:\n\n    \n    \n    #!/bin/sh # First init the repo # ssh -p23 ssh://xxxxx.your-storagebox.de mkdir /home/backup # ssh -p23 ssh://xxxxx@xxxxx.your-storagebox.de mkdir /home/backup/main # borg init --encryption=repokey ssh://xxxxx@xxxxx.your-storagebox.de:23/~/backup/main # Setting this, so the repo does not need to be given on the commandline: export BORG_REPO=ssh://x@x.your-storagebox.de:23/~/backup/main # See the section \"Passphrase notes\" for more infos. export BORG_PASSPHRASE= # some helpers and error handling: info() { printf \"\\n%s %s\\n\\n\" \"$( date )\" \"$*\" >&2; } trap 'echo $( date ) Backup interrupted >&2; exit 2' INT TERM info \"Starting backup\" # Backup the most important directories into an archive named after # the machine this script is currently running on: borg create \\ --verbose \\ --filter AME \\ --list \\ --stats \\ --show-rc \\ --compression lz4 \\ --exclude-caches \\ --exclude 'home/*/.cache/*' \\ --exclude 'var/tmp/*' \\ --exclude '*__pycache__*' \\ --exclude '*.pyenv*' \\ \\ ::'{hostname}-{now}' \\ /etc \\ /home \\ /root \\ /var backup_exit=$? info \"Pruning repository\" # Use the `prune` subcommand to maintain 7 daily, 4 weekly and 6 monthly # archives of THIS machine. The '{hostname}-*' matching is very important to # limit prune's operation to this machine's archives and not apply to # other machines' archives also: borg prune \\ --list \\ --glob-archives '{hostname}-*' \\ --show-rc \\ --keep-daily 7 \\ --keep-weekly 4 \\ --keep-monthly 6 prune_exit=$? # actually free repo disk space by compacting segments info \"Compacting repository\" borg compact compact_exit=$? # use highest exit code as global exit code global_exit=$(( backup_exit > prune_exit ? backup_exit : prune_exit )) global_exit=$(( compact_exit > global_exit ? compact_exit : global_exit )) if [ ${global_exit} -eq 0 ]; then info \"Backup, Prune, and Compact finished successfully\" elif [ ${global_exit} -eq 1 ]; then info \"Backup, Prune, and/or Compact finished with warnings\" else info \"Backup, Prune, and/or Compact finished with errors\" fi exit ${global_exit}\n\nTo run it periodically type in crontab -e\n\nand then\n\n    \n    \n    00 2 * * * /root/borg-backup.sh\n\n## Potential improvements\n\nOfcourse the permissions here and there could be more fine-grained for sure.\n\nWe could also add a bastion in front of the server.\n\nAutomate the deployment so that after each merge stuff gets built & deployed.\n\nAdd monitoring, observability, alerts. (Ain't that hard tbh, we will explore\nthat one day)\n\nThere's much more than that ofcourse but these are the starters.\n\n## Summary\n\nWe have set up:\n\n  1. self-hosted postgres instance with passable initail configuration\n  2. replicated api-service with as many replicas as we want\n  3. proper load balancing and reverse proxy in front of them\n  4. https everywhere\n  5. proper certifcates, all handled automatically\n  6. 1 click deployment of our reverse proxy\n  7. blazing fast deployments/build times in the future (for now manual, but can easily be automated)\n  8. ability to potentially handle hundreds of thousands of users\n  9. very predictable cost & performance\n  10. regular FULL backups\n  11. no additional deployment code\n  12. Absolutely stunning performance with 80 dedicated cores, 128 gb of ram, 2 TB NVMe SSD (you'd be amazed)\n\nWhat more can I say. Cloud IS NOT the solution for everything. Sometimes you\ncan try the alternative path.\n\nSimilar setup on AWS would be probably $6-10k upwards just for the postgres.\nThat plus it wouldn't match the performance we have here. One thing not\ncovered here is how much performance you gain when all the services are within\none network. No calls outside your network == blazing fast shit.\n\nAll of this in 15 minutes and for $200 monthly.\n\nWant some copium cloud bro?\n\nOfcourse this doesn't adhere to some of you and your companies, but you know\nthat. I've simplified lots of things or generlised. However, for the general\npublic and their needs, I think it's worth rethink the whole cloud sometimes.\n\n\u00a9 Olaf G\u00f3rski 2023\n\nPowered by XD philosophy and braindead.\n\n", "frontpage": true}
