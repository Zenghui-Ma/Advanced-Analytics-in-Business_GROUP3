{"aid": "40052651", "title": "CodeGemma vs. CodeLlama", "url": "https://msty.app/blog/codegemma-vs-codellama", "domain": "msty.app", "votes": 1, "user": "chown", "posted_at": "2024-04-16 14:46:25", "comments": 0, "source_title": "CodeGemma Vs CodeLlama", "source_text": "CodeGemma Vs CodeLlama\n\n# CodeGemma Vs CodeLlama\n\nWed Apr 10 2024\n\nGoogle recently released the CodeGemma model, an LLM trained to handle coding-\nrelated tasks. It claims the model to be faster and more performant than\nCodeLlama, Meta's equivalent Code LLM (see benchmarks below). We decided not\nto take their word for it and ran tests of our own using Msty's convenient\nsplit chats feature.\n\nNote: We will be using the 7B and 7B Instruct variants with similar\nconfigurations for both models. If you are interested in exploring further,\nCodeGemma and CodeLlama are available for download directly through Msty.\n\n### Code Completion\n\nLet's start with one of the most basic (and probably one of the first) tasks\nwe learn to solve as programmers - writing a function to reverse a string.\nLet's ask CodeGemma and CodeLlama to write it for us this time in JavaScript\nsince most of us are already familiar with the programming language.\n\nCodeGemma and CodeLlama write a function to reverse a string in JavaScript\n\nCodeGemma answered with included explanations whereas CodeLlama just provided\nthe answer. Also notable is the approach to the problem by the two models.\nCodeGemma provided a manual solution without using JavaScript's built-in\nmethods. CodeLlama chose to use the built-in reverse() method.\n\n### Code Review / Debugging\n\nLLMs can be useful in spotting errors in data that could be overlooked by the\nhuman eye. In this particular example, we asked the models to debug a VueJS\ncode snippet where we directly modify the value of a prop.\n\nCodeGemma and CodeLlama debug a VueJS code\n\nCodeGemma promptly identified the problem with our code and suggested that we\nshould not modify the value of a prop in a child component. On the other hand,\nCodeLlama's response was a bit off-topic. It reported that the problem was\nactually with our syntax rather than the implementation. Both of the syntaxes\nare correct but CodeLlama completely missed out on the main problem with our\ncode.\n\n### Unit Testing\n\nUsing the reverseString() function that CodeGemma generated earlier to reverse\na string in JavaScript, let's ask our models to write test cases for us.\n\nCodeGemma and CodeLlama generate unit tests for a function that reverses a\nstring in JavaScript\n\nWe noticed that CodeGemma's test cases were pretty comprehensive and covered\nvarious edge cases - while CodeLlama's tests only covered a few basic\nscenarios.\n\n### Text-to-SQL Generation\n\nCode LLMs excel at generating complex database queries. Some models like\nDuckDB NSQL and SQL Coder are specifically trained for this purpose. In the\nfollowing example, we gave CodeGemma and CodeLlama a MySQL schema that tracks\nthe attendance of students in classrooms and asked them both to write a query\nto get the total attendance of a particular classroom on a particular date.\n\nCodeGemma and CodeLlama generate a MySQL query given a schema\n\nCodeLlama took a shortcut approach to the solution and directly queried the\nattendance table where classroom_id was 'MEB-1'. This would not work as\nexpected because the classroom_id column values are of INT type. The correct\nsolution would be the one proposed by CodeGemma where you merge the tables and\nthen filter on the name of the classroom after the join.\n\n### Coding Interview Questions\n\nThe Instruct variants of LLMs are trained to respond in natural language which\nallows them to converse in a more human-friendly manner. This means we can\nleverage the models for tasks like interview preparation and quizzes. We can\nalso set system instructions on the models to further fine-tune their\nconversational style.\n\nFor this exercise, we used the 7B Instruct variants of the models and asked\nthem a question about space complexity for a function from Cracking the Coding\nInterview book. The given function adds adjacent elements between 0 and n.\n\nCodeGemma Instruct and CodeLlama Instruct analyze space complexity for a\nfunction that adds adjacent elements between 0 and n\n\nThe correct answer for the space complexity of this function is O(1) but\nCodeLlama insisted that the complexity is O(n). While both models mentioned\nthat the for loop iterates from 0 to n-1, only CodeGemma noted that this\ndoesn't create new objects or data structures in memory thus resulting in the\nspace complexity of O(1).\n\nOverall, we can see that Google's CodeGemma 7B is much superior to CodeLlama\n7B in handling diverse coding problems. It's also better at writing unit tests\nand analyzing code issues and complexities.\n\nDuring our tests, CodeLlama answered most queries incorrectly and its correct\nanswers were often either limited or it didn't cover a lot of edge case\nscenarios.\n\nCodeLlama also seemed to struggle with providing language-aware markdown\nformatting for generated code snippets.\n\nThat's it for CodeGemma vs CodeLlamma! You can explore more LLMs and compare\ntheir responses side-by-side by downloading Msty and using our split chats\nfeature.\n\nThis table compares the performance of CodeGemma with other similar models on\nboth single and multi-line code completion tasks. Source: Google for\nDevelopers.\n\nMade with sipping lots of \u2615\ufe0f by the bank of the Scioto River in Columbus,\nOhio. If the world runs out of coffee, blame our CloudStack, LLC Team.\n\n", "frontpage": false}
