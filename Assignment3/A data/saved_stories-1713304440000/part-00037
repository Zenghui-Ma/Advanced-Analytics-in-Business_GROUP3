{"aid": "40052779", "title": "AI Inference Now Available in Supabase Edge Functions", "url": "https://supabase.com/blog/ai-inference-now-available-in-supabase-edge-functions", "domain": "supabase.com", "votes": 14, "user": "samwillis", "posted_at": "2024-04-16 14:55:55", "comments": 0, "source_title": "AI Inference now available in Supabase Edge Functions", "source_text": "AI Inference now available in Supabase Edge Functions\n\nGeneral Availability Week: Day 2\n\nLearn more\n\nBack\n\nBlog\n\n# AI Inference now available in Supabase Edge Functions\n\n2024-04-16\n\n\u2022\n\n7 minute read\n\nLakshan PereraEngineering\n\nWe're making it super easy to run AI models within Supabase Edge Functions. A\nnew built-in API is available within the Edge Runtime to run inference\nworkloads in just a few lines of code:\n\n1\n\n// Instantiate a new inference session\n\n2\n\nconst session = new Supabase.ai.Session('gte-small')\n\n3\n\n4\n\n// then use the session to run inference on a prompt\n\n5\n\nconst output = await session.run('Luke, I am your father')\n\n6\n\n7\n\nconsole.log(output)\n\n8\n\n// [ -0.047715719789266586, -0.006132732145488262, ...]\n\nWith this new API you can:\n\n  * Generate embeddings using models like gte-small to store and retrieve with pgvector. This is available today.\n  * Use Large Language Models like llama2 and mistral for GenAI workloads. This will be progressively rolled out as we get our hands on more GPUs.\n\nIn our previous Launch Week we announced support for AI inference via\nTransformers.js. This was a good start but had some shortcomings: it takes\ntime to \u201cboot\u201d because it needs to instantiate a WASM runtime and build the\ninference pipeline. We increased CPU limits to mitigate this, but we knew we\nwanted a better Developer Experience.\n\nIn this post we'll cover some of the improvements to remove cold starts using\nOrt and how we're adding LLM support using Ollama.\n\n## Generating Text Embeddings in Edge Functions#\n\nEmbeddings capture the \"relatedness\" of text, images, video, or other types of\ninformation. Embeddings are stored in the database as an array of floating\npoint numbers, known as vectors. Since we released pgvector on the platform,\nPostgres has become a popular vector database.\n\nToday's release solves a few technical challenges for developers who want to\ngenerate embeddings from the content in their database, giving them the\nability to offload this compute-intensive task to background workers.\n\n### Integrated pgvector experience#\n\nYou can now utilize database webhooks to automatically generate embeddings\nwhenever a new row is inserted into a database table.\n\nBecause embedding creation is a compute-intensive task, it makes sense to\noffload the work from your database. Edge Functions are the perfect\n\u201cbackground worker\u201d. We've created a simple example to show how you can\ngenerate embeddings in Edge Functions: Semantic Search with pgvector and\nSupabase Edge Functions.\n\n### Technical architecture#\n\nEmbedding generation uses the ONNX runtime under the hood. This is a cross-\nplatform inferencing library that supports multiple execution providers from\nCPU to specialized GPUs.\n\nLibraries like transformers.js also use ONNX runtime which, in the context of\nEdge Functions, runs as a WASM module, which can be slow during the\ninstantiation process.\n\nTo solve this, we built a native extension in Edge Runtime that enables using\nONNX runtime via the Rust interface. This was made possible thanks to an\nexcellent Rust wrapper called Ort:\n\nEmbedding generation is fairly lightweight compared to LLM workloads, so it\ncan run on a CPU without hardware acceleration.\n\n### Availability: open source embeddings#\n\nEmbeddings models are available on Edge Functions today. We currently support\ngte-small and we'll add more embeddings models based on user feedback.\n\nEmbedding generation via Supabase.ai API is available today for all Edge\nFunctions users in both local, hosted, and self-hosted platforms.\n\n### Lower costs#\n\nGenerating embeddings in an Edge Function doesn't cost anything extra: we\nstill charge on CPU usage. A typical embedding generation request should run\nin less than a 1s, even from a cold start. Typically it won't use more than\n100-200ms of CPU time.\n\nProprietary LLMs like OpenAI and Claude provide APIs to generate text\nembeddings, charging per token. For example, OpenAI's text-embedding-3-small\ncost $0.02/1M tokens at the time of writing this post.\n\nOpen source text embedding models provide similar performance to OpenAI's paid\nmodels. For example, the gte-small model, which operates on 384 dimensions,\nhas an average of 61.36 compared to OpenAI's text-embedding-3-small, which is\nat 62.26 on the MTEB leaderboard, and they perform search faster with fewer\ndimensions.\n\nWith Supabase Edge Functions, you can generate text embeddings 10x cheaper\nthan OpenAI embeddings APIs.\n\n## Large Language Models in Supabase Edge Functions#\n\nEmbedding generation only a part of the solution. Typically you need an LLM\n(like OpenAI's GPT-3.5) to generate human-like interactions. We're working\nwith Ollama to make this possible with Supabase: local development, self-\nhosted, and on the platform.\n\n### Open source inference models#\n\nWe are excited to announce experimental support for Llama & Mistral with\nSupabase.ai API.\n\nThe API is simple to use, with support for streaming responses:\n\n1\n\nconst session = new Supabase.ai.Session('mistral')\n\n2\n\n3\n\nDeno.serve(async (req: Request) => {\n\n4\n\n// Get the prompt from the query string\n\n5\n\nconst params = new URL(req.url).searchParams\n\n6\n\nconst prompt = params.get('prompt') ?? ''\n\n7\n\n8\n\n// Get the output as a stream\n\n9\n\nconst output = await session.run(prompt, { stream: true })\n\n10\n\n11\n\n// Create a stream\n\n12\n\nconst stream = new ReadableStream({\n\n13\n\nasync start(controller) {\n\n14\n\nconst encoder = new TextEncoder()\n\n15\n\nfor await (const chunk of output) {\n\n16\n\ncontroller.enqueue(encoder.encode(chunk.response ?? ''))\n\n17\n\n}\n\n18\n\n},\n\n19\n\n})\n\n20\n\n21\n\n// Return the stream to the user\n\n22\n\nreturn new Response(stream, {\n\n23\n\nheaders: new Headers({\n\n24\n\n'Content-Type': 'text/event-stream',\n\n25\n\nConnection: 'keep-alive',\n\n26\n\n}),\n\n27\n\n})\n\n28\n\n})\n\nCheck out the full guide here.\n\n### Technical architecture#\n\nLLM models are challenging to run directly via ONNX runtime on CPU. For these,\nwe are using a GPU-accelerated Ollama server under the hood:\n\nWe think this is a great match: the Ollama team have worked hard to ensure\nthat the local development experience is great, and we love development\nenvironments that can be run without internet access (for those who enjoy\nprogramming on planes).\n\nAs a Supabase developer, you don't have to worry about deploying models and\nmanaging GPU instances - simply use a serverless API to get your job done.\n\n### Availability: open source embeddings#\n\nAccess to open-source LLMs is currently invite-only while we manage demand for\nthe GPU instances. Please get in touch if you need early access.\n\n## Extending model support#\n\nWe plan to extend support for more models. Let us know which models you want\nnext. We're looking to support fine-tuned models too!\n\n## Getting started#\n\nCheck out the Supabase docs today to get started with the AI models:\n\n  * Edge Functions: supabase.com/docs/guides/functions\n  * Vectors: https://supabase.com/docs/guides/ai\n  * Semantic search demo: GitHub.\n  * Store and query embeddings in Postgres and use them for Retrieval Augmented Generation (RAG) and Semantic Search.\n\nWeek\n\n15-19 April\n\nDay 1 -Supabase is officially launching into General AvailabilityDay 2\n-Supabase Functions now supports AI models\n\nBuild Stage\n\n01 -PostgreSQL Index Advisor02 -Branching now Publicly Available03 -Oriole\njoins Supabase04 -Supabase on AWS Marketplace05 -Supabase BootstrapOpen Source\nHackathon 2024Community Meetups\n\nShare this article\n\nNext post\n\n#### Supabase Swift\n\n15 April 2024\n\nlaunch-week\n\ndatabase\n\nOn this page\n\n  * Generating Text Embeddings in Edge Functions\n\n    * Integrated pgvector experience\n    * Technical architecture\n    * Availability: open source embeddings\n    * Lower costs\n  * Large Language Models in Supabase Edge Functions\n\n    * Open source inference models\n    * Technical architecture\n    * Availability: open source embeddings\n  * Extending model support\n  * Getting started\n\nShare this article\n\n## Build in a weekend, scale to millions\n\n## Footer\n\nWe protect your data.More on Security\n\n  * SOC2 Type 2 Certified\n  * HIPAA Compliant\n\nTwitter\n\nGitHub\n\nDiscord\n\nYoutube\n\n###### Product\n\n  * Database\n\n  * Auth\n\n  * Functions\n\n  * Realtime\n\n  * Storage\n\n  * Vector\n\n  * Pricing\n\n  * GA Week\n\n###### Resources\n\n  * Support\n\n  * System Status\n\n  * Become a Partner\n\n  * Integrations\n\n  * Experts\n\n  * Brand Assets / Logos\n\n  * Security and Compliance\n\n  * DPA\n\n  * SOC2\n\n  * HIPAA\n\n###### Developers\n\n  * Documentation\n\n  * Changelog\n\n  * Contributing\n\n  * Open Source\n\n  * SupaSquad\n\n  * DevTo\n\n  * RSS\n\n###### Company\n\n  * Blog\n\n  * Customer Stories\n\n  * Careers\n\n  * Company\n\n  * Terms of Service\n\n  * Privacy Policy\n\n  * Acceptable Use Policy\n\n  * Support Policy\n\n  * Service Level Agreement\n\n  * Humans.txt\n\n  * Lawyers.txt\n\n  * Security.txt\n\n\u00a9 Supabase Inc\n\nWe only collect analytics essential to ensuring smooth operation of our\nservices. Learn more\n\nLearn more\n\n", "frontpage": false}
