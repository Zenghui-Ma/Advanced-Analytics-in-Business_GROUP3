{"aid": "40052525", "title": "Is Attention All You Need?", "url": "https://www.mackenziemorehead.com/is-attention-all-you-need/", "domain": "mackenziemorehead.com", "votes": 1, "user": "mhdempsey", "posted_at": "2024-04-16 14:37:05", "comments": 0, "source_title": "Is Attention All You Need?", "source_text": "Is Attention All You Need?\n\nMackenzie Morehead\n\nSign in Subscribe\n\n# Is Attention All You Need?\n\n#### Mackenzie Morehead\n\nApr 16, 2024 \u2022 29 min read\n\nGiven the ease with which Transformers generalize, scale, and their efficiency\non existing hardware, they have become the dominant architecture over the last\n~7 years, achieving SoTA in most applications. That\u2019s more true now than ever\ngiven that most of the researchers and developers are working on them, all\nmajor foundational models are Transformers, and their success carries the\nweight of trillions of dollars of market capitalization.\n\nYet, the technology lock-in effect isn\u2019t insurmountable, and several new\narchitectures have recently emerged designed to outperform full attention\nTransformers primarily at where they inherently struggle: long-context\nlearning / generation and inference speed / cost.\n\nThese approaches include sparsified attention mechanisms, linear RNNs, and\nSSMs. Whereas Transformers seek to store all mappings of the past in memory\nand are thus limited by an ever-growing memory burden, these alternatives seek\nto distill the past and are thus limited by their ability to summarize with\nminimal functional loss and often struggle with recall (since they don\u2019t store\nthe entire input, they can\u2019t recall the original segment).\n\nGiven these complementary skill sets, many of the top alternative\narchitectures combine some form of sparse attention with an SMM or RNN block.\nThis helps them retain the best aspects of both model architectures: the\naccuracy of full attention applied to the local context and the throughput +\nlong context modeling of SSMs/RNNs.\n\nWhile these alternatives have been shown to be of competitive quality with\nfull attention Transformers of similar size, so far no model has been\ndeveloped beyond 200M-14B active parameters. It took over three years for\nChatGPT to scale from the current SSM & RNN model sizes to GPT-4\u2019s 1.76T\nparameters.\n\nCan these alternatives be scaled to such large sizes and remain sufficiently\nmore competitive to shift the ecosystem\u2019s center of gravity away from\nTransformers? Moreover, if they do so in a three year time span, will that\nwork have caught up to GPT-6/7?\n\nI focus on this challenge for architecture supremacy because, while progress\nwill continue to happen in every other direction too, the long context arms\nrace has implications for all kinds of use cases as seen below and pits tiny\nstartups / research labs against Microsoft / Google / OpenAI. Plus, exploring\nthe cutting edge of model architecture should become all the more important as\nwe hit limits in our capacity to simply scale compute and data.\n\nNote that the context length handled by the top attention-based models\n(ChatGPT, Gemini, Claude, etc.) has scaled exponentially over the last couple\nyears and can now process up to 1M long inputs. These firms\u2019 secrecy prohibits\nus from knowing whether they\u2019re just eating the costs for the time being, if\nthey use some form of sparse attention, or if they made other algorithmic\nimprovements elsewhere in the model architecture. To the public\u2019s knowledge,\nonly relatively minor improvements have been made to the Transformer\narchitecture. And, on the final point, the attention portion of the model\naccounts for roughly half of the compute / memory, so improvements made\nelsewhere are equally important. These possibilities are explored at the end\nof this paper.\n\nLong context allows for:\n\n  * New use cases:\n\n    * Summaries of entire books or many documents in parallel\n    * Chatbots that remember your dialogue over time, possibly even ones that start to understand your personality or tendencies or interests\n    * Personal assistants as discussed on BG2 that have knowledge of your email, calendar, tendencies, etc. to be perform as well as a high-quality human EA.\n    * Coding tools that can understand projects\u2019 entire GitHub repositories. This would mean a virtual coder capable of understanding the interdependencies of functions and files. The primary startup in this space, Magic Dev, released a model that can ingest 5M tokens (or ~500K lines of code or ~5K files) and claims to have achieved tens of millions context length internally.\n    * Tools to make sense of inherently long context data. Genetics is a great example of this. The human genome is 6 billion letters and it\u2019s well documented that interdependencies can stretch across millions of base pairs. Hazy Research adapted their Hyena model to process 1M nucleotide token context lengths and achieved SoTA in 23/28 tasks. A Broad Institute researcher adapted a Transformer to be able to generate entire bacteriophage genomes with lengths up to 96kbp. More generally, time series, audio, video, medical imaging data naturally modeled as sequences of millions of steps.\n    * Analysis / generation of higher resolution images, videos, or vide-audio content\n    * Replacing fine-tuning as a means for organizations to make their proprietary data / information useful, as described by this engineer\n  * It also appears that longer context lengths makes a model \u201cdramatically better at predicting the next token in a way that you'd normally associate with huge increments in model scale\u201d\n\nMore efficient models / hardware:\n\n  * Meaningful increases in the economics of inference will also make viable things like autonomous agents such that an increasing share of web traffic are smart bots doing tasks for their deployers\n  * Incremental increases in efficiency will enable capable models to be deployed on PCs and mobile devices, as seen to the right.\n  * Step function increases in efficiency will enable deployment on the edge. This will likely come from highly specific, analog devices. Dozens of chip designers are looking to instill intelligence into edge computing chips, physically programming them to be able to identify objects or noises with low energy. This could lead to highly efficient smart doorbells capable of learning the homeowner\u2019s face without ever needing to relay to the cloud, thus saving not only on compute but also on privacy. Or, it could enable analog chips designed to listen for cues like \u201cHey Siri'' in a highly energy efficient manner, which when triggered would then turn on the more general purpose chips.\n\n    * More generalized intelligence with meaningfully similar efficiency will be important for use cases like truly useful Meta glasses.\n\nDramatically improved speed:\n\n  * Will yield not only a better user experience for things like chatbots and code (productivity will really be unleashed when developers wait 2 seconds for the token generation of their code suggestions instead of 10), but also enable fundamentally new possibilities. This includes chatbots that can talk in a live conversation over voice. It\u2019s also essential for robotics. When you watch videos of the deployments of large models on robotics, they\u2019re always sped up 4x. For these systems to be useful in live environments, they\u2019ll need to be able to make decisions much faster.\n  * The speed of generation may also significantly improve the quality of generated results as it\u2019ll enable generative models to become agentic systems of one model prompting suggestions and the core model revising its output. Just as humans produce better results when allowed to make outlines, drafts, edits and incorporate third party feedback, AI systems have been exhibited similar boosts to quality across use cases. The faster the models\u2019 generation speeds, the more iterations and ideas the agentic system will be able to explore. See Andrew Ng\u2019s posts for more.\n\nNote: red denotes sparse attention, RNN & SSM models. Blue denotes full\nattention.\n\nSection I: Progress in Transformers Towards Subquadratic Attention\n\nSection II: Potentially Transformative Alternative Architectures\n\n  1. Linear RNNs, SSMs and More: alternatives to attention\n  2. Takeaways from Alternatives\n  3. Full database of models\n  4. Overview of Selected Models\n  5. Further Detail on Selected Models\n\nSection III: Architecture-Agnostic Improvements Likely Favor Transformers\n\n  1. Model Improvements\n\n    1. Distributed Computing: moar GPUs pls\u263a\n    2. Faster computing of attention:\n    3. Sparsifying the feedforward side of Transformers: MoE\n  2. Chips Advancements in Many Flavors: More Memory, Speedy Inference, and Interconnects at the Speed of Light\n\n# Section I: Progress in Transformers\n\nTowards Subquadratic Attention: finding ways to compute less\n\n[First, for primers on how Transformers work see the following:\n1,2,3,4,5,6,7,8]\n\nAt the heart of Transformer\u2019s architecture is the mechanism of self-attention,\nin which every token is mapped to every other token. In other words, as the\nmodel processes each additional inputted token, it concurrently processes\nevery other token in the sequence. Attention is thus calculated by multiplying\ntwo large matrices of weights and passing it through a softmax function that\nnormalizes the values. The values in each row and column of the resulting\nmatrix represent correlations between words / their similarity score / how\ntightly they depend on one another. Modeling all possible interdependencies of\nwords yields some great properties: \u201cit can losslessly propagate information\nbetween any two entries in the sequence, regardless of their distance (global\nmemory) and has the ability to extract information from any single element\n(precision).\u201d^1 In other words, it drives the accuracy, expressivity,\nflexibility, and recall capacity of modern large models.\n\nThe flipside of this is that Transformers must store in memory a matrix of\nsize NxN where N is the sequence length. In other words, the models scale\nquadratically in sequence length (due to attention) and model dimension (due\nto MLP layers). Thus, Transformer\u2019s uncanny ability to relate distant ideas\ncomes at the cost of higher compute that increases quadratically. For more\ntechnical detail see this paper. This quadratic memory burden hurts their\nability to model long sequences, speed during inference, and overall compute\nefficiency.\n\nThis inherent limitation of vanilla Transformers has led researchers to search\nfor more efficient attention mechanisms. The intuition of subquadratic\nattention lies in the fact that in language modeling, a subset of words\ncontributes most of the meaning. For example, a vanilla transformer would\nattend each word of the sentence, \u201cI want to order a hamburger,\u201d equally\ndespite the words \u201cI\u201d, \u201corder,\u201d \u201chamburger\u201d driving almost all the meaning.\nThough this is of course a simplification, it illustrates the idea.\n\nMoreover, the pareto principle in language modeling illustrated by the example\nabove is true as you zoom out from within-sentence mappings to long-range\nones. The word \u201corder\u201d from this sentence has little to no direct connection\nto the meaning of a given word from a paragraph several pages prior. I.e.\nthere\u2019s locality to meaning. A word contributes more meaning to its neighbors\nthan to those far, far away.\n\nIndeed, this intuition is embedded in the computation due to the exponential\nnature of the softmax, which results in almost all values becoming near zero\nand only a few positions with high values. Moreover, several papers suggest\nthat attention mechanisms only utilize a subset of their quadratic\ncapabilities for language processing^1^,^1.\n\nOne way to try to exploit the concentration of meaning in language is to make\nthe attention matrix similarly sparse. In other words, to only allow each\nposition to attend to a subset of the positions. Here are a very high level\ncategorization of the approaches: (i) fixed and random, (ii) learned and\nadaptive, and (iii) identified with clustering and locality sensitive hashing.\nFor more detailed breakdowns of the various approaches see the excerpt below:\n\nIf you want to go deeper, read these literature reviews: (1,1,2).\n\nThe sparse approaches can be visualized below:\n\nDespite this huge diversity of progress and much more unmentioned, the only\nmajor model to publicly acknowledge using subquadratic or approximate\nattention mechanisms is Mistral\u2019s use of sliding window attention. Lukasz\nKaiser (author of Attention is All You Need paper, some of the key sparse\nattention papers, and now scientist at OpenAI) recently patented a sparse\nattention mechanism. But, the fact that those are the most provocative\nannouncements I could find points to how little it\u2019s been deployed in top\nmodels compared to other conditional computing methods like sparse MoEs and\nquantization. As a top Google researcher said, \u201cthere's a graveyard of ideas\naround attention.\u201d\n\n# Section II: Potentially Transformative Alternative Architectures\n\n#### Linear RNNs, SSMs, and More: alternatives to attention\n\nBoth RNNs and SSMs have decades long histories, but only in the last several\nyears, sufficient architectural tweaks have produced credible alternatives to\nTransformers. In fact, there's a bet going on between the chief scientist at\nMosaicML and a research scientist at Hugging Face on whether or not a\nTransformer-based model will still hold the SoTA in 2027.\n\nRNNs model a hidden state at each time step, updating it as each new input\ndata is encountered. The state size is typically fixed and stored as a single\nvector. So, as time goes on, the model aims to distill and summarize the data\nwith minimal loss and maximal signal. Such internal state can capture the\nlong-term behavior of a system over time reasonably well without having to\nstore the entire input in working memory.\n\nHowever, they historically haven\u2019t scaled well because of issues like the\nvanishing gradient problem and training issues due to their sequential, non-\nparallelizable time dimension. They\u2019ve also historically struggled on\ninformation dense data like language because of their tendency to overweight\nthe recent past.\n\nAs illustrated by the diagram below, they work by applying a function to input\nu to transform into an intermediate hidden state x before outputting y. The\nhidden state is updated like this step by step. This makes for slower training\nspeed with traditional RNNs because you need to wait to compute the next\nhidden state before computing the value but faster inference because you can\njust go from the last hidden state whereas attention requires full look back\nacross all inputs for generation.\n\nIn summary, whereas Transformers seek to store all mappings of the past in\nmemory and are thus limited by an ever-growing memory burden, RNNs seek to\ndistill the past into a fixed state size and are thus limited by their ability\nto do so with minimal functional loss. Said differently, if you keep\neverything in memory, you get high accuracy and recall with low speed, and\nvice versa for methods that prune memory.\n\nAn explosion of alternatives to attention & Transformers over the last four\nyears has sought to retain the fixed state space of RNNs and their quick\ninference while making their training parallelizable and their state more\nexpressive. In other words, they aim to expand the Pareto frontier of the\nimpossible triangle as seen to the right or at least make reasonable speed vs\naccuracy tradeoffs to address unmet market needs.\n\nThese models\u2019 reduced memory footprint combined with any fundamental\narchitectural advancements that expand the Pareto frontier yields them several\nto many times faster inference. For example, Mamba has 5x higher throughput\nthan a Transformer.\n\nLinear RNNs (including their variant, SSMs) model the hidden state at each\nstep as a linear combination of the previous hidden state and the current\ninput. This, upon some nice mathematical distillation, makes it possible to\nrun a 1D convolution of the input data either via either a Fourier transform\nor an associative scan. This circumvents the fatal flaw of prior RNN\nimplementations \u2013 their serial nature, which made training impossibly slow\nrelative to Transformers \u2013 and instead makes the models parallelizable.\n\nThe latest SSMs are a clever parameterization of linear RNNs that combine\nideas from CNNs, RNNs, and attention depending on the exact model. As Albert\nGu described:\n\nThe idea is that instead of going from the input to the state to the output\nyou can go straight from the input to the output, bypassing the state and\ndoing the entire computation over the sequence length in parallel. SSMs turn\nout to be equivalent to convolutions because computing the map from the input\nu to the output y is equivalent to convolving the input by a particular\nconvolution filter. So, to compute this map you just do: y equals u convolved\nwith k for this convolutional kernel.\n\nHere's a more technically involved definition:\n\nStructural SSMs achieve such impressive performance by using three main\nmechanisms: 1) High-order polynomial projection operators (HiPPO) (Gu et al.,\n2020a) that are applied to state and input transition matrices to memorize\nsignals\u2019 history, 2) diagonal plus low-rank parametrization of the obtained\nHiPPO (Gu et al., 2022a), and 3) an efficient (convolution) kernel computation\nof an SSM\u2019s transition matrices in the frequency domain, transformed back in\ntime via an inverse Fourier transformation (Gu et al., 2022a).\n\nLike SSMs scale with O(NlogN) in sequence length, instead of O(N^2) for\nattention. They of course have their drawbacks, which often include the\nfollowing:\n\nSSMs are a bit complex - to train in a modern deep learning stack, they rely\non sophisticated mathematical structures to generate a convolution kernel as\nlong as the input sequence. This process can be unstable, and requires careful\ninitialization for good quality. (from Hazy Research)\n\nCertain implementations of the architecture can also run into other issues\nlike Fourier transforms not being supported on TPUs. Finally, many variants\nhave struggled with associative recall and retrieval.\n\n### Key takeaways from alternative models:\n\nFor models relying on fixed state spaces, input-dependent gating / dynamically\nselecting what input data to remember seems crucial to preserving only the\nmost important info. This contrasts to static mechanisms that remember data\nbased solely on its position in the matrix. Indeed, one of the Mamba authors\nstated that \u201cIt seems that data-dependent gating is the core ingredient for\neffective linear-complexity alternatives to softmax attention, as shown in\nboth our GLA and Mamba.\u201d\n\nAnother key architectural design takeaway is that combinations of\ncomplementary mechanisms appear most effective. As one of the most prominent\nresearchers in the SSM field said: \u201cone principle that was validated over and\nover again (and we're trying to understand better now) is that it seems\ncomposing hybridizing different layers with blocks from different categories\nis better than the individual components.\u201d Indeed, many of the top models\ncombine some form of sparse attention with an SMM or RNN block. This may\nretain the best aspects of both models: the accuracy of dense attention\napplied to localized context with the throughput and long context modeling of\nSSMs/RNNs.\n\nThese new alternative architectures join the sparse attention efforts on the\nmarch from quadratic compute to increasingly linear compute, albeit from a\ndifferent starting point. Indeed, all these different mechanisms and\napproaches often overlap conceptually and practically as they converge on the\ncommon end goal: getting rid of unnecessary data and computation. As time\nmoves on and it becomes more clear which approaches are most consistently and\nuniversally promising, the concentric circles may get ever tighter.\n\nFor some specific examples of how these mechanisms are not just conceptually\nsimilar but can even be reformulated as effectively the same thing, see this\nword game from Sasha Rush: \u201cAttention and RNNs are distinct, but Linear\nAttention is a Linear RNN, but not vice versa. Luckily both are also\nConvolutions.... but not with data dependent parameters.\u201d Additionally, here\u2019s\nthe paper demonstrating linear attention can be viewed as a linear recurrence.\nFinally, a recent paper showed that Mamba blocks can be reformulated as an\nimplicit form of causal self-attention.\n\nWhile these alternatives have been shown to be of competitive quality with\nfull attention Transformers of similar size, so far no model has been\ndeveloped beyond 200M-14B active parameters. It took over three years for\nChatGPT to scale from the current SSM & RNN model sizes to GPT-4\u2019s 1.76T\nparameters. Can these alternatives be scaled to such large sizes and remain\nsufficiently more competitive to shift the ecosystem\u2019s center of gravity away\nfrom Transformers? Moreover, if they do so in a three year time span, will\nthat work have caught up to GPT-6/7?\n\nIf these models truly offer legitimate promise to be competitive accuracy-wise\nwith far higher throughput / efficiency, then why hasn\u2019t one of the groups\nsecured $10-100M in investment to scale them up and push them to their limits?\nWhy haven\u2019t they been tested beyond 14B parameters? One possible explanation\nis that Transformers have been a relatively static architecture since their\nconception 7 years ago, with only minor subcomponent tweaks having taken\nplace. Much of the ecosystem developer and financial resources have been\ndevoted to scaling and deploying the architecture. Whereas, within the\nCambrian explosion of alternative architectures, on a monthly basis entirely\nnew architectures are invented with some achieving SoTA. Until the ecosystem\nsaturates the explorable universe and a clear champion arises, companies with\nthe resources to fund the training of a massive model have an incentive to\nwait for that day.\n\nFinally, as to where this research comes from, the vast majority of the high\nimpact sparse attention papers originate from Google and the SSM / RNN papers\nfrom Hazy Research. These pockets of innovation in architectural design follow\na trend in AI research where roughly every 5 years a new idea that\u2019s\nsufficiently novel and powerful is introduced, and a transient burst of\nexperimentation ignites. RNNs were followed by variants like gated RNNs (e.g.\nLSTM), attention by sparse attention mechanisms, and now S4 by a slew of\nlinear RNN and SSM variants. Eventually, the universe of possibilities is\nexplored and the experimentation subdues, giving way to the similarly valuable\nbut incremental work of refinement, scaling, and application \u2013 until another\nfundamentally new idea is broached.\n\n### Overview of Selected Models\n\n[Note that I focus on language models, but non-attention architectures are\nrelevant for other types of models (ex. a group made an SSM-based diffusion\nmodel) as well as other data domains (see the \u201cDerivative Models\u201d column of\nthe Excel file).]\n\nMamba: improved upon S4 by introducing selectivity / input dependence via a\ngate without reverting to RNNs\u2019 sequential nature, making the model more\nexpressive\n\nStripedHyena: exemplifies how hybrid models may be optimal, as it combines\nconvolutions attention and gated convolutions\n\n  * \u201cEarly in our scaling experiments, we noticed a consistent trend: given a compute budget, architectures built out of mixtures of different key layers always outperform homogenous architectures. These observations echo findings described in various papers: H3, MEGA (among others) and find even earlier connections to the hybrid global-local attention design of GPT-3 (from Sparse Transformers) and GPTNeo.\u201d\n\nBased: improved upon Mamba by addressing the serious limitations in recall of\nit and other SSMs by combining short convolutions and linear attention\n\n  * Efficient architectures (e.g. Mamba) are much faster than Transformers at inference time (e.g. 5x higher throughput) in large part because they have a reduced memory footprint. Smaller memory footprint means larger batch sizes and less I/O. However, it also makes intuitive sense that reducing memory footprint too much could hurt a model\u2019s capacity to recall information seen earlier in the sequence. (Together AI)\n  * Found that gated-convolution models\u2019 poor performance on associative recall tasks account for >82% of the remaining quality gap as measured by perplexity to attention on average. A 70M attention model outperforms 1.4Bn Hyena on AR tokens.\n\nGriffin / Hawk: hybrid combination of gated linear recurrences similar to\nMamba\u2019s blocks and local attention. Like Based, it\u2019s capable of efficiently\ncopying and retrieving data over long horizons. The authors also reinforce the\nimportance of gated recurrences. Finally, the paper demonstrated that Google\nis actively exploring alternatives to attention.\n\nJamba: another hybrid model combining Mamba blocks with attention that\u2019s\nscaled up to 52B total parameter MoE model with 12B active parameters. It\nachieves similar quality to Mistral but with 3x better throughput. It can fit\non a single 80GB GPU and is accessible via API.\n\nMonarch Matrix: first model to replace both attention and MLPs that matches\nequivalently sized Transformers (360M parameters)\n\n### Further Detail on Selected Models\n\nMamba:\n\nS4 precomputes the convolutional kernel. Mamba introduced selectivity (like\nsparse attention) such that the matrices change depending on the input.\nMamba\u2019s gate allows it to reset its state and forget any information it holds\nfrom the past. However, dependence on the input requires training it like an\nRNN where it must be updated sequentially for each token. So, Mamba's second\nmajor idea involves training in RNN mode very quickly. At some point, Gu and\nDao realized that their recurrence was very similar to a scan algorithm, also\nknown as a prefix sum. As a result, we can compute this prefix sum (or scan)\noperation in roughly O(logN) time. Moreover, as Dao was the lead author of\nFlashAttention, he came up with the clever hardware-aware implantation in\nwhich it stores the latent state in SRAM, the most efficient part of memory.\n\nThe author of LSTM echoed the importance of Mamba\u2019s gates: \u201cI always felt that\nS4-related nets didn\u2019t work well because they were missing the two core LSTM\ningredients: fast weight gates + cell skip connections. This seems to bring\nboth of those back.\u201d One of the Mamba authors stated that \u201cit seems that data-\ndependent gating is the core ingredient for effective linear-complexity\nalternatives to softmax attention, as shown in both our GLA and Mamba.\u201d\n\nSee Jack Cook\u2019s blog for more\n\nStripedHyena:\n\n  * First alternative model competitive with the best open-source Transformers (Llama-2, Yi and Mistral 7B) in both short and long-context evaluations on OpenLLM leaderboard. It outperforms on long-context summarization.\n  * Designed using our latest research on scaling laws of efficient architectures. In particular, StripedHyena is a hybrid of attention and gated convolutions arranged in Hyena operators. Via a compute-optimal scaling protocol, we identify several ways to improve on baseline scaling laws for Transformers (Chinchilla) at the architecture level, such as hybridization. With these techniques, we are able to obtain higher quality models than Transformers at each training compute budget, with additional benefits at inference time.\n  * Optimized using a set of new model grafting techniques, enabling us to change the model architecture during training. StripedHyena was obtained by grafting architectural components of Transformers and Hyena, and trained on a mix of the RedPajama dataset, augmented with longer-context data.\n  * Using our latest research on fast kernels for gated convolutions (FlashFFTConv) and on efficient Hyena inference, StripedHyena is >30%, >50%, and >100% faster in end-to-end training on sequences of length 32k, 64k and 128k respectively, compared to an optimized Transformer baseline using FlashAttention v2 and custom kernels. StripedHyena caches for autoregressive generation are >50% smaller than an equivalently-sized Transformer using grouped-query attention.\n\nFrom <https://www.together.ai/blog/stripedhyena-7b>\n\nBased:\n\nBased is a simple architecture that combines two familiar sub-quadratic\noperators: short convolutions and linear attention. These operators have\ncomplementary strengths and together enable high-quality language modeling\nwith strong associative recall capabilities. At inference time, because Based\nonly uses fixed-sized convolutions and linear attentions (computable as\nrecurrences), we can decode with no KV-cache. This enables a 4.5x throughput\nimprovement over Transformers with Flash Attention 2.\n\nWe demonstrate these properties along three axes, finding that Based provides:\n\n  1. We motivate Based with the view that simple convolutions and attentions are good at modeling different kinds of sequences. Instead of introducing new complexities to overcome their individual weaknesses, we can combine familiar versions of each: standard convolutions are great for modeling local dependencies and settings where we might not expect to need associative recall (think building up morphemes [i.e., units of meaning] from individual tokens, similar to how in vision we build up higher-level features from neighboring pixels). Meanwhile, a spiky linear attention calculated as a Taylor approximation of softmax enable Based to do associative recall, e.g., by recovering the global \u201clook-up\u201d inductive bias of standard attention ^1. As we\u2019ll see, the combination of both enables Based to achieve high quality sequence modeling while remaining *fully sub-quadratic*.\n  2. High quality modeling: despite its simplicity, in our evaluations we find Based outperforms full Llama-2 style Transformers (rotary embeddings, SwiGLU MLPs, etc.) and modern state-space models (Mamba, Hyena) in language modeling perplexity at multiple scales. (Section 2. Based outperforms Transformers in language model perplexity.)\n  3. Efficient high-throughput inference: when implemented in pure PyTorch, Based achieves 4.5x higher inference throughput than competitive Transformers, e.g., a parameter-matched Mistral with its sliding window attention and FlashAttention 2. High throughput is critical for enabling batch processing tasks with LLMs. (Section 3. Based is fast.)\n\nWe found that there is a small perplexity gap between recently proposed sub-\nquadratic gated-convolution architectures and Transformers, when training on\nfixed data (10B tokens of the Pile) and infrastructure (EleutherAI GPT-NeoX).\nHowever, after performing a fine-grained error analysis, we see there remains\na significant gap on next-token predictions that require the model to deploy a\nskill called associative recall (AR).\n\nWhat\u2019s AR? Consider the sequence: \u201cShe put vanilla extract in her strawberry\nsmoothie ... then she drank her strawberry?\u201d \u2013 the model needs to be able to\nlook back at the prior context and recall that the next word should be\n\u201csmoothie\u201d. We find that the gated-convolution models\u2019 poor performance on\nthese sorts of token predictions account for >82% of the remaining quality gap\nto attention on average! A 70M attention model outperforms 1.4Bn Hyena on AR\ntokens.\n\nEfficiency. We also note that in contrast to prior work (H3, Hyena, Striped\nHyena, Multi-Head Hyena, M2, BIGS, etc.), the block does not use convolutions\nwhere the filter is as long as the input sequence. The use of short\nconvolutions plus linear attention permits parallel training and recurrent\ninference, without requiring any further modifications like distillation.\n\nBecause linear attention can be viewed as a recurrence and short convolutions\nonly require computing over the last filter-size terms during generation,\nBased\u2019s hidden states only require constant memory; no KV-cache or growing\nwith generated sequence length!\n\nFrom <https://hazyresearch.stanford.edu/blog/2023-12-11-zoology2-based>\n\nGriffin / Hawk:\n\nWe propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid\nmodel that mixes gated linear recurrences similar to Mamba\u2019s block with local\nattention. We have found this fixed state size combination extremely\neffective, since local attention accurately models the recent past, while the\nrecurrent layers can transmit information across long sequences. Hawk exceeds\nthe reported performance of Mamba on downstream tasks, while Griffin matches\nthe performance of Llama-2 despite being trained on over 6 times fewer tokens.\nWe also show that Griffin can extrapolate on sequences significantly longer\nthan those seen during training and are capable of efficiently copying and\nretrieving data over long horizons.\n\nGate behavior:\n\n  * The input gate i_t is similar to the one in LSTM, which can filter (or scale down) the input x_t. However, to our knowledge, our recurrence gate rt is different from other gating mechanisms in the literature. For example, the selection mechanism proposed in Mamba (Gu and Dao, 2023) is comparable to the update gate of GRUs which interpolates between the previous state and the current observation x_t. Its effect on the hidden state allows it to reset its state and forget any information it holds from the past, similar to the forget gate in the LSTM. In contrast, our recurrence gate can approximately interpolate between the standard LRU update from Orvieto et al. (2023a) and the previous hidden state, which allows it to effectively discard the input and preserve all information from the previous history (see Appendix A for further details). We believe the key role of this gate is to enable the model to achieve super-exponential memory by reducing the influence of uninformative inputs.\n\nFrom <https://arxiv.org/pdf/2402.19427.pdf>\n\nMonarch Mixer:\n\n  * M2 matches GPT-style Transformers at 360M parameters in pretraining perplexity on the PILE--showing for the first time that it may be possible to match Transformer quality without attention or MLPs.\n  * We replace attention by using Monarch matrices to construct a gated long convolution layer, similar to work like H3, Hyena, GSS, and BiGS. Specifically, Monarch matrices can implement the FFT, which can be used to compute a long convolution efficiently: M2 uses Monarch matrices to implement a gated long convolution, by implementing the convolution using FFT operations.\n  * We replace the MLPs by replacing the dense matrices in the MLP with block diagonal matrices: M2 replaces dense matrices in an MLP with block-diagonal matrices.\n  * Incidentally, this makes the model look similar to MoE models, without learned routing.\n\nFrom <https://www.together.ai/blog/long-context-retrieval-models-with-monarch-\nmixer>\n\nJamba:\n\n  * Joint Attention and Mamba (Jamba) architecture. Each Jamba block contains either an attention or a Mamba layer, followed by an MLP, producing an overall ratio of one Transformer layer out of every eight total layers. This ratio was optimized during research. Jamba uses a total of 4 attention layers.\n  * 52B total parameter MoE model with 12B active parameters\n  * Created by enterprise model developer AI21 Labs, it\u2019s a production-grade model that\u2019s currently accessible for use via Nvidia API and Hugging Face\n  * The only model in its size class that fits up to 140K context on a single 80GB GPU\n  * 3X throughput on long contexts compared to Mixtral 8x7B\n  * Reports good results on needle-in-a-haystack, retrieval, and in-context learning evaluations \u2013 the three of which are often downsides to pure SSM/linear RNN models.\n\nFrom <https://arxiv.org/pdf/2403.19887.pdf>\n\n# Section III: Architecture-Agnostic Improvements Likely Favor Transformers\n\n### Model Improvements\n\nThe factors below could lighten the relative disadvantages of attention /\nTransformers in speed, efficiency, and throughput to maintain or extend their\nedge as the universal architecture. More specifically, it\u2019s possible there\u2019s a\nminimum threshold of context necessary to sustain dominance as a general\npurpose foundation model architecture \u2013 say, 1M tokens (~10 books or full\nGitHub repositories). Beyond that, only specific tasks will require anything\nmore extreme (ex. personal assistants or full genome DNA analysis). With the\ncomputational and algorithmic approaches below (as well as possibly some of\nthe sparsified mechanisms above), major Transformer models in the wild have\nalready reportedly achieved that threshold (ex. Google\u2019s Gemini and Anthopic\u2019s\nClaude 3 both have a context length of 1M). Any further architecture-agnostic\nimprovements will continue to disproportionately improve Transformer\u2019s\nrelative position.\n\na) Distributed Computing: moar GPUs pls\u263a\n\nThis method aims to circumvent the limited memory of individual GPUs/TPUs. The\ninput sequence is split up and distributed across multiple devices. Ring\nattention, which gained notoriety after rumors that Google\u2019s latest Gemini\nmodels with up to 1-10M context length relies on it, rearranges the\ncomputation in a metaphorical ring, with each GPU / TPU device processing its\nassigned segment and sharing only crucial information (key-value pairs) with\nthe next device in the ring to compute attention and feedforward. This enables\nattention computation across the full sequence without requiring the entire\nmemory to be present on a single device. Moreover, it enables sequence\ntraining and inference size to scale linearly with the number of GPU count\nwithout making approximations to attention or additional / overhead\ncomputation. The original UC Berkeley research team exceeded a context length\nof 100M (the number depends on the model size and GPUs available). In tests,\nthe 13B parameter model yielded competitive accuracy to Claude 2 & GPT3.5. The\ngroup used it to build a 7B parameter model capable of analyzing and\ngenerating video, image, and language with a 1M sequence length that\noutperforms GPT-4V and Gemini Pro at 1hr+ long video analysis. Another group\nsince published a paper purportedly making improvements to the ring attention\narchitecture.\n\nMeanwhile, a group recently used a distributed computing technique to speed up\nthe inference of high resolution images and video from diffusion models by 6x\non eight A100s with no quality degradation.\n\nb) Faster computing of attention:\n\nFlashAttention is fast and memory-efficient algorithm that computes the exact\nattention. FlashAttention is 2-4x faster than standard attention. It achieves\nthis enormous increase in compute efficiency by restructuring how GPUs compute\nattention such that the bottleneck (memory not FLOPs) is addressed by\nminimizing reads and writes to HBM.\n\nFor more detail read the following:\n\nIt uses two main techniques: tiling and recomputation. Tiling happens in\nforward pass and it involves splitting large matrices in attention(K key and V\nvalue) into blocks. Rather than computing attention over entire matrices,\nFlashAttention computes it over blocks and concatenate the resulting blocks\nsaving a huge amount of memory. Recomputation happens in backward pass and it\nbasically means recomputing the attention matrix rather than storing it in\nforward. The idea of FlashAttention boils down to improving the memory and not\ndecreasing computations because modern GPUs have high theorical FLOPs (which\nmeans you want to max that out) but limited memory^12 (which means any saving\nin memory can improve the training speed). HBM is typically large but it is\nnot faster than on-chip SRAM and thus, the computations over blocks (of K and\nV) happens in SRAM (because it is faster) but all full matrices are stored in\nHBM (because it\u2019s big). This high-level explanation is probably an over-\nsimplication provided that FlashAttention is implemented at the GPU level\n(with CUDA software) and this is in fact the reason why it is IO aware but\nhopefully that explains what\u2019s going on in this fast algorithm.\n\nIdeally, we would want the bulk of computations to be taken by matrix\nmultiplication(matmul) operations but surprisingly, dropout, softmax, and mask\n(i.e, GPT-2 is decoder model) end up taking the whole runtime in GPT-2\nattention because they are computed over full matrices. Matmuls take less\nruntime than those other operations because GPUs are exactly designed to be\nfast at matrix multiplications(they have really high theorical FLOPs and\nmaximizing FLOPs usage doesn\u2019t reduce the runtime). By using tiling and\nrecomputation techniques, the compute time of FlashAttention is significantly\nlow compared to standard attention as you can see below. See Tri Dao\u2019s video\nfor more.\n\nFlashAttention-2 improves upon FlashAttention-1 by parallelizing over sequence\nlength dimension instead of batch size and number of attention heads and\nsplits Q(query) matrix instead of K and V. This release blog post explains\nwell what FlashAttention2 brings to the tensor table.\n\nOther methods of speeding up attention include quantization, speculative\ndecoding, etc. An exciting recent Google paper thoughtfully combines multiple\nsuch methods to purportedly achieve \u201cnear-lossless 4-bit KV cache compression\nwith up to 2.38x throughput improvement, while reducing peak-memory size up to\n2.29x.\u201d\n\nc) Sparsifying the feedforward side of Transformers: MoE\n\nNote that most of the top proprietary models used sparsified MoE protocols\n(Mistral, Gemini, etc.)\n\nThe compute costs of the self-attention mechanism contributes partially to the\noverall compute cost of the Transformer. A non-trivial amount of compute still\nstems from the two layer feed-forward layers at every Transformer block\n(approximately half the compute time and/or FLOPs). The complexity of the FFN\nis linear with respect to sequence length but is generally still costly.\nHence, a large portion of recent work have explored sparsity (Lepikhin et al.,\n2020; Fedus et al., 2021) as a means to scale up the FFN without incurring\ncompute costs.\n\nDue to their computational demands, feed-forward layers in Transformers have\nbecome the standard target of various MoE techniques (Lepikhin et al., 2020;\nFedus et al., 2022; Du et al., 2022; Zoph et al., 2022). Scaling properties\nand generalization abilities of MoE Transformers have been studied more\nclosely by Artetxe et al. (2021); Clark et al. (2022); Krajewski et al.\n(2024).\n\nAgain, this approach could and has been used for RNNs/SSMs but I\u2019d assume it'd\ndipropionate improve Transformer\u2019s relative position.\n\nFor more detail on other methods to make models more efficient see these\nreviews: (1,1,2)\n\n### Chips Advancements in Many Flavors: More Memory, Speedy Inference, and\nInterconnects at the Speed of Light\n\nI\u2019d expect most of the inevitable advancements in chips to similarly play in\nTransformer\u2019s favor for most applications.\n\n  1. The latest competitor to NVIDIA to generate buzz, Groq focuses on making AI chips that offer super-fast token generation. See my write-up on the company and their approach. Their video demonstrations at speeds of 480 tokens / sec are 3-4x standards. They\u2019ve even since reported 800 tokens / sec. Groq\u2019s deterministic hardware design in which the sequence of operations must be pre-defined advantages any model inherently expressed that way, namely Transformers. It would presumably not work for any data dependent or time variant model architectures. Plenty of other companies are trying to make AI ASICs that accelerate performance well beyond Jensen\u2019s master plans. One\u2019s even trying to \u201cEtch\u201d the Transformer architecture into silicon. These are speculative ventures unlikely to prevail over NVIDIA given they use the same fundamental materials and approaches as Nvidia, in addition to how excellent and reasonably well specialized Nvidia\u2019s products already are and their newly announced one-year product cycle. Either way, the speed of AI accelerator inference will improve whether it be incrementally or in a step function. That should favor Transformers.\n  2. Photonic computing has taken major strides in recent years towards implementation in real chips. Several billion dollars have been raised by the several dozen photonics startups, with top ones like Lightmatter and Ayar inking deals with the top chip designers and fabs. In a recent interview I conducted, a Lightmatter engineer seconded my suspicion that optical interconnects are 3-5 years away from being integrated into chips. Enabling GPUs to communicate with one another at the speed of light melds their individual memories into one collective memory pool, helping evolve the unit of compute from individual GPUs to racks. This should make ring attention and other distributed computing approaches meaningfully more effective.\n  3. The one I\u2019m unsure about is how the near doubling of GPU memory size every two years will affect the architecture competition. It may favor fixed state models because you can store a larger state: i.e. you don\u2019t have to distill so much of the information. Easing this bottleneck for those models would help with both accuracy and retrieval tasks, while only effecting speed in an incremental way.\n\n[The H200 is expected to nearly double the memory available to 141 GB and to\nrelease in Q2.\n\nIn summary, the dynamics identified in Section III suggest that the longer\nthat Transformer alternatives take to decisively outperform, the less likely\nit will ever happen. This isn\u2019t to mention developer and enterprise lock-in.\n\n# Conclusion\n\nFor the first time since their invention, Transformers have real competition.\nSeveral models in the 1-14B active parameter range have achieved similar\nquality scores as SoTA Transformers (e.g. Mistral) with far longer context and\n/ or higher throughput.\n\nThe rate of iteration and progress in this subfield seems as rapid as any in\nartificial intelligence. Researchers are devising entirely new compute\nmechanisms like the Mamba blocks and Monarch Mixers; iteratively improving,\ngeneralizing, and distilling those mechanisms; and then trying all possible\npermutations of mechanism combinations.\n\nOn a near monthly basis, a new architecture is released that pushes the field\nor at least contributes meaningfully to our understanding of what makes a good\narchitecture. Such high level findings include that hybrid models combining\ncomplementary mechanisms appear superior and that data dependent input gating\nis important to effectively distilling the past to a limited state size.\n\nThis competition for architectural supremacy is part of an inevitable march\ntowards more efficient AI models. The room for far greater efficiency is a\nnatural reflection of the fact that meaning is highly concentrated in a small\nproportion of words and that the same amount of compute shouldn\u2019t be applied\nto predict every token or to solve every problem. Over time, efforts like\nthose described above will find ways to compress information more lossessly\nand apply compute more intelligently.\n\nIt\u2019s impossible to know whether a sparse attention mechanism, an SSM, a linear\nRNN, an approach yet to be invented or some combination therein will reign\nsupreme in 3+ years \u2013 especially until each is demonstrated at scale. But I\nhave confidence that SoTA models won\u2019t use vanilla full self-attention\nforever.\n\nThe Pareto frontier will continue to expand such that longer context modeling\nand / or greater inference throughput and cost will be possible without\nsignificant quality losses. As that frontier expands, so do the number of use\ncases for AI.\n\n## Sign up for more like this.\n\nEnter your email\n\nSubscribe\n\n## Textiles, Beauty Products, & Home Goods (The Century of Biology: Part\nII.VI)\n\nA parallel evolution to that changing how the products listed above are made\nis happening with consumer products. These efforts are also being made to\nreduce emissions and transition away from oil- and animal-derived products.\nAfter all, our current faux leathers called \u201cpleather\u201d are made with plastics.\nAlongside these reasons,\n\nNov 21, 2023 6 min read\n\n## Circular Plastics: Designing Our Tupperware as Mother Nature Would (The\nCentury of Biology: Part II.V)\n\nSince the invention of plastics a century ago, they\u2019ve become absolutely\nubiquitous \u2013 you interact with them hundreds of times every day in every\ncomponent of your life. Why? Well, they\u2019re among the greatest products ever\ninvented. They\u2019re dirt cheap, perfectly uniform, lightweight, can be flexible\nand / or\n\nNov 21, 2023 13 min read\n\n## Sustainable Fuels & Chemicals (The Century of Biology: Part II.IV)\n\nThe nearly $10 trillion chemicals and fuels industries dig up hydrocarbons and\nthen turn them into nearly every physical good we use in our day to day lives\nfrom plastic to laundry detergent to running shoes to fertilizers. Efforts to\nde-carbonize these industries have attracted tens of billions of dollars\n\nNov 21, 2023 4 min read\n\nMackenzie Morehead \u00a9 2024\n\nPowered by Ghost\n\n", "frontpage": false}
