{"aid": "40052829", "title": "Need for Proactiveness in Data Security", "url": "https://www.jayeshs.com/p/proactive-data-security", "domain": "jayeshs.com", "votes": 1, "user": "JayeshSidhwani", "posted_at": "2024-04-16 14:59:11", "comments": 0, "source_title": "Proactive Data Security", "source_text": "Proactive Data Security - by Jayesh Sidhwani - @jayesh\n\n# @jayesh\n\nShare this post\n\n#### Proactive Data Security\n\nwww.jayeshs.com\n\n#### Discover more from @jayesh\n\nMy thoughts on startups, technology, and anything that interests me :)\n\nContinue reading\n\nSign in\n\n# Proactive Data Security\n\n### While data breaches are not new, something significantly changed in 2020.\n\nJayesh Sidhwani\n\nApr 16, 2024\n\nShare this post\n\n#### Proactive Data Security\n\nwww.jayeshs.com\n\nShare\n\nThe number of breaches increased significantly post 2020, primarily because\nbusinesses rapidly adopted digital solutions in response to COVID-19 but\nlacked cybersecurity preparedness.\n\nNumber of Data Breaches in the US (source)\n\nIn 2023 alone, data breaches affected 353 million individuals. That's a\nstaggering 6.6% of the world's internet population. It is not unknown that the\nimpact on individuals doesn't stop here; consumers continue to face numerous\nfinancial scams, identity thefts, and more (checkout this interactive graphics\nfor more details)\n\nThanks for reading @jayesh! Subscribe for free to receive new posts and\nsupport my work.\n\nUnsurprisingly, the trend of increased breaches positively correlates with the\nspending on cybersecurity initiatives. In 2024, companies will spend up to\n$215 billion on upgrading their cybersecurity posture.\n\n##\n\nWhat are companies spending on?\n\nWhen it comes to cybersecurity, there are broadly two implementation\napproaches: monitoring tools (which are reactive) and security-by-design tools\n(which are proactive). While each alone is insufficient, I want to highlight\nthe growing need to focus on implementing security-by-design proactive\napproaches.\n\nThe article is mainly for practitioners and leaders in the Software\nEngineering, Infrastructure, and Security teams. Given the breadth of areas to\ncover, I do not intend to analyze each tool or capability in-depth \u2013 maybe for\na follow-up blog.\n\n##\n\nOver-Reliance on monitoring tools\n\nThe premise of monitoring tools as the be-all-tool is massively oversold.\nCompanies prefer buying or building monitoring tools over solving this problem\nat the core.\n\nThere are a few reasons why:\n\nOne is the incentive misalignment between the Security and Engineering teams.\nThe security team must prevent security breaches at all costs, whereas the\nengineering team must release new features to production as fast as possible.\n\n> The CAP theorem equivalent for engineering productivity is f(Quality, Speed,\n> Amount of work); you can only choose two.\n\nWhat makes matters worse is that engineers do not empathize enough with their\nsecurity counterparts in most organizations. Security members are treated\nalmost as villains who get sadistic pleasure by blocking their features on\nrelease day!\n\nOn the other hand, given the criticality of preventing any breach, security\nfolks have historically enjoyed very high budgets to do what it takes to build\na substantial war chest against attacks.\n\nThus, monitoring tools become the perfect drug. You achieve the desired\nsecurity visibility without using engineering cycles. And if you have the\nbudget for it, it's a match made in heaven.\n\n##\n\nMonitoring is a lagging Indicator\n\nThe biggest problem with relying on monitoring tools alone is that these tools\nare a lagging indicator of a problem. These tools usually operate on aggregate\ndata to detect a statistically significant anomaly. Lagging indicators that\noperate on aggregated data are risky to rely on.\n\nI will take a detour and explain my real experience with a lagging aggregate\nindicator. This happened in a domain other than security, so please bear with\nme.\n\n> In one of my previous project, we developed a data platform feature that\n> tracked daily, weekly, and monthly active users. The challenge was that many\n> users were anonymous, requiring us to use fingerprinting techniques for\n> identification. These methods were unreliable five to seven years ago,\n> especially on web browsers.\n>\n> Our analysis and reporting relied heavily on these user counts. At one\n> point, a bug in our fingerprinting code, affecting a popular browser, led to\n> incorrect user counts.\n>\n> This situation resembled a salami attack, where small inaccuracies in user\n> counts initially seemed insignificant but compounded over time to become\n> substantial. During this period, decisions were made based on these\n> inaccurate counts, and subsequent decisions were based on the initial ones.\n> The delay in detecting this anomaly made it challenging to reverse the\n> damage.\n\nAnother problem with monitoring tools is the net accuracy. It is challenging\nfor these monitoring tools to achieve 100% accuracy.\n\nFor instance, we deployed a secret scanner in one of my previous companies.\nThe scanner would scan our codebase and other filesystems to detect any\npotential secret leak. Given the massive size of our organization's codebase,\neach scan would return ~10,000 \"potential\" leaked secrets. You have no clue\nabout how many of those are True Positives. Eventually, there's a time when\nyou stop running these exercises until, unfortunately, a secret leaks.\n\nTwo metrics further validate the impact of a lagging indicator:\n\n  1. time_to_contain_breach\n\n  2. time_to_exploit\n\nOn average, a typical organization takes 204 days to contain a data breach\nfully.\n\nOn the other hand, attackers take only 32 days to exploit a vulnerability.\nWhat is worrying is that this number is expected to decrease significantly as\nwe see more AI adoption. Nikesh Arora, CEO of Palo Alto Networks, articulated\nthis thought very well.\n\nNikesh Arora's take on mismatch between speed of attack & resolution\n\nThese two metrics clearly show that relying solely on a lagging metric for\ncybersecurity is not a wise choice.\n\n##\n\nCase for more proactive measures\n\nI am not the first to propose proactive measures (also referred as shift-\nleft); however, to my surprise, every 3 in 4 companies I interviewed were yet\nto invest significantly in such capabilities.\n\n> A company recently acquired for double-digit billions of dollars managed\n> 100+ API keys in a spreadsheet. According to their DevOps lead, it was\n> alright because the spreadsheet had access to only four team members.\n>\n> The LastPass incident continues to teach us the perils of high reliance on\n> humans for security.\n\nIf not all, there are a few areas where I strongly feel that securing-data-by-\ndesign is a winning strategy; these include understanding:\n\n  1. Why & What you collect?\n\n  2. Where & How you store?\n\n  3. Why, How, & Who accesses?\n\n  4. Why & Where you share?\n\nLet's dive into each.\n\n###\n\nWhy & What data you collect?\n\nKnowing your data is the first step to securing your data.\n\nAs engineers & product managers, we often have a free hand in collecting &\nusing data to solve business outcomes. We are biased in doing whatever\nimproves the customer experience. An engineer working on Personalisation will\ninclude the user's gender, preferences, etc., to improve recommendations. To\nenhance transaction success, a fraud-risk product manager will include the\nuser's location, age, etc.\n\nIs that wrong? Certainly not. However, if you do not ask why every time, the\nengineer will start using the user's race, or the product manager will start\n\"securely\" reading the user's SMS data to improve their model's efficiency by\n0.1%!\n\nIronically, knowing your data is often an afterthought. Months after\ncollecting, using, and sharing data, organisations decide to set up Data\nCataloging tools to visualise data usage.\n\n> I often visualise these decisions as reverse onion peeling. You start with\n> the onion core and layer a peel whenever you decide. If you do not evaluate\n> your decisions carefully, several decisions and peelings later, you realize\n> this is not the onion you wanted.\n\nA simple approach is to add a checklist item to your PR or PRD to review\nwhether consumer data is being used or processed and, accordingly, reason the\nneed.\n\nAdditionally, data cataloging & observability tools such as Amundsen, DataHub,\nOpenMetadata, AccelData, Atlan, etc., along with data scanning tools such as\nPII Catcher, OctoPII (for images, documents, etc.) should be explored.\n\n###\n\nWhere & How are you storing?\n\nKnowing your data significantly reduces the problem space.\n\nOnce you know your sensitive data, you must always store it separately from\nthe rest of your data. Separating the data further reduces the problem space,\nallowing you to implement stricter access & governance policies on the\nisolated data.\n\nThis separation must ideally occur at the left-most perimeter of your\ninfrastructure \u2013 as early as possible once the data reaches your backend.\n\nThere are a few approaches.\n\nA simple approach is to take this decision at an API handler or ORM layer.\nOnce the data reaches your application code, you can set up rules to route the\nsensitive data to a secure store, while the remaining data can be stored in\nyour regular datastore.\n\nAnother approach is to separate at an API gateway layer. VGS has a reference\narchitecture for this pattern. While this option is more foolproof, it is\ncomplex to implement. Additionally, the application context spills over to the\ninfrastructure layer in this approach; however, the centralized approach\nensures higher separation efficacy.\n\nOnce the sensitive data is sniffed away and stored separately, the rest of the\nentities can refer to it using their tokenized identifiers. Tokenization is a\nmature concept widely used in the payments industry. AWS has a blog that you\ncan refer to implement tokenisation.\n\nYou can explore a few commercial solutions in this space, such as SkyFlow,\nVery Good Security, HashiCorp Vault, etc.\n\n###\n\nWhy & Who is accessing data?\n\nOnce we identify and securely store sensitive data, we must restrict access.\n\nWe mostly worry about humans having access to data. It is fair in many ways\nbecause human error is one of the leading reasons for data breaches.\n\nHowever, do you know that on average, organizations have 45 machine identities\nto manage for every 1 human? And this gap is widening very fast.\n\nThis makes it crucial to know and govern which machine has access to what data\nand with what privileges. The machine here can refer to any compute resource\nthat runs your application code, scripts, bots, etc.\n\nSecuring access requires you to implement the following:\n\n  1. A secure way to generate, store, and use credentials (Passwords, API keys, Tokens, Service Accounts, etc.).\n\n  2. A proactive review of permissions granted to every credential\n\nCompanies are often nonchalant about identity and access governance,\nparticularly with non-human identities.\n\nSecret Manager is one of the first tools you must set up. However, with the\nincreased adoption of third-party vendors in the stack, there is a rising need\nto implement operational controls, such as how the credentials are shared with\nthe runtime, how to prevent a human from accessing a credential, how to rotate\ncredentials periodically, and so on\n\nBeyond secret managers, ensuring that no human has a long-lived access to any\ncredential has become critical. Numerous options are available today to\nimplement secure access channels for humans.\n\nTools such as P0, ConductorOne, and others have made it extremely\nstraightforward to set up just-in-time access control.\n\nFurther, multiple Privilege Access Manager (PAM) tools improve audibility and\ninject rich governance into access.\n\nA thing to note is that organizations often have variable success setting up\nPAM tools on production, given the implementation complexity; however, having\na secrets manager (with operational automation) and a just-in-time access\nprovisioner must be the first things to explore as you start the journey.e\n\n###\n\nWhy & Where is data shared?\n\nDespite setting up enough access guardrails, employees will likely share\nsensitive data with third parties for legitimate reasons. For instance,\nsharing customer data on CRMs, internal conversations on Slack, customer\nissues on Jira, etc.\n\nUnchecked growth will cause problems:\n\n  * Of the CISOs I interviewed, every two out of four knew only a small subset of all vendors in use.\n\n  * Few to no guardrails control how and what consumer data humans or programs share externally.\n\n  * Companies without a legal team do not review Data Processing Agreements before signing up vendors.\n\n  * In startups & smaller companies, engineers often have a free hand in trying and adopting new SaaS vendors that, many times, may not be compliant.\n\nKnowing where you share data is essential because this data is out of your\ncontrol. If the vendor does not handle the data carefully, your consumers'\ndata will be vulnerable.\n\nWhile there are tools that scan for consumer data leaked to vendors, I would\nonly rely partially on them since these scans are fuzzy and depend a lot on\nthe underlying vendor's APIs.\n\nThe obvious starters are setting up a vendor onboarding process, single-sign-\non tools, etc., but as your infrastructure and business mature, you must\nincreasingly invest resources in setting up a zero-trust architecture; as a\nstart, SASE for human access.\n\n##\n\nConclusion\n\n\ud83e\udee1 Collecting consumer data is a responsibility, not a privilege.\n\nAs our digital footprint expands, so does the number of internet users.\nUnfortunately, many users lack an understanding of how much sensitive data is\ncollected about them, and therefore do not raise objections. Even a slight\nmishandling of this data can lead to severe consequences for our consumers.\n\nOften, the development of security capabilities is deferred until a company\nhires its first CISO or reaches a critical milestone, such as acquiring the\nfirst enterprise customer or expanding into a new geographic market. This\ndelay is typically attributed to a perceived lack of security skills and focus\nwithin the organization. The problem with this approach is the extensive\nmigration work required later, which often turns into a game of catch-up with\nnewer software releases.\n\nWhile adopting secure-by-design practices may seem daunting initially, the\napproach yields compounding benefits as it becomes woven into the fabric of\nyour everyday processes.\n\nThank you, Shadab for reviewing the post.\n\nThanks for reading my blog! Subscribe for free to receive new posts and\nsupport my work.\n\nShare this post\n\n#### Proactive Data Security\n\nwww.jayeshs.com\n\nShare\n\nComments\n\nReady for more?\n\n\u00a9 2024 Jayesh Sidhwani\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": false}
