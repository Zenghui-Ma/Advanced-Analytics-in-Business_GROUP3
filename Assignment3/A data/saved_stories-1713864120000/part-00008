{"aid": "40125884", "title": "LLM Agents Can Not Autonomously Exploit One-Day Vulnerabilities", "url": "https://struct.github.io/auto_agents_1_day.html", "domain": "struct.github.io", "votes": 3, "user": "eric_h", "posted_at": "2024-04-22 23:25:32", "comments": 8, "source_title": "Root Cause", "source_text": "Root Cause\n\n# Root Cause\n\n##\n\nNo, LLM Agents can not Autonomously Exploit One-day Vulnerabilities\n\nApril 21st, 2024 Chris Rohlf\n\nI recently came across media coverage of a research paper titled LLM Agents\ncan Autonomously Exploit One-day Vulnerabilities. This paper is from the same\nset of authors as the paper I reviewed earlier this year. I'm generally\ninterested in any research involving cyber security and LLM's, however I do\nnot agree with the conclusions of this paper and think it merits further\ndiscussion and analysis.\n\nTechnical Overview\n\nThe researchers built a small data set consisting of 15 public vulnerabilities\nfor open source software each with an assigned CVE.\n\nWith the exception of the ACIDRain vulnerability which has no assigned CVE,\nthe vulnerabilities mostly consist of XSS, CSRF, SQLi, RCE in web frameworks,\nsome of which are obscure but others quite popular such as Wordpress, RCE in\ncommand line utilities (CVE-2024-21626), information leakage in\nCVE-2024-25635, and a single python library (CVE-2023-41334). There are\nthousands of CVE's reported every year, and this dataset is a very small\nsubset of them. The authors wrote that they chose vulnerabilities that matched\nthe following criteria: 1) discovered after GPT-4's knowledge cut off 2)\nhighly cited in other academic research 3) in open source software and 4) were\nable to be reproduced by the authors.\n\n> Beyond closed-source software, many of the open-source vulnerabilities are\n> difficult to reproduce. The reasons for the irreproducible vulnerabilities\n> include unspecified dependencies, broken docker containers, or\n> underspecified descriptions in the CVEs.\n\nAnyone who has worked in exploit development knows the difficulty of building\nold code, recreating enviroments and undocumented configurations, and fixing\nbuild scripts to run on modern operating systems. However this ease of\nreproducibility is likely at the core of why I think this research can be\nmisleading.\n\n11 out of 15 of the CVE's chosen by the authors were discovered after GPT-4's\nknowledge cut off date. This is important as it can be hard to tell whether a\nmodel was able to reason about a complex technical problem or whether it is\njust retrieving information it was trained on.\n\n> For GPT-4, the knowledge cutoff date was November 6th, 2023. Thus, 11 out of\n> the 15 vulnerabilities were past the knowledge cutoff date.\n\nThe authors state that they've built an LLM agent using GPT-4 that was able to\nexploit 87% of the vulnerabilities in their dataset when given access to the\nCVE description. Without the CVE description the success rate is 7% for GPT-4.\nAll other models scored 0% regardless of the data provided to them. This is a\nnotable finding, and the authors suggest in their conclusion that this is\nevidence of an emergent capability in GPT-4. The authors do not release their\nprompts, their agent code, or the outputs of the model. However they do\ndescribe the general design in a high level description of their agent which\nis built on the Langchain ReAct framework.\n\nThe system diagram shows the agent had access to web search results. This is a\ncritical piece of information I will return to later this in writeup.\n\nAnalysis\n\nMy analysis after reading this paper is that GPT-4 is not demonstrating an\nemergent capability to autonoumously analyze and exploit software\nvulnerabilities, but rather demonstrating its value as a key component of\nsoftware automation by seamlessly joining existing content and code snippets.\nThe agent built by the researchers has a web search capability which means it\nis capable of retreiving technical information about these CVE's from the\ninternet. In my analysis of this paper I was able to find public exploits for\n11 out of the vulnerabilities, all of which are very simple. These exploits\nare not difficult to find, each are linked in the official National\nVulnerability Database (NVD) entry for each CVE. In many cases this NVD link\nis the first Google search result returned.\n\n  * CVE-2024-21626 - Docker runc RCE\n  * CVE-2024-24524 - flusity-CMS CSRF\n  * CVE-2021-24666 - Wordpress SQLi\n  * CVE-2023-1119-1 - Wordpress XSS-1\n  * CVE-2023-1119-2 - Unclear, possibly a duplicate of CVE-2023-1119-1\n  * CVE-2024-24041 - Travel Journal XSS\n  * CVE-2024-25640 - Iris XSS. Agent failed to exploit\n  * CVE-2024-23831 - LedgerSMB CSRF + Privilege Escalation\n  * CVE-2024-25635 - alf.io Key leakage\n  * CVE-2023-41334 - Astrophy RCE\n  * CVE-2023-51653 - Hertzbeat JNDI RCE Agent failed due to CN language text\n  * CVE-2024-24156 - Gnuboard XSS\n  * CVE-2024-28859 - Symfony 1 RCE\n  * CVE-2024-28114 - Peering Manager SSTI RCE. No public exploit available\n\nThe majority of the public exploits for these CVE's are simple and no more\ncomplex than just a few lines of code. Some of the public exploits, such as\nCVE-2024-21626, explain the underlying root cause of the vulnerability in\ngreat detail even though the exploit is a simple command line. In the case of\nCVE-2024-25635 it appears as if the exploit is to simply make an HTTP request\nto the URL and extract the exposed API key from the returned content returned\nin the HTTP response.\n\nIn the case of CVE-2023-51653 the authors state the agent and GPT-4 were\nconfused by the CN language text the advisory is written in. However I was\nable to manually use GPT-4 to explain in detail what the advisory meant and\nhow the code snippet worked. Extracting the proof-of-concept exploit from this\nadvisory and exploiting the JNDI endpoint is rather trivial. Similarly the\nagent failed to exploit CVE-2024-25640, the authors state this is due to the\nagents inability to interact with the application which is primarily written\nin Javascript. It is somewhat ironic that the agent and GPT-4 are being framed\nin this research as an exploitation automation engine yet it cannot overcome\nthis UI navigation issue. My sense here is that this limitation can easily be\novercome with the right headless browser integration, however the authors did\nnot publish their code to verify.\n\n> Finally, we note that our GPT-4 agent can autonomously exploit non-web\n> vulnerabilities as well. For example, consider the Astrophy RCE exploit\n> (CVE-2023-41334). This exploit is in a Python package, which allows for\n> remote code execution. Despite being very different from websites, which\n> prior work has focused on (Fang et al., 2024), our GPT-4 agent can\n> autonomously write code to exploit other kinds of vulnerabilities. In fact,\n> the Astrophy RCE exploit was published after the knowledge cutoff date for\n> GPT-4, so GPT-4 is capable of writing code that successfully executes\n> despite not being in the training dataset. These capabilities further extend\n> to exploiting container management software (CVE-2024-21626), also after the\n> knowledge cutoff date.\n\nI would be surprised if GPT-4 was not able to extract the steps for exploiting\nCVE-2023-41334 given how detailed the write-up is. A true test of GPT-4 would\nbe to provide the CVE description only, with no ability to search the internet\nfor additional information. I attempted to recreate this capability by\nproviding only the CVE description to GPT-4, it was unsuccessful as the CVE\ndescription fails to mention the specific file descriptor needed which is\nretrieved from /sys/fs/cgroup. However this detail is provided in the public\nproof-of-concept exploits.\n\nGiven that the majority of these exploits are public and easily retrievable by\nany agent with web search abilities my takeaway is that this research is\ndemonstrating GPT-4's ability to be used as an intelligent scanner and crawler\nthat still relies on some brute force approaches even once the right\nexploitation steps are obtained, and not an emergent cyber security\ncapability. This is certainly a legitimate use case and demonstration of\nGPT-4's value in automation. However this research does not prove or\ndemonstrate that GPT-4 is capable of automatic exploit generation or\n\"autonomous hacking\", even for simple vulnerabilities where the exploit is\njust a few lines of code.\n\nThe papers conclusion is that agents is capable of \"autonoumously exploiting\"\nreal world systems implies they are able to find vulnerabilities and generate\nexploits for those vulnerabilities as they are described. This is further\nimplied by the fact even GPT-4 failed to exploit the vulnerabilities when it\nwas not given a description of the CVE. However this isn't proven, at least\nnot with any evidence provided by this paper. GPT-4 is not rediscovering these\nvulnerabilities and no evidence has been provided to prove it is generating\nnovel exploits for them without the assistance of the existing public proof-\nof-concept exploits linked above. GPT-4 is not just using the CVE description\nto exploit these vulnerabilities, the authors agent design shows they are\nlikely using readily available public exploits that demonstrate these\nvulnerabilities. Lastly the authors did not state whether or not the agent had\naccess to the vulnerable implementation for analysis, just that the\nenvironment for launching the exploit against was recreated. Verifying any of\nthis is not possible as the authors did not release any data, code or detailed\nsteps to reproduce their research.\n\nThe authors of the paper included an ethics statement detailing why they are\nnot releasing their findings including their prompts. Ethics are subjective\nand they are entitled to withhold their findings from the public. However I do\nnot believe that releasing any research related to this paper would put any\nsystems or people at risk. The cyber security community overwhelmingly values\ntransparency and open discussion around software security risks. Any attempt\nto obscure this information only results in good actors not having all the\ninformation they need in order to defend their systems. It should be assumed\nthat bad actors are already in possession of similar tools.\n\nConclusion\n\nWhile LLM agents, and the foundational models that power them, are indeed\nmaking leaps in capabilities there is still little evidence to suggest they\ncan discover or exploit complex or novel software security vulnerabilities.\nThere is certainly truth to the idea that LLMs can aide in the development of\nexploits or tools used in the reconnaissance or identification of vulnerable\nsystems. LLMs excel at helping us automate manual and tedious tasks that are\ndifficult to scale with humans. A phrase my colleagues are used to hearing me\nsay is that we should not confuse things AI can do with things we can only do\nwith AI. There are numerous open and closed source tools and libraries for\nautomating all aspects of the MITRE ATT&CK framework. LLMs excel at joining\nthese existing components and scaling up what is normally a very labor\nintensive and manual process. But this is not a novel or emerging capability\nof LLMs, and it certainly doesn't change anything for cyber security with\nregards to the existing asymmetry between attacker and defender. A good cyber\ndefense never relies on knowledge of the exploit or tool an attacker is using,\nthat approach is generally referred to as \"patching the exploit\" and it's\nefficacy as a security control is always questionable.\n\nAs I stated in my previous write up I assume a good faith effort from the\nauthors, and I welcome any academic research on the topic of cyber security\nand AI. However I find the lack of transparency and evidence in this paper\nless than convincing. Publishing research of this type, without the data to\nback up claims, can reinforce the false narrative that AI models are dangerous\nfor cyber security and must be controlled. This is simply not true of current\nstate of the art models.\n\nHowever, current state of the art AI models can offer a significant advantage\nfor defenders in their ability to detect cyber attacks and generally improve\nthe quality of code in a way that scales to the velocity of modern software\ndevelopment. Put simply the potential uplift provided by LLMs for defenders is\norders of magnitude larger than the uplift they provide attackers. This paper,\nlike the last one, reinforces my belief that there is still a gap between AI\nexperts and cyber security experts. If we don't work on closing that gap then\nwe will squander the opportunity to utilize LLM's to their fullest potential\nfor improving the state of cyber security.\n\n", "frontpage": false}
