{"aid": "40046698", "title": "PyTorch internals: ezyang's blog (2019)", "url": "http://blog.ezyang.com/2019/05/pytorch-internals/", "domain": "ezyang.com", "votes": 1, "user": "ashvardanian", "posted_at": "2024-04-15 23:08:51", "comments": 0, "source_title": "ezyang\u2019s blog", "source_text": "PyTorch internals : ezyang\u2019s blog\n\n# ezyang\u2019s blog\n\nthe arc of software bends towards understanding\n\n  * About\n  * Archives\n  * Subscribe\n\n## PyTorch internals\n\nThis post is a long form essay version of a talk about PyTorch internals, that\nI gave at the PyTorch NYC meetup on May 14, 2019.\n\nHi everyone! Today I want to talk about the internals of PyTorch.\n\nThis talk is for those of you who have used PyTorch, and thought to yourself,\n\"It would be great if I could contribute to PyTorch,\" but were scared by\nPyTorch's behemoth of a C++ codebase. I'm not going to lie: the PyTorch\ncodebase can be a bit overwhelming at times. The purpose of this talk is to\nput a map in your hands: to tell you about the basic conceptual structure of a\n\"tensor library that supports automatic differentiation\", and give you some\ntools and tricks for finding your way around the codebase. I'm going to assume\nthat you've written some PyTorch before, but haven't necessarily delved deeper\ninto how a machine learning library is written.\n\nThe talk is in two parts: in the first part, I'm going to first introduce you\nto the conceptual universe of a tensor library. I'll start by talking about\nthe tensor data type you know and love, and give a more detailed discussion\nabout what exactly this data type provides, which will lead us to a better\nunderstanding of how it is actually implemented under the hood. If you're an\nadvanced user of PyTorch, you'll be familiar with most of this material. We'll\nalso talk about the trinity of \"extension points\", layout, device and dtype,\nwhich guide how we think about extensions to the tensor class. In the live\ntalk at PyTorch NYC, I skipped the slides about autograd, but I'll talk a\nlittle bit about them in these notes as well.\n\nThe second part grapples with the actual nitty gritty details involved with\nactually coding in PyTorch. I'll tell you how to cut your way through swaths\nof autograd code, what code actually matters and what is legacy, and also all\nof the cool tools that PyTorch gives you for writing kernels.\n\nThe tensor is the central data structure in PyTorch. You probably have a\npretty good idea about what a tensor intuitively represents: its an\nn-dimensional data structure containing some sort of scalar type, e.g.,\nfloats, ints, et cetera. We can think of a tensor as consisting of some data,\nand then some metadata describing the size of the tensor, the type of the\nelements in contains (dtype), what device the tensor lives on (CPU memory?\nCUDA memory?)\n\nThere's also a little piece of metadata you might be less familiar with: the\nstride. Strides are actually one of the distinctive features of PyTorch, so\nit's worth discussing them a little more.\n\nA tensor is a mathematical concept. But to represent it on our computers, we\nhave to define some sort of physical representation for them. The most common\nrepresentation is to lay out each element of the tensor contiguously in memory\n(that's where the term contiguous comes from), writing out each row to memory,\nas you see above. In the example above, I've specified that the tensor\ncontains 32-bit integers, so you can see that each integer lies in a physical\naddress, each offset four bytes from each other. To remember what the actual\ndimensions of the tensor are, we have to also record what the sizes are as\nextra metadata.\n\nSo, what do strides have to do with this picture?\n\nSuppose that I want to access the element at position tensor[0, 1] in my\nlogical representation. How do I translate this logical position into a\nlocation in physical memory? Strides tell me how to do this: to find out where\nany element for a tensor lives, I multiply each index with the respective\nstride for that dimension, and sum them all together. In the picture above,\nI've color coded the first dimension blue and the second dimension red, so you\ncan follow the index and stride in the stride calculation. Doing this sum, I\nget two (zero-indexed), and indeed, the number three lives two below the\nbeginning of the contiguous array.\n\n(Later in the talk, I'll talk about TensorAccessor, a convenience class that\nhandles the indexing calculation. When you use TensorAccessor, rather than raw\npointers, this calculation is handled under the covers for you.)\n\nStrides are the fundamental basis of how we provide views to PyTorch users.\nFor example, suppose that I want to extract out a tensor that represents the\nsecond row of the tensor above:\n\nUsing advanced indexing support, I can just write tensor[1, :] to get this\nrow. Here's the important thing: when I do this, I don't create a new tensor;\ninstead, I just return a tensor which is a different view on the underlying\ndata. This means that if I, for example, edit the data in that view, it will\nbe reflected in the original tensor. In this case, it's not too hard to see\nhow to do this: three and four live in contiguous memory, and all we need to\ndo is record an offset saying that the data of this (logical) tensor lives two\ndown from the top. (Every tensor records an offset, but most of the time it's\nzero, and I'll omit it from my diagrams when that's the case.)\n\n> Question from the talk: If I take a view on a tensor, how do I free the\n> memory of the underlying tensor?\n>\n> Answer: You have to make a copy of the view, thus disconnecting it from the\n> original physical memory. There's really not much else you can do. By the\n> way, if you have written Java in the old days, taking substrings of strings\n> has a similar problem, because by default no copy is made, so the substring\n> retains the (possibly very large string). Apparently, they fixed this in\n> Java 7u6.\n\nA more interesting case is if I want to take the first column:\n\nWhen we look at the physical memory, we see that the elements of the column\nare not contiguous: there's a gap of one element between each one. Here,\nstrides come to the rescue: instead of specifying a stride of one, we specify\na stride of two, saying that between one element and the next, you need to\njump two slots. (By the way, this is why it's called a \"stride\": if we think\nof an index as walking across the layout, the stride says how many locations\nwe stride forward every time we take a step.)\n\nThe stride representation can actually let you represent all sorts of\ninteresting views on tensors; if you want to play around with the\npossibilities, check out the Stride Visualizer.\n\nLet's step back for a moment, and think about how we would actually implement\nthis functionality (after all, this is an internals talk.) If we can have\nviews on tensor, this means we have to decouple the notion of the tensor (the\nuser-visible concept that you know and love), and the actual physical data\nthat stores the data of the tensor (called storage):\n\nThere may be multiple tensors which share the same storage. Storage defines\nthe dtype and physical size of the tensor, while each tensor records the\nsizes, strides and offset, defining the logical interpretation of the physical\nmemory.\n\nOne thing to realize is that there is always a pair of Tensor-Storage, even\nfor \"simple\" cases where you don't really need a storage (e.g., you just\nallocated a contiguous tensor with torch.zeros(2, 2)).\n\n> By the way, we're interested in making this picture not true; instead of\n> having a separate concept of storage, just define a view to be a tensor that\n> is backed by a base tensor. This is a little more complicated, but it has\n> the benefit that contiguous tensors get a much more direct representation\n> without the Storage indirection. A change like this would make PyTorch's\n> internal representation a bit more like Numpy's.\n\nWe've talked quite a bit about the data layout of tensor (some might say, if\nyou get the data representation right, everything else falls in place). But\nit's also worth briefly talking about how operations on the tensor are\nimplemented. At the very most abstract level, when you call torch.mm, two\ndispatches happen:\n\nThe first dispatch is based on the device type and layout of a tensor: e.g.,\nwhether or not it is a CPU tensor or a CUDA tensor (and also, e.g., whether or\nnot it is a strided tensor or a sparse one). This is a dynamic dispatch: it's\na virtual function call (exactly where that virtual function call occurs will\nbe the subject of the second half of this talk). It should make sense that you\nneed to do a dispatch here: the implementation of CPU matrix multiply is quite\ndifferent from a CUDA implementation. It is a dynamic dispatch because these\nkernels may live in separate libraries (e.g., libcaffe2.so versus\nlibcaffe2_gpu.so), and so you have no choice: if you want to get into a\nlibrary that you don't have a direct dependency on, you have to dynamic\ndispatch your way there.\n\nThe second dispatch is a dispatch on the dtype in question. This dispatch is\njust a simple switch-statement for whatever dtypes a kernel chooses to\nsupport. Upon reflection, it should also make sense that we need to a dispatch\nhere: the CPU code (or CUDA code, as it may) that implements multiplication on\nfloat is different from the code for int. It stands to reason you need\nseparate kernels for each dtype.\n\nThis is probably the most important mental picture to have in your head, if\nyou're trying to understand the way operators in PyTorch are invoked. We'll\nreturn to this picture when it's time to look more at code.\n\nSince we have been talking about Tensor, I also want to take a little time to\nthe world of tensor extensions. After all, there's more to life than dense,\nCPU float tensors. There's all sorts of interesting extensions going on, like\nXLA tensors, or quantized tensors, or MKL-DNN tensors, and one of the things\nwe have to think about, as a tensor library, is how to accommodate these\nextensions.\n\nOur current model for extensions offers four extension points on tensors.\nFirst, there is the trinity three parameters which uniquely determine what a\ntensor is:\n\n  * The device, the description of where the tensor's physical memory is actually stored, e.g., on a CPU, on an NVIDIA GPU (cuda), or perhaps on an AMD GPU (hip) or a TPU (xla). The distinguishing characteristic of a device is that it has its own allocator, that doesn't work with any other device.\n  * The layout, which describes how we logically interpret this physical memory. The most common layout is a strided tensor, but sparse tensors have a different layout involving a pair of tensors, one for indices, and one for data; MKL-DNN tensors may have even more exotic layout, like blocked layout, which can't be represented using merely strides.\n  * The dtype, which describes what it is that is actually stored in each element of the tensor. This could be floats or integers, or it could be, for example, quantized integers.\n\nIf you want to add an extension to PyTorch tensors (by the way, if that's what\nyou want to do, please talk to us! None of these things can be done out-of-\ntree at the moment), you should think about which of these parameters you\nwould extend. The Cartesian product of these parameters define all of the\npossible tensors you can make. Now, not all of these combinations may actually\nhave kernels (who's got kernels for sparse, quantized tensors on FPGA?) but in\nprinciple the combination could make sense, and thus we support expressing it,\nat the very least.\n\nThere's one last way you can make an \"extension\" to Tensor functionality, and\nthat's write a wrapper class around PyTorch tensors that implements your\nobject type. This perhaps sounds obvious, but sometimes people reach for\nextending one of the three parameters when they should have just made a\nwrapper class instead. One notable merit of wrapper classes is they can be\ndeveloped entirely out of tree.\n\nWhen should you write a tensor wrapper, versus extending PyTorch itself? The\nkey test is whether or not you need to pass this tensor along during the\nautograd backwards pass. This test, for example, tells us that sparse tensor\nshould be a true tensor extension, and not just a Python object that contains\nan indices and values tensor: when doing optimization on networks involving\nembeddings, we want the gradient generated by the embedding to be sparse.\n\nOur philosophy on extensions also has an impact of the data layout of tensor\nitself. One thing we really want out of our tensor struct is for it to have a\nfixed layout: we don't want fundamental (and very frequently called)\noperations like \"What's the size of a tensor?\" to require virtual dispatches.\nSo when you look at the actual layout of a Tensor (defined in the TensorImpl\nstruct), what we see is a common prefix of all fields that we consider all\n\"tensor\"-like things to universally have, plus a few fields that are only\nreally applicable for strided tensors, but are so important that we've kept\nthem in the main struct, and then a suffix of custom fields that can be done\non a per-Tensor basis. Sparse tensors, for example, store their indices and\nvalues in this suffix.\n\nI told you all about tensors, but if that was the only thing PyTorch provided,\nwe'd basically just be a Numpy clone. The distinguishing characteristic of\nPyTorch when it was originally released was that it provided automatic\ndifferentiation on tensors (these days, we have other cool features like\nTorchScript; but back then, this was it!)\n\nWhat does automatic differentiation do? It's the machinery that's responsible\nfor taking a neural network:\n\n...and fill in the missing code that actually computes the gradients of your\nnetwork:\n\nTake a moment to study this diagram. There's a lot to unpack; here's what to\nlook at:\n\n  1. First, rest your eyes on the variables in red and blue. PyTorch implements reverse-mode automatic differentiation, which means that we effectively walk the forward computations \"backward\" to compute the gradients. You can see this if you look at the variable names: at the bottom of the red, we compute loss; then, the first thing we do in the blue part of the program is compute grad_loss. loss was computed from next_h2, so we compute grad_next_h2. Technically, these variables which we call grad_ are not really gradients; they're really Jacobians left-multiplied by a vector, but in PyTorch we just call them grad and mostly everyone knows what we mean.\n  2. If the structure of the code stays the same, the behavior doesn't: each line from forwards is replaced with a different computation, that represents the derivative of the forward operation. For example, the tanh operation is translated into a tanh_backward operation (these two lines are connected via a grey line on the left hand side of the diagram). The inputs and outputs of the forward and backward operations are swapped: if the forward operation produced next_h2, the backward operation takes grad_next_h2 as an input.\n\nThe whole point of autograd is to do the computation that is described by this\ndiagram, but without actually ever generating this source. PyTorch autograd\ndoesn't do a source-to-source transformation (though PyTorch JIT does know how\nto do symbolic differentiation).\n\nTo do this, we need to store more metadata when we carry out operations on\ntensors. Let's adjust our picture of the tensor data structure: now instead of\njust a tensor which points to a storage, we now have a variable which wraps\nthis tensor, and also stores more information (AutogradMeta), which is needed\nfor performing autograd when a user calls loss.backward() in their PyTorch\nscript.\n\n> This is yet another slide which will hopefully be out of date in the near\n> future. Will Feng is working on a Variable-Tensor merge in C++, following a\n> simple merge which happened to PyTorch's frontend interface.\n\nWe also have to update our picture about dispatch:\n\nBefore we dispatch to CPU or CUDA implementations, there is another dispatch\non variables, which is responsible for unwrapping variables, calling the\nunderlying implementation (in green), and then rewrapping the results into\nvariables and recording the necessary autograd metadata for backwards.\n\nSome implementations don't unwrap; they just call into other variable\nimplementations. So you might spend a while in the Variable universe. However,\nonce you unwrap and go into the non-Variable Tensor universe, that's it; you\nnever go back to Variable (except by returning from your function.)\n\nIn my NY meetup talk, I skipped the following seven slides. I'm also going to\ndelay writeup for them; you'll have to wait for the sequel for some text.\n\nEnough about concepts, let's look at some code.\n\nPyTorch has a lot of folders, and there is a very detailed description of what\nthey are in the CONTRIBUTING document, but really, there are only four\ndirectories you really need to know about:\n\n  * First, torch/ contains what you are most familiar with: the actual Python modules that you import and use. This stuff is Python code and easy to hack on (just make a change and see what happens). However, lurking not too deep below the surface is...\n  * torch/csrc/, the C++ code that implements what you might call the frontend of PyTorch. In more descriptive terms, it implements the binding code that translates between the Python and C++ universe, and also some pretty important pieces of PyTorch, like the autograd engine and the JIT compiler. It also contains the C++ frontend code.\n  * aten/, short for \"A Tensor Library\" (coined by Zachary DeVito), is a C++ library that implements the operations of Tensors. If you're looking for where some kernel code lives, chances are it's in ATen. ATen itself bifurcates into two neighborhoods of operators: the \"native\" operators, which are modern, C++ implementations of operators, and the \"legacy\" operators (TH, THC, THNN, THCUNN), which are legacy, C implementations. The legacy operators are the bad part of town; try not to spend too much time there if you can.\n  * c10/, which is a pun on Caffe2 and A\"Ten\" (get it? Caffe 10) contains the core abstractions of PyTorch, including the actual implementations of the Tensor and Storage data structures.\n\nThat's a lot of places to look for code; we should probably simplify the\ndirectory structure, but that's how it is. If you're trying to work on\noperators, you'll spend most of your time in aten.\n\nLet's see how this separation of code breaks down in practice:\n\nWhen you call a function like torch.add, what actually happens? If you\nremember the discussion we had about dispatching, you already have the basic\npicture in your head:\n\n  1. We have to translate from Python realm to the C++ realm (Python argument parsing)\n  2. We handle variable dispatch (VariableType--Type, by the way, doesn't really have anything to do programming language types, and is just a gadget for doing dispatch.)\n  3. We handle device type / layout dispatch (Type)\n  4. We have the actual kernel, which is either a modern native function, or a legacy TH function.\n\nEach of these steps corresponds concretely to some code. Let's cut our way\nthrough the jungle.\n\nOur initial landing point in the C++ code is the C implementation of a Python\nfunction, which we've exposed to the Python side as something like\ntorch._C.VariableFunctions.add. THPVariable_add is the implementation of one\nsuch implementation.\n\nOne important thing to know about this code is that it is auto-generated. If\nyou search in the GitHub repository, you won't find it, because you have to\nactually build PyTorch to see it. Another important thing is, you don't have\nto really deeply understand what this code is doing; the idea is to skim over\nit and get a sense for what it is doing. Above, I've annotated some of the\nmost important bits in blue: you can see that there is a use of a class\nPythonArgParser to actually pull out C++ objects out of the Python args and\nkwargs; we then call a dispatch_add function (which I've inlined in red); this\nreleases the global interpreter lock and then calls a plain old method on the\nC++ Tensor self. On its way back, we rewrap the returned Tensor back into a\nPyObject.\n\n(At this point, there's an error in the slides: I'm supposed to tell you about\nthe Variable dispatch code. I haven't fixed it here yet. Some magic happens,\nthen...)\n\nWhen we call the add method on the Tensor class, no virtual dispatch happens\nyet. Instead, we have an inline method which calls a virtual method on a\n\"Type\" object. This method is the actual virtual method (this is why I say\nType is just a \"gadget\" that gets you dynamic dispatch.) In the particular\ncase of this example, this virtual call dispatches to an implementation of add\non a class named TypeDefault. This happens to be because we have an\nimplementation of add that is the same for every device type (both CPU and\nCUDA); if we had happened to have different implementations, we might have\ninstead landed on something like CPUFloatType::add. It is this implementation\nof the virtual method that finally gets us to the actual kernel code.\n\n> Hopefully, this slide will be out-of-date very soon too; Roy Li is working\n> on replacing Type dispatch with another mechanism which will help us better\n> support PyTorch on mobile.\n\nIt's worth reemphasizing that all of the code, until we got to the kernel, is\nautomatically generated.\n\nIt's a bit twisty and turny, so once you have some basic orientation about\nwhat's going on, I recommend just jumping straight to the kernels.\n\nPyTorch offers a lot of useful tools for prospective kernel writers. In this\nsection, we'll walk through a few of them. But first of all, what do you need\nto write a kernel?\n\nWe generally think of a kernel in PyTorch consisting of the following parts:\n\n  1. First, there's some metadata which we write about the kernel, which powers the code generation and lets you get all the bindings to Python, without having to write a single line of code.\n  2. Once you've gotten to the kernel, you're past the device type / layout dispatch. The first thing you need to write is error checking, to make sure the input tensors are the correct dimensions. (Error checking is really important! Don't skimp on it!)\n  3. Next, we generally have to allocate the result tensor which we are going to write the output into.\n  4. Time for the kernel proper. At this point, you now should do the second, dtype dispatch, to jump into a kernel which is specialized per dtype it operates on. (You don't want to do this too early, because then you will be uselessly duplicating code that looks the same in any case.)\n  5. Most performant kernels need some sort of parallelization, so that you can take advantage of multi-CPU systems. (CUDA kernels are \"implicitly\" parallelized, since their programming model is built on top of massive parallelization).\n  6. Finally, you need to access the data and do the computation you wanted to do!\n\nIn the subsequent slides, we'll walk through some of the tools PyTorch has for\nhelping you implementing these steps.\n\nTo take advantage of all of the code generation which PyTorch brings, you need\nto write a schema for your operator. The schema gives a mypy-esque type of\nyour function, and also controls whether or not we generate bindings for\nmethods or functions on Tensor. You also tell the schema what implementations\nof your operator should be called for given device-layout combinations. Check\nout the README in native is for more information about this format.\n\nYou also may need to define a derivative for your operation in\nderivatives.yaml.\n\nError checking can be done by way of either a low level or a high level API.\nThe low level API is just a macro, TORCH_CHECK, which takes a boolean, and\nthen any number of arguments to make up the error string to render if the\nboolean is not true. One nice thing about this macro is that you can intermix\nstrings with non-string data; everything is formatted using their\nimplementation of operator<<, and most important data types in PyTorch have\noperator<< implementations.\n\nThe high level API saves you from having to write up repetitive error messages\nover and over again. The way it works is you first wrap each Tensor into a\nTensorArg, which contains information about where the tensor came from (e.g.,\nits argument name). It then provides a number of pre-canned functions for\nchecking various properties; e.g., checkDim() tests if the tensor's\ndimensionality is a fixed number. If it's not, the function provides a user-\nfriendly error message based on the TensorArg metadata.\n\nOne important thing to be aware about when writing operators in PyTorch, is\nthat you are often signing up to write three operators: abs_out, which\noperates on a preallocated output (this implements the out= keyword argument),\nabs_, which operates inplace, and abs, which is the plain old functional\nversion of an operator.\n\nMost of the time, abs_out is the real workhorse, and abs and abs_ are just\nthin wrappers around abs_out; but sometimes writing specialized\nimplementations for each case are warranted.\n\nTo do dtype dispatch, you should use the AT_DISPATCH_ALL_TYPES macro. This\ntakes in the dtype of the tensor you want to dispatch over, and a lambda which\nwill be specialized for each dtype that is dispatchable from the macro.\nUsually, this lambda just calls a templated helper function.\n\nThis macro doesn't just \"do dispatch\", it also decides what dtypes your kernel\nwill support. As such, there are actually quite a few versions of this macro,\nwhich let you pick different subsets of dtypes to generate specializations\nfor. Most of the time, you'll just want AT_DISPATCH_ALL_TYPES, but keep an eye\nout for situations when you might want to dispatch to some more types. There's\nguidance in Dispatch.h for how to select the correct one for your use-case.\n\nOn CPU, you frequently want to parallelize your code. In the past, this was\nusually done by directly sprinkling OpenMP pragmas in your code.\n\nAt some point, we have to actually access the data. PyTorch offers quite a few\noptions for doing this.\n\n  1. If you just want to get a value at some specific location, you should use TensorAccessor. A tensor accessor is like a tensor, but it hard codes the dimensionality and dtype of the tensor as template parameters. When you retrieve an accessor like x.accessor<float, 3>();, we do a runtime test to make sure that the tensor really is this format; but after that, every access is unchecked. Tensor accessors handle strides correctly, so you should prefer using them over raw pointer access (which, unfortunately, some legacy kernels do.) There is also a PackedTensorAccessor, which is specifically useful for sending an accessor over a CUDA launch, so that you can get accessors from inside your CUDA kernel. (One notable gotcha: TensorAccessor defaults to 64-bit indexing, which is much slower than 32-bit indexing in CUDA!)\n  2. If you're writing some sort of operator with very regular element access, for example, a pointwise operation, you are much better off using a higher level of abstraction, the TensorIterator. This helper class automatically handles broadcasting and type promotion for you, and is quite handy.\n  3. For true speed on CPU, you may need to write your kernel using vectorized CPU instructions. We've got helpers for that too! The Vec256 class represents a vector of scalars and provides a number of methods which perform vectorized operations on them all at once. Helpers like binary_kernel_vec then let you easily run vectorized operations, and then finish everything that doesn't round nicely into vector instructions using plain old instructions. The infrastructure here also manages compiling your kernel multiple times under different instruction sets, and then testing at runtime what instructions your CPU supports, and using the best kernel in those situations.\n\nA lot of kernels in PyTorch are still written in the legacy TH style. (By the\nway, TH stands for TorcH. It's a pretty nice acronym, but unfortunately it is\na bit poisoned; if you see TH in the name, assume that it's legacy.) What do I\nmean by the legacy TH style?\n\n  1. It's written in C style, no (or very little) use of C++.\n  2. It's manually refcounted (with manual calls to THTensor_free to decrease refcounts when you're done using tensors), and\n  3. It lives in generic/ directory, which means that we are actually going to compile the file multiple times, but with different #define scalar_t.\n\nThis code is pretty crazy, and we hate reviewing it, so please don't add to\nit. One of the more useful tasks that you can do, if you like to code but\ndon't know too much about kernel writing, is to port some of these TH\nfunctions to ATen.\n\nTo wrap up, I want to talk a little bit about working efficiently on PyTorch.\nIf the largeness of PyTorch's C++ codebase is the first gatekeeper that stops\npeople from contributing to PyTorch, the efficiency of your workflow is the\nsecond gatekeeper. If you try to work on C++ with Python habits, you will have\na bad time: it will take forever to recompile PyTorch, and it will take you\nforever to tell if your changes worked or not.\n\nHow to work efficiently could probably be a talk in and of itself, but this\nslide calls out some of the most common anti-patterns I've seen when someone\ncomplains: \"It's hard to work on PyTorch.\"\n\n  1. If you edit a header, especially one that is included by many source files (and especially if it is included by CUDA files), expect a very long rebuild. Try to stick to editing cpp files, and edit headers sparingly!\n  2. Our CI is a very wonderful, zero-setup way to test if your changes worked or not. But expect to wait an hour or two before you get back signal. If you are working on a change that will require lots of experimentation, spend the time setting up a local development environment. Similarly, if you run into a hard to debug problem on a specific CI configuration, set it up locally. You can download and run the Docker images locally\n  3. The CONTRIBUTING guide explains how to setup ccache; this is highly recommended, because sometimes it will help you get lucky and avoid a massive recompile when you edit a header. It also helps cover up bugs in our build system, when we recompile files when we shouldn't.\n  4. At the end of the day, we have a lot of C++ code, and you will have a much more pleasant experience if you build on a beefy server with CPUs and RAM. In particular, I don't recommend doing CUDA builds on a laptop; building CUDA is sloooooow and laptops tend to not have enough juice to turnaround quickly enough.\n\nSo that's it for a whirlwind tour of PyTorch's internals! Many, many things\nhave been omitted; but hopefully the descriptions and explanations here can\nhelp you get a grip on at least a substantial portion of the codebase.\n\nWhere should you go from here? What kinds of contributions can you make? A\ngood place to start is our issue tracker. Starting earlier this year, we have\nbeen triaging issues; issues labeled triaged mean that at least one PyTorch\ndeveloper has looked at it and made an initial assessment about the issue. You\ncan use these labels to find out what issues we think are high priority or\nlook up issues specific to some module, e.g., autograd or find issues which we\nthink are small (word of warning: we're sometimes wrong!)\n\nEven if you don't want to get started with coding right away, there are many\nother useful activities like improving documentation (I love merging\ndocumentation PRs, they are so great), helping us reproduce bug reports from\nother users, and also just helping us discuss RFCs on the issue tracker.\nPyTorch would not be where it is today without our open source contributors;\nwe hope you can join us too!\n\n  * May 16, 2019\n  * PyTorch\n\n### 55 Responses to \u201cPyTorch internals\u201d\n\n  1. wynne says:\n\nNovember 7, 2023 at 2:41 am\n\nwish a better doc about pytorch internel\n\n  2. Benjamin says:\n\nNovember 9, 2023 at 10:41 pm\n\nHi Edward, is there any systematic docs on pytorch internals?\n\n  3. zuko says:\n\nNovember 29, 2023 at 6:14 pm\n\nthanks for the writeup :)\n\n  4. Avishake says:\n\nJanuary 26, 2024 at 7:36 am\n\nThanks for the amazing blog post that we have here. It was really very\ninteresting. :)\n\n  5. dosmas says:\n\nMarch 21, 2024 at 12:28 am\n\nNo one knows why c10 is c10: either it is Caffe TENsor, or Core TENsor, or\nCaffe 10 (in binary).\n\n\u00ab Previous Post Next Post \u00bb\n\n\u00a9 ezyang\u2019s blog. Powered by WordPress, theme based off of Ashley.\n\n", "frontpage": false}
