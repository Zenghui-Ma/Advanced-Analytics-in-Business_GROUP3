{"aid": "40098312", "title": "Optimizing RTC bandwidth estimation with machine learning", "url": "https://engineering.fb.com/2024/03/20/networking-traffic/optimizing-rtc-bandwidth-estimation-machine-learning/", "domain": "fb.com", "votes": 1, "user": "jayFu", "posted_at": "2024-04-20 15:59:05", "comments": 0, "source_title": "Optimizing RTC bandwidth estimation with machine learning", "source_text": "Optimizing RTC bandwidth estimation with machine learning - Engineering at\nMeta\n\nSkip to content\n\nPOSTED ON MARCH 20, 2024 TO ML Applications, Networking & Traffic, Video\nEngineering\n\n#\n\nOptimizing RTC bandwidth estimation with machine learning\n\nBy Santhosh Sunderrajan, Liyan Liu\n\n  * Bandwidth estimation (BWE) and congestion control play an important role in delivering high-quality real-time communication (RTC) across Meta\u2019s family of apps.\n  * We\u2019ve adopted a machine learning (ML)-based approach that allows us to solve networking problems holistically across cross-layers such as BWE, network resiliency, and transport.\n  * We\u2019re sharing our experiment results from this approach, some of the challenges we encountered during execution, and learnings for new adopters.\n\nOur existing bandwidth estimation (BWE) module at Meta is based on WebRTC\u2019s\nGoogle Congestion Controller (GCC). We have made several improvements through\nparameter tuning, but this has resulted in a more complex system, as shown in\nFigure 1.\n\nFigure 1: BWE module\u2019s system diagram for congestion control in RTC.\n\nOne challenge with the tuned congestion control (CC)/BWE algorithm was that it\nhad multiple parameters and actions that were dependent on network conditions.\nFor example, there was a trade-off between quality and reliability; improving\nquality for high-bandwidth users often led to reliability regressions for low-\nbandwidth users, and vice versa, making it challenging to optimize the user\nexperience for different network conditions.\n\nAdditionally, we noticed some inefficiencies in regards to improving and\nmaintaining the module with the complex BWE module:\n\n  1. Due to the absence of realistic network conditions during our experimentation process, fine-tuning the parameters for user clients necessitated several attempts.\n  2. Even after the rollout, it wasn\u2019t clear if the optimized parameters were still applicable for the targeted network types.\n  3. This resulted in complex code logics and branches for engineers to maintain.\n\nTo solve these inefficiencies, we developed a machine learning (ML)-based,\nnetwork-targeting approach that offers a cleaner alternative to hand-tuned\nrules. This approach also allows us to solve networking problems holistically\nacross cross-layers such as BWE, network resiliency, and transport.\n\n## Network characterization\n\nAn ML model-based approach leverages time series data to improve the bandwidth\nestimation by using offline parameter tuning for characterized network types.\n\nFor an RTC call to be completed, the endpoints must be connected to each other\nthrough network devices. The optimal configs that have been tuned offline are\nstored on the server and can be updated in real-time. During the call\nconnection setup, these optimal configs are delivered to the client. During\nthe call, media is transferred directly between the endpoints or through a\nrelay server. Depending on the network signals collected during the call, an\nML-based approach characterizes the network into different types and applies\nthe optimal configs for the detected type.\n\nFigure 2 illustrates an example of an RTC call that\u2019s optimized using the ML-\nbased approach.\n\nFigure 2: An example RTC call configuration with optimized parameters\ndelivered from the server and based on the current network type.\n\n## Model learning and offline parameter tuning\n\nOn a high level, network characterization consists of two main components, as\nshown in Figure 3. The first component is offline ML model learning using ML\nto categorize the network type (random packet loss versus bursty loss). The\nsecond component uses offline simulations to tune parameters optimally for the\ncategorized network type.\n\nFigure 3: Offline ML-model learning and parameter tuning.\n\nFor model learning, we leverage the time series data (network signals and non-\npersonally identifiable information, see Figure 6, below) from production\ncalls and simulations. Compared to the aggregate metrics logged after the\ncall, time series captures the time-varying nature of the network and\ndynamics. We use FBLearner, our internal AI stack, for the training pipeline\nand deliver the PyTorch model files on demand to the clients at the start of\nthe call.\n\nFor offline tuning, we use simulations to run network profiles for the\ndetected types and choose the optimal parameters for the modules based on\nimprovements in technical metrics (such as quality, freeze, and so on.).\n\n## Model architecture\n\nFrom our experience, we\u2019ve found that it\u2019s necessary to combine time series\nfeatures with non-time series (i.e., derived metrics from the time window) for\na highly accurate modeling.\n\nTo handle both time series and non-time series data, we\u2019ve designed a model\narchitecture that can process input from both sources.\n\nThe time series data will pass through a long short-term memory (LSTM) layer\nthat will convert time series input into a one-dimensional vector\nrepresentation, such as 16\u00d71. The non-time series data or dense data will pass\nthrough a dense layer (i.e., a fully connected layer). Then the two vectors\nwill be concatenated, to fully represent the network condition in the past,\nand passed through a fully connected layer again. The final output from the\nneural network model will be the predicted output of the target/task, as shown\nin Figure 4.\n\nFigure 4: Combined-model architecture with LSTM and Dense Layers\n\n## Use case: Random packet loss classification\n\nLet\u2019s consider the use case of categorizing packet loss as either random or\ncongestion. The former loss is due to the network components, and the latter\nis due to the limits in queue length (which are delay dependent). Here is the\nML task definition:\n\nGiven the network conditions in the past N seconds (10), and that the network\nis currently incurring packet loss, the goal is to characterize the packet\nloss at the current timestamp as RANDOM or not.\n\nFigure 5 illustrates how we leverage the architecture to achieve that goal:\n\nFigure 5: Model architecture for a random packet loss classification task.\n\n### Time series features\n\nWe leverage the following time series features gathered from logs:\n\nFigure 6: Time series features used for model training.\n\n### BWE optimization\n\nWhen the ML model detects random packet loss, we perform local optimization on\nthe BWE module by:\n\n  * Increasing the tolerance to random packet loss in the loss-based BWE (holding the bitrate).\n  * Increasing the ramp-up speed, depending on the link capacity on high bandwidths.\n  * Increasing the network resiliency by sending additional forward-error correction packets to recover from packet loss.\n\n## Network prediction\n\nThe network characterization problem discussed in the previous sections\nfocuses on classifying network types based on past information using time\nseries data. For those simple classification tasks, we achieve this using the\nhand-tuned rules with some limitations. The real power of leveraging ML for\nnetworking, however, comes from using it for predicting future network\nconditions.\n\nWe have applied ML for solving congestion-prediction problems for optimizing\nlow-bandwidth users\u2019 experience.\n\n## Congestion prediction\n\nFrom our analysis of production data, we found that low-bandwidth users often\nincur congestion due to the behavior of the GCC module. By predicting this\ncongestion, we can improve the reliability of such users\u2019 behavior. Towards\nthis, we addressed the following problem statement using round-trip time (RTT)\nand packet loss:\n\nGiven the historical time-series data from production/simulation (\u201cN\u201d\nseconds), the goal is to predict packet loss due to congestion or the\ncongestion itself in the next \u201cN\u201d seconds; that is, a spike in RTT followed by\na packet loss or a further growth in RTT.\n\nFigure 7 shows an example from a simulation where the bandwidth alternates\nbetween 500 Kbps and 100 Kbps every 30 seconds. As we lower the bandwidth, the\nnetwork incurs congestion and the ML model predictions fire the green spikes\neven before the delay spikes and packet loss occur. This early prediction of\ncongestion is helpful in faster reactions and thus improves the user\nexperience by preventing video freezes and connection drops.\n\nFigure 7: Simulated network scenario with alternating bandwidth for congestion\nprediction\n\n## Generating training samples\n\nThe main challenge in modeling is generating training samples for a variety of\ncongestion situations. With simulations, it\u2019s harder to capture different\ntypes of congestion that real user clients would encounter in production\nnetworks. As a result, we used actual production logs for labeling congestion\nsamples, following the RTT-spikes criteria in the past and future windows\naccording to the following assumptions:\n\n  * Absent past RTT spikes, packet losses in the past and future are independent.\n  * Absent past RTT spikes, we cannot predict future RTT spikes or fractional losses (i.e., flosses).\n\nWe split the time window into past (4 seconds) and future (4 seconds) for\nlabeling.\n\nFigure 8: Labeling criteria for congestion prediction\n\n## Model performance\n\nUnlike network characterization, where ground truth is unavailable, we can\nobtain ground truth by examining the future time window after it has passed\nand then comparing it with the prediction made four seconds earlier. With this\nlogging information gathered from real production clients, we compared the\nperformance in offline training to online data from user clients:\n\nFigure 9: Offline versus online model performance comparison.\n\n## Experiment results\n\nHere are some highlights from our deployment of various ML models to improve\nbandwidth estimation:\n\n### Reliability wins for congestion prediction\n\nconnection_drop_rate -0.326371 +/- 0.216084 last_minute_quality_regression_v1\n-0.421602 +/- 0.206063 last_minute_quality_regression_v2 -0.371398 +/-\n0.196064 bad_experience_percentage -0.230152 +/- 0.148308\ntransport_not_ready_pct -0.437294 +/- 0.400812\n\npeer_video_freeze_percentage -0.749419 +/- 0.180661\npeer_video_freeze_percentage_above_500ms -0.438967 +/- 0.212394\n\n### Quality and user engagement wins for random packet loss characterization\nin high bandwidth\n\npeer_video_freeze_percentage -0.379246 +/- 0.124718\npeer_video_freeze_percentage_above_500ms -0.541780 +/- 0.141212\npeer_neteq_plc_cng_perc -0.242295 +/- 0.137200\n\ntotal_talk_time 0.154204 +/- 0.148788\n\n### Reliability and quality wins for cellular low bandwidth classification\n\nconnection_drop_rate -0.195908 +/- 0.127956 last_minute_quality_regression_v1\n-0.198618 +/- 0.124958 last_minute_quality_regression_v2 -0.188115 +/-\n0.138033\n\npeer_neteq_plc_cng_perc -0.359957 +/- 0.191557 peer_video_freeze_percentage\n-0.653212 +/- 0.142822\n\n### Reliability and quality wins for cellular high bandwidth classification\n\navg_sender_video_encode_fps 0.152003 +/- 0.046807 avg_sender_video_qp\n-0.228167 +/- 0.041793 avg_video_quality_score 0.296694 +/- 0.043079\navg_video_sent_bitrate 0.430266 +/- 0.092045\n\n## Future plans for applying ML to RTC\n\nFrom our project execution and experimentation on production clients, we\nnoticed that a ML-based approach is more efficient in targeting, end-to-end\nmonitoring, and updating than traditional hand-tuned rules for networking.\nHowever, the efficiency of ML solutions largely depends on data quality and\nlabeling (using simulations or production logs). By applying ML-based\nsolutions to solving network prediction problems \u2013 congestion in particular \u2013\nwe fully leveraged the power of ML.\n\nIn the future, we will be consolidating all the network characterization\nmodels into a single model using the multi-task approach to fix the\ninefficiency due to redundancy in model download, inference, and so on. We\nwill be building a shared representation model for the time series to solve\ndifferent tasks (e.g., bandwidth classification, packet loss classification,\netc.) in network characterization. We will focus on building realistic\nproduction network scenarios for model training and validation. This will\nenable us to use ML to identify optimal network actions given the network\nconditions. We will persist in refining our learning-based methods to enhance\nnetwork performance by considering existing network signals.\n\n### Share this:\n\n  * Facebook\n  * Threads\n  * X\n  * LinkedIn\n  * Hacker News\n  * Email\n\nPrev Better video for mobile RTC with AV1 and HD\n\nNext Threads has entered the fediverse\n\n### Read More in ML Applications\n\nView All\n\nAPR 10, 2024\n\nIntroducing the next-gen Meta Training and Inference Accelerator\n\nMAR 18, 2024\n\nLogarithm: A logging engine for AI training workflows and services\n\nMAR 12, 2024\n\nBuilding Meta\u2019s GenAI Infrastructure\n\nJAN 29, 2024\n\nImproving machine learning iteration speed with faster application build and\npackaging\n\nJAN 18, 2024\n\nLazy is the new fast: How Lazy Imports and Cinder accelerate machine learning\nat Meta\n\nJAN 11, 2024\n\nHow Meta is advancing GenAI\n\n### Related Posts\n\nNov 15, 2023\n\n#### Watch: Meta\u2019s engineers on building network infrastructure for AI\n\nJan 27, 2023\n\n#### Watch Meta\u2019s engineers discuss optimizing large-scale networks\n\nDec 19, 2023\n\n#### AI debugging at Meta with HawkEye\n\n### Related Positions\n\n  * Software Engineer, Product\n\nTEL AVIV, ISRAEL\n\n  * Software Engineer - Infrastructure\n\nZURICH, SWITZERLAND\n\n  * Software Engineer - Infrastructure\n\nTEL AVIV, ISRAEL\n\n  * Software Engineer - Product (Technical Leadership)\n\nMENLO PARK, US\n\n  * Software Engineer - Product (Technical Leadership)\n\nSEATTLE, US\n\nSee All Jobs\n\n### Available Positions\n\n  * Software Engineer, Product\n\nTEL AVIV, ISRAEL\n\n  * Software Engineer - Infrastructure\n\nZURICH, SWITZERLAND\n\n  * Software Engineer - Infrastructure\n\nTEL AVIV, ISRAEL\n\n  * Software Engineer - Product (Technical Leadership)\n\nMENLO PARK, US\n\n  * Software Engineer - Product (Technical Leadership)\n\nSEATTLE, US\n\nSee All Jobs\n\n### Stay Connected\n\n  * Engineering at Meta\n\n  * Meta Open Source\n\n  * Meta Research\n\n  * Meta for Developers\n\n  * RSS\n\n### Open Source\n\nMeta believes in building community through open source technology. Explore\nour latest projects in Artificial Intelligence, Data Infrastructure,\nDevelopment Tools, Front End, Languages, Platforms, Security, Virtual Reality,\nand more.\n\n  * ANDROID\n\n  * iOS\n\n  * WEB\n\n  * BACKEND\n\n  * HARDWARE\n\nEngineering at Meta is a technical news resource for engineers interested in\nhow we solve large-scale technical challenges at Meta.\n\n  * Home\n  * Company Info\n  * Careers\n\n\u00a9 2024 Meta\n\n  * Terms\n  * Privacy\n  * Cookies\n  * Help\n\nTo help personalize content, tailor and measure ads and provide a safer\nexperience, we use cookies. By clicking or navigating the site, you agree to\nallow our collection of information on and off Facebook through cookies. Learn\nmore, including about available controls: Cookie Policy\n\n", "frontpage": false}
