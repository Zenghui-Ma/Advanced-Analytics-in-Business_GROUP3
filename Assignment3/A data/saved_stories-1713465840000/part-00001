{"aid": "40075583", "title": "Memory Management in Mpmetrics", "url": "https://blog.trends.tf/memory-management-in-mpmetrics.html", "domain": "trends.tf", "votes": 1, "user": "todsacerdoti", "posted_at": "2024-04-18 12:41:36", "comments": 0, "source_title": "pair of pared pears | Memory management in mpmetrics", "source_text": "pair of pared pears | Memory management in mpmetrics\n\n# pair of pared pears\n\n#### Apr 17, 2024\n\n## Memory management in mpmetrics\n\nThis article is an extended, semi-literate overview of the memory management\nin mpmetrics. I think there are a lot of neat little details in the design of\nthis library that lend themselves better to a more guided exposition than API\ndocumentation. I\u2019ve made a few simplifications for didactic reasons, but the\ncode otherwise closely mirrors the actual implementation.\n\nIf you\u2019re not familiar with mpmetrics, check out my introduction to mpmetrics.\nThe problem focused on in this post is dynamic allocation of variables backed\nby shared memory... in Python. It\u2019s a tough challenge since, in many ways,\nPython is the wrong language for this kind of task. However its dynamic and\nflexible nature allow unusual and satisfying solutions to many challenges.\n\n## In a pickle\n\nIn order to store data in a structured fashion, we are going to need some\ntypes backed by shared memory. Let\u2019s start by wrapping c_int64:\n\n    \n    \n    import ctypes class Int64: size = ctypes.sizeof(ctypes.c_int64) def __init__(self, mem): self._mem = mem self._value = ctypes.c_int64.from_buffer(mem) self._value.value = 0\n\nand proxy all of our attributes (except _value) onto c_int64:\n\n    \n    \n    def __getattr__(self, name): return getattr(self.__dict__['_value'], name) def __setattr__(self, name, value): if '_value' in self.__dict__: setattr(self.__dict__['_value'], name, value) else: self.__dict__[name] = value def __delattr__(self, name): delattr(self.__dict__['_value'], name)\n\nLets try it out:\n\n    \n    \n    >>> mem = bytearray(Int64.size) >>> x = Int64(mem) >>> x.value 0 >>> x.value += 1 >>> x.value 1 >>> mem bytearray(b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00')\n\nAs I\u2019m on a little-endian system, the least-significant byte comes first. We\ncan even pickle and unpickle it as long as we define some helpers:\n\n    \n    \n    def __getstate__(self): return self._mem def __setstate__(self, mem): self._mem = mem self._value = ctypes.c_int64.from_buffer(mem)\n\nContinuing our example from above,\n\n    \n    \n    >>> import pickle >>> pickle.loads(pickle.dumps(x)) >>> # No errors :)\n\nThis is an important feature, since with the forkserver and spawn start\nmethods, all objects are pickled when passing them to the subprocess.\n\nThat went well, so lets try and tackle an array next. The size of the array\nwill depend on the element class, and the number of elements. In order to keep\nthe size as a class attribute (so we know how much memory we need to\nallocate), we create a new class for each type of array:\n\n    \n    \n    def Array(cls, n): class Array: size = cls.size * n def __init__(self, mem): self._mem = mem self._vals = [] for i in range(n): off = i * cls.size self._vals.append(cls(mem[off:off + cls.size]))\n\nWe can also define some extra methods to make our class behave more like an\narray:\n\n    \n    \n    def __len__(self): return n def __getitem__(self, key): return self._vals[key] def __iter__(self): return iter(self._vals)\n\nas well as some helpers for pickling:\n\n    \n    \n    def __getstate__(self): return self._mem def __setstate__(self, mem): self._mem = mem self._vals = [] for i in range(n): off = i * cls.size val = cls.__new__(cls) val.__setstate__(self._mem[off:off + cls.size]) self._vals.append(val) return Array\n\nLet\u2019s try it out:\n\n    \n    \n    >>> IntArray5 = Array(Int64, 5) >>> a = IntArray5(bytearray(IntArray5.size)) >>> a[0].value = 5 >>> a[0].value += 10 >>> a[0].value 15\n\nBut there\u2019s a problem when pickling:\n\n    \n    \n    >>> pickle.dumps(a) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> AttributeError: Can't pickle local object 'Array.<locals>.Array'\n\nThe problem is that pickle uses the type\u2019s __qualname__ to identify the class\nto use when unpickling. We can see this if we disassemble our pickle from\nearlier:\n\n    \n    \n    >>> import pickletools >>> pickletools.dis(pickletools.optimize(pickle.dumps(x))) 0: \\x80 PROTO 4 2: \\x95 FRAME 56 11: \\x8c SHORT_BINUNICODE '__main__' 21: \\x8c SHORT_BINUNICODE 'Int64' 28: \\x93 STACK_GLOBAL 29: ) EMPTY_TUPLE 30: \\x81 NEWOBJ 31: \\x8c SHORT_BINUNICODE 'builtins' 41: \\x8c SHORT_BINUNICODE 'bytearray' 52: \\x93 STACK_GLOBAL 53: C SHORT_BINBYTES b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00' 63: \\x85 TUPLE1 64: R REDUCE 65: b BUILD 66: . STOP\n\nBut since we create a new class every time we call Array, we can\u2019t identify\nthe class we created this way, since pickle has no way to tell what the\narguments were to Array. We could rewrite Array to take cls and n as arguments\nto __init__, but then we wouldn\u2019t know how much memory to allocate.\n\nWhat we need is a way to record the arguments to Array so that we can create\nthe correct class when unpickling. But the only thing we have to work with is\nthe __qualname__\n\nWhat if we store the arguments to Array in the class name itself?\n\n### The trick\n\nImagine for a moment that we just want to create Int64 Arrays, and we only\nneed to store the length. We could create an object like\n\n    \n    \n    class IntType: def __init__(self, name, cls): self.__qualname__ = name self.name = name self.cls = cls def __getattr__(self, attr): return self.cls(self.name + '.' + attr, int(attr))\n\nThe usage is perhaps best-demonstrated by example:\n\n    \n    \n    >>> test = IntType('test', lambda *args: args) >>> getattr(test, '5') ('test.5', 5)\n\nThe first argument to the function is the path we used to access that\nattribute, and the second is the value of the attribute. Now we can use this\nto create a new IntArray:\n\n    \n    \n    def _IntArray(__name__, n): cls = Int64 size = cls.size * n ... return type(__name__, (), locals()) IntArray = IntType('IntArray', _IntArray)\n\nWe need to call the three-argument type instead of using the class keyword,\nsince the name of the class we create will change based on n. Let\u2019s try using\nthis class again\n\n    \n    \n    >>> IntArray5 = getattr(IntArray, '5') >>> a = IntArray5(bytearray(IntArray5.size)) >>> a[0].value = 5 >>> a[0].value += 10 >>> a[0].value 15\n\nLooking good so far. Let\u2019s try pickling it\n\n    \n    \n    >>> pickle.dumps(a) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> _pickle.PicklingError: Can't pickle <class '__main__.IntArray.5'>: it's not the same object as __main__.IntArray.5\n\nWhoops. The problem is that every time we call _IntArray we create a new\nclass. This is pretty easy to solve by wrapping __getattr__ in a decorator\nwhich saves its return value:\n\n    \n    \n    def saveattr(get): def wrapped(self, name): attr = get(self, name) setattr(self, name, attr) return attr return wrapped\n\nPython won\u2019t bother calling __getattr__ if the relevant attribute is already\npresent in __dict__. Lets try pickling again:\n\n    \n    \n    >>> pickle.loads(pickle.dumps(a))\n\nSuccess!\n\n## Further objectives\n\nThat was a nice warm up. Let\u2019s try something more challenging. What if instead\nof a known element type and an unknown length, we tried making a class with an\nunknown object type and a fixed length:\n\n    \n    \n    def _Array5(__name__, cls): n = 5 size = cls.size * n # ... return type(__name__, (), locals()) Array5 = ObjectType('Array5', _Array5)\n\nSo what should the name of IntArray5 be? Well, perhaps the most obvious thing\nwould be\n\n    \n    \n    Array5.__main__.Int64\n\nbut with this kind of name we wouldn\u2019t know when the name of the object\nstarted and when it ended. This would prevent us from nesting multiple\nObjectTypes. So let\u2019s use this name instead:\n\n    \n    \n    Array5.<.__main__.Int64.>\n\nThe base ObjectType just needs to have a < attribute:\n\n    \n    \n    class ObjectType: def __init__(self, name, cls): self.__qualname__ = name setattr(self, '<', self.Module(name + '.<', cls))\n\nThis attribute\u2019s job is to parse the module portion of the class\u2019s name. We do\nthis by repeatedly trying to import the next attribute as a module:\n\n    \n    \n    class Module: def __init__(self, name, cls, parent=None): self.name = name self.cls = cls self.parent = parent @saveattr def __getattr__(self, name): try: if self.parent: module = self.parent.__name__ + '.' + name else: module = name return type(self)(self.name, self.cls, importlib.import_module(module)) except ModuleNotFoundError: if self.parent: prefix = self.name + '.' + self.parent.__name__ else: prefix = self.name return ObjectType.Attr(prefix, self.cls, getattr(self.parent, name))\n\nFor example, say that we have a file a/b.py and inside that file we have a\nclass C. When we access Array5.<.a.b.C, we will have\n\n    \n    \n    Array5: ObjectType('Array5', _Array5) <: ObjectType.Module('Array5.<', _Array5, None) a: ObjectType.Module('Array5.<.a', _Array5, a) b: ObjectType.Module('Array5.<.a.b', _Array5, a.b) C: ObjectType.Attr('Array5.<.a.b.C', _Array5, a.b.C)\n\nAt this point, we\u2019ve gotten through the modules and finally made it to an\nobject. Now we need to walk its attributes:\n\n    \n    \n    class Attr: def __init__(self, name, cls, obj, nesting=1): self.name = name self.cls = cls self.obj = obj self.nesting = nesting @saveattr def __getattr__(self, name): nesting = self.nesting + (name == '<') - (name == '>') if name == '>' and not nesting: return self.cls(self.name + '.' + self.obj.__qualname__ + '.>', self.obj) else: return type(self)(self.name, self.cls, getattr(self.obj, name), nesting)\n\nTo continuing the above example, say that class C has a nested class D. When\nwe access Array5.<.a.b.C.D.> we will have\n\n    \n    \n    Array5: ObjectType('Array5', _Array5) <: ObjectType.Module('Array5.<', _Array5, None) a: ObjectType.Module('Array5.<.a', _Array5, a) b: ObjectType.Module('Array5.<.a.b', _Array5, a.b) C: ObjectType.Attr('Array5.<.a.b.C', _Array5, a.b.C, 1) D: ObjectType.Attr('Array5.<.a.b.C.D', _Array5, a.b.C.D, 1) >: _Array5('Array5.<.a.b.C.D.>', a.b.C.D)\n\nThe nesting attribute helps us keep track of nested objects. For example say\nwe wanted to create an Array5 of an Array5 of Int64s:\n\n    \n    \n    Array5: ObjectType('Array5', _Array5) <: ObjectType.Module('Array5.<', _Array5, None) __main__: ObjectType.Module('Array5.<.__main__', _Array5, __main__) Array5: ObjectType.Attr('Array5.<.__main__.Array5', _Array5, __main__.Array5, 1) <: ObjectType.Attr('Array5.<.__main__.Array5.<', _Array5, __main__.Array5.<, 2) __main__: ObjectType.Attr('Array5.<.__main__.Array5.<.__main__', _Array5, __main__.Array5.<.__main__, 2) Int64: ObjectType.Attr('Array5.<.__main__.Array5.<.__main__.Int64', _Array5, __main__.Array5.<.__main__.Int64, 2) >: ObjectType.Attr('Array5.<.__main__.Array5.<.__main__.Int64.>', _Array5, __main__.Array5.<.__main__.Int64.>, 1) >: _Array5('Array5.<.__main__.Array5.<.__main__.Int64.>.>', __main__.Array5.<.__main__.Int64.>.>)\n\nOf course, this also means that < and > are special, and you can\u2019t include\nunmatched brackets in your class hierarchy (like a certain ticklish language).\nA more robust system could prefix the type with the number attributes in the\ntype:\n\n    \n    \n    Array5.6.__main__.Array5.2.__main__.Int64\n\nbut I like the aesthetics of angle brackets more. Speaking of which, to\nactually access the above class name, we\u2019d have to type out something like\n\n    \n    \n    >>> getattr(getattr(getattr(getattr(Array5, '<').__main__.Array5, '<').__main__.Int64, '>'), '>') <class '__main__.Array5.<.__main__.Array5.<.__main__.Int64.>.>'>\n\nThis is a real pain. Let\u2019s add a helper to ObjectType:\n\n    \n    \n    def __getitem__(self, cls): parent = getattr(self, '<') for subpath in itertools.chain(cls.__module__.split('.'), cls.__qualname__.split('.')): parent = getattr(parent, subpath) return getattr(parent, '>')\n\nNow we can do\n\n    \n    \n    >>> Array5[Array5[Int64]] <class '__main__.Array5.<.__main__.Array5.<.__main__.Int64.>.>'>\n\nMuch better.\n\n### A product of necessity\n\nIntArrays and Array5s are all well and good, but what we really want is an\nArray where we can specify both the element type and the length. Since we\nalready have an IntType and ObjectType, we can combine them together with a\nProductType\n\n    \n    \n    class ProductType: def __init__(self, name, cls, argtypes, args=()): self.__qualname__ = name self.name = name self.cls = cls self.argtype = argtypes[0](self.name, self._chain) self.argtypes = argtypes[1:] self.args = args def _chain(self, name, arg): if self.argtypes: return type(self)(name, self.cls, self.argtypes, (*self.args, arg)) return self.cls(name, *self.args, arg) @saveattr def __getattr__(self, name): return getattr(self.argtype, name) # __getitem__ omitted for brevity\n\nInstead of constructing the class immediately, as before, we instead _chain\ninto the next argtype. With this, we can now redefine Array:\n\n    \n    \n    def _Array(__name__, cls, n): size = cls.size * n # ... return type(__name__, (), locals()) Array = ProductType('Array', _Array, (ObjectType, IntType))\n\nWhen we access something like Array[Int64, 5], the attributes will look like:\n\n    \n    \n    Array: ProductType('Array', _Array, (ObjectType, IntType)) <: ObjectType.Module('Array.<', Array._chain, None) __main__: ObjectType.Module('Array5.<.__main__', Array._chain, __main__) Int64: ObjectType.Attr('Array5.<.__main__.Int64', Array._chain, __main__.Int64) >: IntType('Array5.<.__main__.Int64.>', Array[Int64]._chain) 5: _Array('Array5.<.__main__.Int64.>.5', Int64, 5)\n\nAnd we can finally pickle and unpickle:\n\n    \n    \n    >>> IntArray5 = Array[Int64, 5] >>> a = IntArray5(bytearray(IntArray5.size)) >>> pickle.loads(pickle.dumps(a))\n\nThere\u2019s a problem though:\n\n    \n    \n    >>> a[0].value = 15 >>> pickle.loads(pickle.dumps(a))[0].value 0\n\nThis is because when we slice a bytearray, we get a new bytearray with a copy\nof the original bytearray's contents. We can get around this by using a\nmemoryview:\n\n    \n    \n    >>> a = IntArray5(memoryview(bytearray(IntArray5.size))) >>> a[0].value = 15 >>> bytearray(a[0]._mem) bytearray(b'\\x0f\\x00\\x00\\x00\\x00\\x00\\x00\\x00') >>> bytearray(a._mem)[0:8] bytearray(b'\\x0f\\x00\\x00\\x00\\x00\\x00\\x00\\x00')\n\nBut we can\u2019t pickle it:\n\n    \n    \n    >>> pickle.dumps(a) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> TypeError: cannot pickle 'memoryview' object\n\nIt\u2019s time to actually start working with shared memory.\n\n## Malloc madness\n\nThe first thing we need need is a bulk source of shared memory. Unfortunately,\nwe cannot use multiprocessing.shared_memory because we can\u2019t expand its memory\nlater. Metrics can be created at any point in the application\u2019s lifetime, and\nwe don\u2019t necessarily know how many we will need when we have to create the\nfirst metric. For example, adding a label creates a new copy of a metric for\nthat label, and it\u2019s common to generate labels dynamically based on endpoints\nor status codes.\n\nInstead, we open a TemporaryFile and truncate it as necessary to extend it.\n\n    \n    \n    import os import mmap from tempfile import TemporaryFile class Heap: def __init__(self): # File backing our shared memory self._file = TemporaryFile() self._fd = self._file.fileno() # Allocate a page to start with os.truncate(self._fd, 4096) # Keep track of the memory we've mapped self._maps = [mmap.mmap(self._fd, map_size)] # Initialize a base pointer with the memory we just mapped self._base = Int64(memoryview(self._maps[0])[:self.size]) # And make sure we don't reuse that memory later self._base.value = self._base.size\n\nWe\u2019re going to be making a basic \"bump\" style allocator. The algorithm is\nreally simple; in pseudocode it\u2019s just:\n\n    \n    \n    def malloc(size): start = base base += size return start\n\nAlthough it\u2019s a little more complex than that: we need to ensure we have\nenough space in the file and take care when crossing page boundaries.\n\nmmap doesn\u2019t have to return contiguous memory when extending an existing\nmapping. For example, if we made two allocations of size 2048 and 4096, and we\ntried to allocate the first one at offset 0 and the second one at offset 2048,\nthe second allocation would span two pages (ending at byte 6143). If the first\npage was mapped at address 16384, the second page would have to be mapped at\naddress 20480 to ensure a contiguous mapping. But we can\u2019t guarantee that with\nthe mmap API. So instead, we round up to the next page boundary if we would\notherwise cross it.\n\nAllocations larger than a single page always cross page boundaries no matter\nhow we align things. To solve this issue, we map all the pages for these\nallocations in one call to mmap, ensuring that we get a contiguous mapping.\nThen, we bump the base address to the next page boundary, ensuring that no\nother allocations will need those pages.\n\nIn detail, if the allocation spans multiple pages, we page-align the size.\n\n    \n    \n    def _align_mask(x, mask): return (x + mask) & ~mask def align(x, a): return _align_mask(x, a - 1) def malloc(self, size): if size > 4096: size = align(size, 4096)\n\nIf we need to allocate a new page, enlarge the file and update the base:\n\n    \n    \n    if self._base.value + size >= total: os.ftruncate(self._fd, align(total + size, 4096)) self._base.value = align(self._base.value, 4096)\n\nAnd finally, we can bump the base pointer and return a new Block:\n\n    \n    \n    start = self._base.value self._base.value += size return Block(self, start, size)\n\nBlock is like a pointer, except it keeps track of how big it is and where it\nwas allocated from.\n\n    \n    \n    import itertools class Block: def __init__(self, heap, start, size): self.heap = heap self.start = start self.size = size\n\nThere\u2019s only one major method, deref, which creates a memoryview. The first\nhalf of this function determines the page(s) we need to access, and what their\noffsets are:\n\n    \n    \n    def deref(self): heap = self.heap first_page = int(self.start / 4096) last_page = int((self.start + self.size - 1) / 4096) nr_pages = last_page - first_page + 1 page_off = first_page * 4096 off = self.start - page_off\n\nWe store our mapped pages in list. Each element is a memoryview of the page,\nor None if we haven\u2019t mapped it yet. To start, we extend the length of our\nlist if it\u2019s not big enough.\n\n    \n    \n    if len(heap._maps) <= last_page: heap._maps.extend(itertools.repeat(None, last_page - len(heap._maps) + 1))\n\nThen, we create a map at the location of the first page. malloc ensures we\nnever have Blocks which cross page boundaries unless they are larger than a\nsingle page. Since multi-page allocations are the only allocations in the\npages they use, we will never try to access the Nones occupying the later\nindices in the list.\n\n    \n    \n    if not self.heap._maps[first_page]: heap._maps[first_page] = mmap.mmap(heap._fd, nr_pages * 4096, offset=page_off)\n\nFinally, we can create a memory view out of the mapped page:\n\n    \n    \n    return memoryview(heap._maps[first_page])[off:off+self.size]\n\nLet\u2019s try it out:\n\n    \n    \n    >>> h = Heap() >>> block = h.malloc(InteArray5.size) >>> a = IntArray5(block.deref()) >>> a[0].value = 15 >>> bytearray(a[0]._mem) bytearray(b'\\x0f\\x00\\x00\\x00\\x00\\x00\\x00\\x00') >>> bytearray(a._mem)[0:8] bytearray(b'\\x0f\\x00\\x00\\x00\\x00\\x00\\x00\\x00')\n\nGood so far, but we still can\u2019t pickle the memoryview:\n\n    \n    \n    >>> pickle.dumps(a) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> TypeError: cannot pickle 'memoryview' object\n\nWhat about pickling the Block, which can create the memoryview from the Heap?\n\n    \n    \n    >>> pickle.dumps(block) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> TypeError: cannot pickle '_io.BufferedRandom' object\n\nNow the problem is that we can\u2019t pickle the open file backing the Heap. And in\ngeneral, there\u2019s no way to pickle an open file since it might not be around\nwhenever another python process gets around to unpickling it. But we just need\nto make pickling work when spawning new processes. As it turns out, the\nmultiprocessing authors had the same problem, and came up with DupFd. We can\nuse it to implement Heap's pickle helpers:\n\n    \n    \n    from multiprocessing.reduction import DupFd def __getstate__(self): return DupFd(self._fd) def __setstate__(self, df): self._fd = df.detach() self._file = open(self._fd, 'a+b') self._maps = [mmap.mmap(self._fd, 4096)] self._base = Int64(memoryview(self._maps[0])[:Int64.size])\n\nUnder the hood, DupFd sets up a UNIX domain server which duplicates the file\ndescriptor, and then sends it to the client when requested. The pickle data is\njust the address of the server:\n\n    \n    \n    >>> pickletools.dis(pickletools.optimize(pickle.dumps(h))) 0: \\x80 PROTO 4 2: \\x95 FRAME 113 11: \\x8c SHORT_BINUNICODE '__main__' 21: \\x8c SHORT_BINUNICODE 'Heap' 27: \\x93 STACK_GLOBAL 28: ) EMPTY_TUPLE 29: \\x81 NEWOBJ 30: \\x8c SHORT_BINUNICODE 'multiprocessing.resource_sharer' 63: \\x8c SHORT_BINUNICODE 'DupFd' 70: \\x93 STACK_GLOBAL 71: ) EMPTY_TUPLE 72: \\x81 NEWOBJ 73: } EMPTY_DICT 74: \\x8c SHORT_BINUNICODE '_id' 79: \\x8c SHORT_BINUNICODE '/tmp/pymp-i7ih27es/listener-a61oo9mt' 117: K BININT1 2 119: \\x86 TUPLE2 120: s SETITEM 121: b BUILD 122: b BUILD 123: . STOP\n\nThe server shuts down after sending the file descriptor, so we can only\nunpickle the heap once. Lets try it out:\n\n    \n    \n    >>> block = h.malloc(IntArray5.size) >>> a = IntArray5(block.deref()) >>> b = IntArray5(pickle.loads(pickle.dumps(block)).deref()) >>> a[0].value = 85 >>> b[0].value 85\n\nSuccess! But wouldn\u2019t it be nice if we could just pickle the array directly?\n\n## Shipping and Receiving\n\nGoing back to Int64, we could rewrite it to take a Heap instead of raw memory:\n\n    \n    \n    class Int64: def __init__(self, heap): self._block = heap.malloc(self.size) self._value = ctypes.c_int64.from_buffer(self._block.deref()) self._value.value = 0 def __getstate__(self): return self._block def __setstate__(self, block): self._block = block self._value = ctypes.c_int64.from_buffer(block.deref()) ...\n\nBut this breaks Array and Heap, since now we no longer have a way to create an\nInt64 from memory. What we really want is a second BoxedInt64 which takes a\nHeap while the regular Int64 still uses memory directly.\n\n    \n    \n    class BoxedInt64(Int64): def __init__(self, heap): self._block = heap.malloc(self.size) super().__init__(self._block.deref()) def __getstate__(self): return self._block def __setstate__(self, block): self._block = block super()._setstate(block.deref())\n\nWhere we implement _setstate in Int64 like\n\n    \n    \n    def _setstate(self, mem): self._value = ctypes.c_int64.from_buffer(mem)\n\nExamining BoxedInt64, you may notice that aside from inheriting from Int64, it\nis otherwise completely generic. In fact, we can create boxed types on the fly\nby creating new subclasses with ObjectType:\n\n    \n    \n    class _Box: # Same as BoxedInt64 Box = ObjectType('Box', lambda name, cls: type(name, (_Box, cls), {}))\n\nWhich we can now use like\n\n    \n    \n    >>> a = Box[Array[Int64, 5]](h) >>> b = pickle.loads(pickle.dumps(a)) >>> a[0].value = 33 >>> b[0].value 33\n\n\u220e\n\n## Epilogue\n\nHopefully this has been an interesting journey through the heart of mpmetrics.\nFor expository purposes, I left out or skipped over many details, such as the\nmany other types, locking, and of course this doesn\u2019t even cover the metrics\nthemselves. If you are interested in more details of how this library works,\ncheck out the mpmetrics internals documentation.\n\nposted at 15:26 \u00b7 mpmetrics \u00b7 python openmetrics\n\nBlue Penguin Theme \u00b7 Powered by Pelican \u00b7 Atom Feed\n\n", "frontpage": false}
