{"aid": "40061490", "title": "Torchtune: Easily fine-tune LLMs using PyTorch", "url": "https://pytorch.org/blog/torchtune-fine-tune-llms/", "domain": "pytorch.org", "votes": 1, "user": "g42gregory", "posted_at": "2024-04-17 07:15:03", "comments": 0, "source_title": "torchtune: Easily fine-tune LLMs using PyTorch", "source_text": "torchtune: Easily fine-tune LLMs using PyTorch | PyTorch\n\n  * Get Started\n  * Ecosystem\n\nPyTorch Conference - 2024\n\nSeptember 18-19 in San Francisco\n\nTools\n\nLearn about the tools and frameworks in the PyTorch Ecosystem\n\n  * Edge\n\nAbout PyTorch Edge ExecuTorch\n\n  * Blog\n  * Tutorials\n  * Docs\n\nPyTorch\n\ntorchaudio\n\ntorchtext\n\ntorchvision\n\ntorcharrow\n\nTorchData\n\nTorchRec\n\nTorchServe\n\nPyTorch on XLA Devices\n\n  * Resources\n\nAbout\n\nLearn about PyTorch\u2019s features and capabilities\n\nPyTorch Foundation\n\nLearn more about the PyTorch Foundation.\n\nCommunity\n\nJoin the PyTorch developer community to contribute, learn, and get your\nquestions answered.\n\nCommunity stories\n\nLearn how our community solves real, everyday machine learning problems with\nPyTorch\n\nDeveloper Resources\n\nFind resources and get questions answered\n\nEvents\n\nFind events, webinars, and podcasts\n\nForums\n\nA place to discuss PyTorch code, issues, install, research\n\nModels (Beta)\n\nDiscover, publish, and reuse pre-trained models\n\n  * GitHub\n  * X\n\nApril 16, 2024\n\n# torchtune: Easily fine-tune LLMs using PyTorch\n\nby Team PyTorch\n\nWe\u2019re pleased to announce the alpha release of torchtune, a PyTorch-native\nlibrary for easily fine-tuning large language models.\n\nStaying true to PyTorch\u2019s design principles, torchtune provides composable and\nmodular building blocks along with easy-to-extend training recipes to fine-\ntune popular LLMs on a variety of consumer-grade and professional GPUs.\n\ntorchtune supports the full fine-tuning workflow from start to finish,\nincluding\n\n  * Downloading and preparing datasets and model checkpoints.\n  * Customizing the training with composable building blocks that support different model architectures, parameter-efficient fine-tuning (PEFT) techniques, and more.\n  * Logging progress and metrics to gain insight into the training process.\n  * Quantizing the model post-tuning.\n  * Evaluating the fine-tuned model on popular benchmarks.\n  * Running local inference for testing fine-tuned models.\n  * Checkpoint compatibility with popular production inference systems.\n\nTo get started, jump right into the code or walk through our many tutorials!\n\n## Why torchtune?\n\nOver the past year there has been an explosion of interest in open LLMs. Fine-\ntuning these state of the art models has emerged as a critical technique for\nadapting them to specific use cases. This adaptation can require extensive\ncustomization from dataset and model selection all the way through to\nquantization, evaluation and inference. Moreover, the size of these models\nposes a significant challenge when trying to fine-tune them on consumer-level\nGPUs with limited memory.\n\nExisting solutions make it hard to add these customizations or optimizations\nby hiding the necessary pieces behind layers of abstractions. It\u2019s unclear how\ndifferent components interact with each other and which of these need to be\nupdated to add new functionality. torchtune empowers developers to adapt LLMs\nto their specific needs and constraints with full control and visibility.\n\n## torchtune\u2019s Design\n\ntorchtune was built with the following principles in mind\n\n  * Easy extensibility - New techniques emerge all the time and everyone\u2019s fine-tuning use case is different. torchtune\u2019s recipes are designed around easily composable components and hackable training loops, with minimal abstraction getting in the way of fine-tuning your fine-tuning. Each recipe is self-contained - no trainers or frameworks, and is designed to be easy to read - less than 600 lines of code!\n  * Democratize fine-tuning - Users, regardless of their level of expertise, should be able to use torchtune. Clone and modify configs, or get your hands dirty with some code! You also don\u2019t need beefy data center GPUs. Our memory efficient recipes have been tested on machines with a single 24GB gaming GPU.\n  * Interoperability with the OSS LLM ecosystem - The open source LLM ecosystem is absolutely thriving, and torchtune takes advantage of this to provide interoperability with a wide range of offerings. This flexibility puts you firmly in control of how you train and use your fine-tuned models.\n\nOver the next year, open LLMs will become even more powerful, with support for\nmore languages (multilingual), more modalities (multimodal) and more tasks. As\nthe complexity of these models increases, we need to pay the same attention to\n\u201chow\u201d we design our libraries as we do to the features provided or performance\nof a training run. Flexibility will be key to ensuring the community can\nmaintain the current pace of innovation, and many libraries/tools will need to\nplay well with each other to power the full spectrum of use cases. torchtune\nis built from the ground up with this future in mind.\n\nIn the true PyTorch spirit, torchtune makes it easy to get started by\nproviding integrations with some of the most popular tools for working with\nLLMs.\n\n  * Hugging Face Hub - Hugging Face provides an expansive repository of open source models and datasets for fine-tuning. torchtune seamlessly integrates through the tune download CLI command so you can get started right away with fine-tuning your first model.\n  * PyTorch FSDP - Scale your training using PyTorch FSDP. It is very common for people to invest in machines with multiple consumer level cards like the 3090/4090 by NVidia. torchtune allows you to take advantage of these setups by providing distributed recipes powered by FSDP.\n  * Weights & Biases - torchtune uses the Weights & Biases AI platform to log metrics and model checkpoints during training. Track your configs, metrics and models from your fine-tuning runs all in one place!\n  * EleutherAI\u2019s LM Evaluation Harness - Evaluating fine-tuned models is critical to understanding whether fine-tuning is giving you the results you need. torchtune includes a simple evaluation recipe powered by EleutherAI\u2019s LM Evaluation Harness to provide easy access to a comprehensive suite of standard LLM benchmarks. Given the importance of evaluation, we will be working with EleutherAI very closely in the next few months to build an even deeper and more \u201cnative\u201d integration.\n  * ExecuTorch - Models fine-tuned with torchtune can be easily exported to ExecuTorch, enabling efficient inference to be run on a wide variety of mobile and edge devices.\n  * torchao - Easily and efficiently quantize your fine-tuned models into 4-bit or 8-bit using a simple post-training recipe powered by the quantization APIs from torchao.\n\n## What\u2019s Next?\n\nThis is just the beginning and we\u2019re really excited to put this alpha version\nin front of a vibrant and energetic community. In the coming weeks, we\u2019ll\ncontinue to augment the library with more models, features and fine-tuning\ntechniques. We\u2019d love to hear any feedback, comments or feature requests in\nthe form of GitHub issues on our repository, or on our Discord channel. As\nalways, we\u2019d love any contributions from this awesome community. Happy Tuning!\n\n## Docs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\n\n## Tutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\n\n## Resources\n\nFind development resources and get your questions answered\n\nView Resources\n\n  * PyTorch\n  * Get Started\n  * Features\n  * Ecosystem\n  * Blog\n  * Contributing\n  * Security\n\n  * Resources\n  * Tutorials\n  * Docs\n  * Discuss\n  * GitHub Issues\n  * Brand Guidelines\n\n  * Stay up to date\n\n  * Facebook\n  * Twitter\n  * YouTube\n  * LinkedIn\n  * Mastodon\n\n  * PyTorch Podcasts\n\n  * Spotify\n  * Apple\n  * Google\n  * Amazon\n\n  * Terms\n  * |\n  * Privacy\n\n\u00a9 Copyright The Linux Foundation. The PyTorch Foundation is a project of The\nLinux Foundation. For web site terms of use, trademark policy and other\npolicies applicable to The PyTorch Foundation please see\nwww.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch\nopen source project, which has been established as PyTorch Project a Series of\nLF Projects, LLC. For policies applicable to the PyTorch Project a Series of\nLF Projects, LLC, please see www.lfprojects.org/policies/.\n\n  * Get Started\n  * Ecosystem\n    * PyTorch Conference - 2024\n    * Tools\n  * Edge\n    * About PyTorch Edge\n    * ExecuTorch\n  * Blog\n  * Tutorials\n  * Docs\n    * PyTorch\n    * torchaudio\n    * torchtext\n    * torchvision\n    * torcharrow\n    * TorchData\n    * TorchRec\n    * TorchServe\n    * PyTorch on XLA Devices\n  * Resources\n    * About\n    * PyTorch Foundation\n    * Community\n    * Community stories\n    * Developer Resources\n    * Events\n    * Forum\n    * Models (Beta)\n  * GitHub\n\nTo analyze traffic and optimize your experience, we serve cookies on this\nsite. By clicking or navigating, you agree to allow our usage of cookies. As\nthe current maintainers of this site, Facebook\u2019s Cookies Policy applies. Learn\nmore, including about available controls: Cookies Policy.\n\n", "frontpage": false}
