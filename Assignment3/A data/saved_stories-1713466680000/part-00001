{"aid": "40075711", "title": "Natural Language Processing in Bash", "url": "https://massimo-nazaria.github.io/nlp.html", "domain": "massimo-nazaria.github.io", "votes": 1, "user": "todsacerdoti", "posted_at": "2024-04-18 12:57:16", "comments": 0, "source_title": "Natural Language Processing in Bash", "source_text": "Natural Language Processing in Bash\n\n# Natural Language Processing in Bash\n\nOct 2, 2023\n\nLet\u2019s implement a Bash toolchain to generate random prose that resembles the\ntext corpus by using the n-gram language model!\n\n## A Glimpse at the Toolchain\n\nBasically, NLP aims at teaching computers to understand and work with human\nlanguage by using different techniques.\n\nIn what follows, we\u2019re going to have a glimpse at the toolchain by training\nour model using the novel Moby-Dick.\n\nFirst of all, let\u2019s get the bash-textgen/ folder from the repository and enter\nit:\n\n    \n    \n    $ git clone https://github.com/massimo-nazaria/bash-textgen.git $ cd bash-textgen/\n\n### Preprocessing\n\nIn a typical NLP process, a text corpus (i.e. input data) is preprocessed by\norganizing it into structured data before training mathematical models.\n\nThis involves tasks like tokenization (i.e. breaking the text into words) or\ndata cleaning (i.e. removing any non-relevant text).\n\nFor our example, let\u2019s extract the single words from moby-dick.txt by removing\nunnecessary characters and put the result in words.txt:\n\n    \n    \n    $ cat moby-dick.txt | ./words.sh > words.txt\n\nInitial 10 extracted words:\n\n    \n    \n    $ cat words.txt | head -n 10 moby dick by herman melville chapter loomings call me ishmael\n\nAs you can see, the initial words include: the title, the author full name,\nthe 1st chapter title, and the incipit Call me Ishmael.\n\nPrecisely, words.sh performs the following text transformations:\n\n  1. Make all input text lowercase;\n  2. Remove non-alphabetical characters except for the periods .;\n  3. Remove multiple whitespaces and period characters;\n  4. Output one word (or period) per line.\n\n### Training\n\nDuring the training process, NLP models learn patterns and relationships in\nthe language from the preprocessed training data.\n\nThis way, the resulting trained models can provide a variety of services like\nsentiment analysis and text generation.\n\n#### The N-Gram Language Model\n\nN-gram language models are easy to understand and implement.\n\nWhile considered state-of-the-art in the early days of NLP, nowadays more\nadvanced NLP techniques outperform n-grams for text generation.\n\nIn spite of that, they still work very well for next-word suggestions or auto-\ncompletion in text editors.\n\nTraining data preparation consists in organizing the preprocessed words from\nthe text corpus into n-tuples of consecutive words, namely n-grams.\n\nLet\u2019s do this by computing bigrams (i.e. 2-grams) out of the extracted words:\n\n    \n    \n    $ cat words.txt | ./ngrams.sh 2 > bigrams.txt\n\nInitial 10 bigrams computed:\n\n    \n    \n    cat bigrams.txt | head -n 10 moby dick dick by by herman herman melville melville chapter chapter loomings loomings call call me me ishmael ishmael .\n\nDuring the training process, the n-gram language model learns to predict the\nnext word in a sentence based on the previous n-1 words.\n\nThus, with bigrams they learn to predict next word based on just the previous\nword in the sentence.\n\n### Text Generation\n\nLet\u2019s generate text from the computed bigrams starting from the initial word\n\u201cthe\u201d.\n\n    \n    \n    $ ./textgen.sh bigrams.txt \"the\"\n\nOutput:\n\n> the spare poles and more certain wild creatures to his retired whaleman as\n> he never tell the pier heads to this inclined for the imposed and i lay them\n> endless sculptures.\n\nNot bad at all! The generated prose actually mimics the style of Herman\nMelville from his novel we initially used as our text corpus.\n\nLet\u2019s try it again with the initial word \u201cman\u201d:\n\n    \n    \n    $ ./textgen.sh bigrams.txt \"man\"\n\nOutput:\n\n> man from the reminiscence even for the thames tunnel then tow line not have\n> to the whale and selecting our hemisphere.\n\nIt\u2019s kind of surprising (and funny!) to see how in a few lines of Bash our\ntool can emulate the poetry of such a literary giant.\n\nIn particular, in the example above textgen.sh starts generating a sentence\nfrom a given initial word as follows:\n\nLet \u201cman\u201d be the initial given word.\n\n#### Step 1: Get all the bigrams starting with \u201cman\u201d\n\n    \n    \n    cat bigrams.txt | grep -e \"^man \"\n\nThe result of the command above is a list of bigrams, e.g.:\n\n    \n    \n    man on man receives man enter man distracted man travelled ...\n\nPlease note that the list of bigrams generally contains a lot of duplicates.\nAmong them, some bigrams will show up much more frequently than the others.\n\nAnd the most common bigrams will be the most likely to be selected in the next\nsteps in order to extract our next word.\n\n#### Step 2: Shuffle all such bigrams\n\n    \n    \n    cat bigrams.txt | grep -e \"^man \" | shuf\n\nWe randomly rearrange the list of bigrams so as to avoid extracting always the\nsame next word. Example of shuffled list:\n\n    \n    \n    man from man who man prefers man interested man slipped ...\n\n#### Step 3: Extract the 1st bigram\n\n    \n    \n    cat bigrams.txt | grep -e \"^man \" | shuf | head -n 1\n\nFrom the shuffled list above, we pick the 1st bigram:\n\n    \n    \n    man from\n\nIn our example, \u201cfrom\u201d is our next word in the generated sentence!\n\nNote that we could have chosen any bigram from the shuffled list, without loss\nof generality.\n\nIn fact, the most common bigrams will always be the more likely to be\nextracted, regardless of their position in the shuffled list.\n\n#### Final Step: Get the 2nd word from the bigram\n\n    \n    \n    cat bigrams.txt | grep -e \"^man \" | shuf | head -n 1 | cut -d ' ' -f2`\n\nClearly, the final command in the pipeline cut -d ' ' -f2 extracts the 2nd\nword in the bigram:\n\n    \n    \n    from\n\nThe above generation steps repeat iteratively word-by-word until either the\nnext word is a period character or no next-word is present.\n\n## Play with the Toolchain\n\nLet\u2019s play a little bit more with the toolchain by using trigrams, namely\nn-grams with n=3:\n\n    \n    \n    cat words.txt | ./ngrams.sh 3 > trigrams.txt\n\nIn general, ngrams.sh accepts as its argument any natural integer starting\nfrom 2.\n\nLet\u2019s generate a random sentence starting from the initial 2 words \u201ca man\u201d.\n\n    \n    \n    $ ./textgen.sh trigrams.txt \"a man\"\n\nOutput:\n\n> a man gives himself out of this blubbering now we are about to begin from\n> the hearts of whales is always under great and extraordinary difficulties\n> that every one knows the fine carnation of their yet suspended boats.\n\nLet\u2019s now try the initial 2 words \u201cby falling\u201d:\n\n    \n    \n    $ ./textgen.sh trigrams.txt \"by falling\"\n\nOutput:\n\n> by falling in the best harpooneers of nantucket both messmates of mine tis\n> not me all loveliness is anguish to me we are to credit the old craft s\n> cockpits especially of such a whale ship which like a hot sun s pilot\n> yesterday i wrecked thee and wrong not captain ahab if it might light upon\n> for all deficiencies of that is a new born sight.\n\nLet\u2019s try with \u201cone moment\u201d:\n\n    \n    \n    $ ./textgen.sh trigrams.txt \"one moment\"\n\nOutput:\n\n> one moment to watch the same no the reason of numerous rude scratches\n> altogether of an enormous practical resolution in facing death this old\n> whale hunter of the word why don t you all the men swung in the deep yet is\n> there any reason possessed the most ancient extant portrait anyways\n> purporting to be tried out without being taken out of all voyages now or\n> never for a time when they come from a drooping orchard twig.\n\nWhat a fun!\n\n## The Implementation\n\nGet the toolchain usage and code!\n\nPlease read also \u201cUnix Philosophy with an Example\u201d to learn how to compose\nmultiple commands together as seen in this tutorial.\n\n\u00a9 2024 Massimo Nazaria\n\nRSS\n\nLicensed under a Creative Commons Attribution-NonCommercial 4.0 International\nLicense.\n\n", "frontpage": false}
