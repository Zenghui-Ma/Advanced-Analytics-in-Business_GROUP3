{"aid": "40048028", "title": "Choosing an Embedding Model", "url": "https://www.pinecone.io/learn/series/rag/embedding-models-rundown/", "domain": "pinecone.io", "votes": 2, "user": "gk1", "posted_at": "2024-04-16 03:13:32", "comments": 0, "source_title": "Choosing an Embedding Model | Pinecone", "source_text": "Choosing an Embedding Model | Pinecone\n\nOpens in a new window Opens an external website Opens an external website in a\nnew window\n\nThis website utilizes technologies such as cookies to enable essential site\nfunctionality, as well as for analytics, personalization, and targeted\nadvertising purposes. You may change your settings at any time or accept the\ndefault settings. You may close this banner to continue with only essential\ncookies. Cookie Policy\n\nNew ArticleAdvanced RAG TechniquesRead Now\n\n# Choosing an Embedding Model\n\n  * MTEB Leaderboards\n  * Downloading Test Data\n\nMost of us are using OpenAI's Ada 002 for text embeddings. The reason for that\nis OpenAl built a good embedding model that was easy to use long before anyone\nelse. However, this was a long time ago. One look at the MTEB leaderboards\ntells us that Ada is far from the best option for embedding text.\n\nNowadays, many propriety embedding models far outperform Ada, and there are\neven tiny open-source models with comparable performance, such as E5.\n\nIn this article, we will explore two models - the open-source E5 and Cohere's\nembed v3 models - and see how they compare to the incumbent Ada 002.\n\nVideo walkthrough of this chapter\n\nSource:YouTube\n\n## MTEB Leaderboards\n\nThe most popular place for finding the latest performance benchmarks for text\nembedding models is the MTEB leaderboards hosted by Hugging Face. MTEB is a\ngreat place to start but does require some caution and skepticism - the\nresults are self-reported, and unfortunately, many results prove inaccurate\nwhen attempting to use the models on real-world data.\n\nMany of these models (typically the open-source ones) seem to have been fine-\ntuned on the MTEB benchmarks, producing inflated performance numbers.\nNonetheless, the reported performance of some open-source models \u2014 such as E5\n\u2014 is accurate.\n\nThere are many fields in MTEB that we can mostly ignore. The fields that\nmatter most for those of us using these models in the real world are:\n\n  * Score: the score we should focus on is \"average\" and \"retrieval average\". Both are highly correlated, so focusing on either works.\n  * Sequence length tells us how many tokens a model can consume and compress into a single embedding. Generally speaking, we wouldn't recommend stuffing more than a paragraph of heft into a single embedding - so models supporting up to 512 tokens are usually more than enough.\n  * Model size: the size of a model indicates how easy it will be to run. All models near the top of MTEB are reasonably sized. One of the largest is instructor-xl (requiring 4.96GB of memory), which we can easily run on consumer hardware.\n\nFocusing on these columns gives us all the information we need to choose\nmodels that will likely fit our needs. With this in mind, we choose three\nmodels to feature in this article \u2014 two proprietary models, Ada 002 and embed-\nenglish-v3.0 \u2014 and one tiny but performant open-source model; e5-base-v2.\n\n## Downloading Test Data\n\nTo perform our comparison, we need a dataset. We will use the prechunked AI\nArXiv dataset from HuggingFace datasets.\n\n    \n    \n    !pip install -qU datasets==2.14.6\n    \n    \n    from datasets import load_dataset data = load_dataset( \"jamescalam/ai-arxiv-chunked\", split= \"train\" )\n\nThis dataset gives us ~42K text chunks to embed, each roughly a paragraph or\ntwo.\n\n### Prerequisites\n\nThe prerequisites for each model varies slightly. OpenAI and Cohere store the\ntwo propriety models behind APIs, so their client libraries are very\nlightweight, and all we need is to install those and grab their respective API\nkeys.\n\n    \n    \n    !pip install -qU \\ cohere==4.34 \\ openai==1.2.2\n    \n    \n    import os import cohere import openai # initialize cohere os.environ[\"COHERE_API_KEY\"] = \"your cohere api key\" co = cohere.Client() # openai doesn't need to be initialized, but need to set api key os.environ[\"OPENAI_API_KEY\"] = \"your openai api key\"\n\nE5 is a local model, and we need a little more code and installs to run it \u2014\nincluding fairly heavy libraries like PyTorch. The model is comparatively\nlightweight, so we don't need heavy GPU instances or a ton of run memory.\nHowever ideally, we do want at least a GPU to run it on for faster\nperformance, but this isn't a strict requirement, and we can manage with CPU\nonly.\n\n    \n    \n    !pip install -qU \\ torch==2.1.2 \\ transformers==4.25.0\n    \n    \n    import torch from transformers import AutoModel, AutoTokenizer # use GPU if available, on mac can use MPS device = \"cuda\" if torch.cuda.is_available() else \"cpu\" model_id = \"intfloat/e5-base-v2\" # initialize tokenizer and model tokenizer = AutoTokenizer.from_pretrained(model_id) model = AutoModel.from_pretrained(model_id).to(device) model.eval()\n\n### Creating Embeddings\n\nTo create our embeddings, we will create an embed function for each model.\nWe'll pass a list of strings into these embed functions and expect to return a\nlist of vector embeddings.\n\n#### API Embeddings\n\nNaturally, the code for our proprietary embedding models is more\nstraightforward, so let's cover those two first:\n\n    \n    \n    # cohere embedding function def embed(docs: list[str]) -> list[list[float]]: doc_embeds = co.embed( docs, input_type=\"search_document\", model=\"embed-english-v3.0\" ) return doc_embeds.embeddings # openai embedding function def embed(docs: list[str]) -> list[list[float]]: res = openai.embeddings.create( input=docs, model=\"text-embedding-ada-002\" ) doc_embeds = [r.embedding for r in res.data] return doc_embeds\n\nWith Cohere and OpenAI, we're making a simple API call. There's little to note\nhere other than the input_type parameter for the Cohere API. The input_type\ndefines whether the current inputs are document vectors or query vectors. We\ndefine this to support improved performance for asymmetric semantic search \u2014\nwhere we are querying with a smaller chunk of text (i.e., a search query) and\nattempting to retrieve larger chunks (i.e., a couple of sentences or\nparagraphs).\n\n#### Local Embeddings\n\nE5 works similarly to the Cohere embedding model with support for asymmetric\nsearch. However, the implementation is slightly different. Rather than\nspecifying whether an input is a query or document via a parameter, we prefix\nthat information to the input text. For query, we prefix \"query:\" , and for\ndocuments, we prefix \"passage:\" (another name for documents).\n\n    \n    \n    def embed(docs: list[str]) -> list[list[float]]: docs = [f\"passage: {d}\" for d in docs] # tokenize tokens = tokenizer( docs, padding=True, max_length=512, truncation=True, return_tensors=\"pt\" ).to(device) with torch.no_grad(): # process with model for token-level embeddings out = model(**tokens) # mask padding tokens last_hidden = out.last_hidden_state.masked_fill( ~tokens[\"attention_mask\"][..., None].bool(), 0.0 ) # create mean pooled embeddings doc_embeds = last_hidden.sum(dim=1) / \\ tokens[\"attention_mask\"].sum(dim=1)[..., None] return doc_embeds.cpu().numpy()\n\nAfter specifying that these chunks are documents, we tokenize them to give us\nthe tokens parameter. Every transformer-based model requires a tokenization\nstep. Tokenization is where we translate human-readable plain text into\ntransformer-readable inputs, which is simply a list of integers like [0, 531,\n81, 944, ...], where each integer represents a word or sub-word.\n\nOnce we have our tokens we feed them into our model with model(**tokens). From\nthis, we get our output logits (i.e., predictions) in the out parameter.\n\nSome of our input tokens are padding tokens. These are used as placeholders to\nalign the dimensions of the arrays/tensors that we feed through each model\nlayer. By default, we ignore the output logits produced by these tokens, but\nin some cases (e.g., with embedding models), we must calculate an average\nvalue across all output logits. If we were to consider the output logits\nproduced by the padding tokens in this calculation, we would degrade embedding\nquality.\n\nTo avoid degradation of embedding quality, we must mask (i.e., hide by setting\nto None) the output logits produced by padding tokens. That is what the\nout.last_hidden_state.masked_fill line is doing.\n\nFinally, we're ready to calculate our single vector embedding \u2014 which we do so\nby mean pooling. Mean pooling means taking the average values from all of our\noutput logit vectors to produce a single vector, which we store in the\ndoc_embeds parameter.\n\nFrom there, we return our doc_embeds after moving it from GPU to CPU (if we\nused a GPU) with .cpu() and transforming the PyTorch tensor of doc_embeds into\na Numpy array with .numpy().\n\n### Building a Vector Index\n\nWe can create our vector index with the same logic once we have defined our\nchosen embed function. We define a batch_size and iterate through our dataset\nto create the embeddings and add them to a local vector index called arr.\n\n    \n    \n    from tqdm.auto import tqdm import numpy as np chunks = data[\"chunk\"] batch_size = 256 for i in tqdm(range(0, len(chunks), batch_size)): i_end = min(len(chunks), i+batch_size) chunk_batch = chunks[i:i_end] # embed current batch embed_batch = embed(chunk_batch) # add to existing np array if exists (otherwise create) if i == 0: arr = embed_batch.copy() else: arr = np.concatenate([arr, embed_batch.copy()])\n\nHere, we can measure two metrics \u2014 embedding latency and vector\ndimensionality. Running all of these on Google Colab, we see the time taken to\nindex the entire dataset for each model is:\n\nModel| Batch size| Time taken| Vector dim  \n---|---|---|---  \nembed-english-v3.0| 128| 05:32| 1024  \ntext-embedding-ada-002| 128| 09:07| 1536  \nintfloat/e5-base-v2| 256| 03:53| 768  \n  \nAda 002 is the slowest method here. E5 was the fastest, _but_ was run on a\nV100 GPU instance in Google Colab \u2014 the API models don't require us to run our\ncode on GPU instances. Another consideration is storage requirements. Higher\ndimensional vectors cost more to store, and these costs can build up over\ntime.\n\n### Performance\n\nWhen testing these models, we will see relatively similar results. We're using\na messy dataset, which is more challenging _but_ also more realistic.\n\n#### Q1: Why should I use Llama 2?\n\nNote: we have paraphrased the results below for brevity. See the original\nnotebooks for full results and code [Ada 002, Cohere embed v3, E5 base v2].\n\nAda 002| Embed v3| E5 base v2  \n---|---|---  \n\u2705 \"Llama 2 is intended for commercial and research use in English. Tuned\nmodels are intended for assistant like chat, whereas pretrained models can be\nadapted for a variety of natural language generation tasks.\"| \u26aa\ufe0f \"The focus of\nthis work is to train a series of language models that achieve optimal\nperformance at various sizes. The resulting models, called LLaMA... LLaMA-13B\noutperforms GPT-3 on most benchmarks, despite being 10x smaller. We believe\nthat this model will help democratize the access and study of LLMs, since it\ncan be run on a single GPU.\"| \u274c \"rflowanswerability classificationtask957e2e\ndatato _texttask288_gigaword_title generationtask1728web\n_nlg_data_to_texttask1358 xlsum title generationtask1529_\nscitailv1.1_textual...\"  \n\u2705 \"We develop and release Llama 2, a collection of pretrained and fine-tuned\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion\nparameters. They are optimized for dialogue use cases. Our models outperform\nopen-source chat models on most benchmarks we tested, and based on our human\nevaluations for helpfulness and safety, may be a suitable substitute for\nclosed source models.\"| \u2705 \"We develop and release Llama 2, a collection of\npretrained and fine-tuned large language models (LLMs) ranging in scale from 7\nbillion to 70 billion parameters. They are optimized for dialogue use cases.\nOur models outperform open-source chat models on most benchmarks we tested,\nand based on our human evaluations for helpfulness and safety, may be a\nsuitable substitute for closed source models.\"| \u2705 \"We develop and release\nLlama 2, a collection of pretrained and fine-tuned large language models\n(LLMs) ranging in scale from 7 billion to 70 billion parameters. They are\noptimized for dialogue use cases. Our models outperform open-source chat\nmodels on most benchmarks we tested, and based on our human evaluations for\nhelpfulness and safety, may be a suitable substitute for closed source\nmodels.\"  \n\u2705 \"These closed product LLMs are heavily fine-tuned to align with human\npreferences, which greatly enhances their usability and safety. We develop and\nrelease Llama 2, a family of pretrained and fine-tuned LLMs, at scales up to\n70B parameters. On the series of helpfulness and safety benchmarks we tested,\nLlama 2 models generally perform better than existing open-source models. They\nalso appear to be on par with some of the closed-source models.\"| \u2705 \"These\nclosed product LLMs are heavily fine-tuned to align with human preferences,\nwhich greatly enhances their usability and safety. We develop and release\nLlama 2, a family of pretrained and fine-tuned LLMs, at scales up to 70B\nparameters. On the series of helpfulness and safety benchmarks we tested,\nLlama 2 models generally perform better than existing open-source models. They\nalso appear to be on par with some of the closed-source models.\"| \u2705 \"These\nclosed product LLMs are heavily fine-tuned to align with human preferences,\nwhich greatly enhances their usability and safety. We develop and release\nLlama 2, a family of pretrained and fine-tuned LLMs, at scales up to 70B\nparameters. On the series of helpfulness and safety benchmarks we tested,\nLlama 2 models generally perform better than existing open-source models. They\nalso appear to be on par with some of the closed-source models.\"  \n  \nFor the first set of results, we get slightly better results from Ada. For\nthat first result, Cohere's model returns a result about the original Llama\nmodel (rather than Llama 2), and the E5 model returns some strange malformed\ntext. However, all of the models return the same relevant results in positions\ntwo and three.\n\n#### Q2: Can you tell me about red teaming for llama 2?\n\nAda 002| Embed v3| E5 base v2  \n---|---|---  \n\u26aa\ufe0f \"Visualization of the red team attacks. Each point corresponds to a red\nteam attack embedded in a two dimensional space using UMAP. The color\nindicates attack success (brighter means a more successful attack) as rated by\nthe red team member who carried out the attack. We manually annotated attacks\nand found several clusters of attack types.\"| \u26aa\ufe0f \"... aiding in disinformation\ncampaigns, generating extremist texts, spreading falsehoods, and more. As AI\nsystems improve, the scope of possible harms seems likely to grow. One\npotentially useful tool for adressing harm is red teaming. We describe our\nearly efforts to implement red teaming to make our models safer.\"| \u26aa\ufe0f \"We\ncreated this dataset to analyze and address potential harms in LLMs through\nred teaming. This dataset adds to a limited number of publicly-available red\nteam datasets, and is the only dataset of red team attacks on a language model\ntrained with RLHF as a safety technique.\"  \n\u26aa\ufe0f \"Red teaming ChatGPT via Jailbreaking: Observations indicate that LLMs may\nexhibit social prejudice and toxicity, posing ethical and societal dangers. We\nperform a qualitative research method called red teaming on OpenAI's\nChatGPT.\"| \u26aa\ufe0f \"A red team exercise is an effort to find flaws and\nvulnerabilities, often performed by dedicated red teams that seek to adopy an\nattacker's mindset. In security, red teams are routinely tasked with emulating\nattackers.\"| \u26aa\ufe0f \"We conducted interviews with Trust & Safety experts and\nincorporated their suggested best practices into our experiments to ensure the\nwell-being of the red team. Red team members enjoyed participating in our\nexperiments and felt motivated to make AI systems less harmful.  \n\u26aa\ufe0f \"In the red team task instructions, we provide clear warnings that red team\nmembers may be exposed to sensitive content. Through surveys and feedback, we\nfound that red team members enjoyed the task and did not experience\nsignificant negative emotions.\"| \u26aa\ufe0f \"Red teaming ChatGPT via Jailbreaking:\nObservations indicate that LLMs may exhibit social prejudice and toxicity,\nposing ethical and societal dangers. We perform a qualitative research method\ncalled red teaming on OpenAI's ChatGPT.\"| \u274c\n\"\u679c,\u5e76\u89e3\u91ca\u6211\u6240\u6301\u7acb\u573a\u7684\u539f\u56e0\u3002\u56e0\u6b64,\u6211\u81f4\u529b\u4e8e\u63d0\u4f9b\u79ef\u6781\u3001\u6709\u8da3\u3001\u5b9e\u7528\u548c\u5438\u5f15\u4eba\u7684\u56de \u7b54\u3002\u6211\u7684\u903b\u8f91\u548c\u63a8\u7406\u529b\u6c42\u4e25\u5bc6\u3001\u667a\u80fd\u548c\u6709\u7406\u6709\u636e\u3002\u53e6\u5916,\u6211\u53ef\u4ee5\u63d0\u4f9b\u66f4\u591a\u76f8\u5173\u7ec6\u8282\u6765\nSeed Prompts for Topic-Guided Red-Teaming Self-Instruct\"  \n  \nThe red teaming question returned the worst results across all models. Despite\ninformation about red teaming Llama 2 existing in the dataset, none of that\ninformation was returned. All models did return information about generic red\nteaming, with Cohere's model returning the most informative results (in the\nauthor's opinion). E5 returned what seems to be the poorest result due to the\nlack of English text \u2014 however, it does seem to be related to red teaming,\njust in the wrong language.\n\n#### Q3: What is the difference between gpt-4 and llama?\n\nAda 002| Embed v3| E5 base v2  \n---|---|---  \n\u2705 \"31.39%LLaMA-GPT4 25.99% Tie 42.61% HonestyAlpaca 25.43%LLaMA-GPT4 16.48%\nTie 58.10% Harmlessness(a) LLaMA-GPT4 vs Alpaca ( i.e.,LLaMA-GPT3 ) GPT4\n44.11% LLaMA-GPT4 42.78% Tie 13.11% Helpfulness GPT4 37.48% LLaMA-GPT4 37.88%\nTie 24.64% Honesty GPT4 35.36% LLaMA-GPT4 31.66% Tie 32.98% Harmlessness (b)\nLLaMA-GPT4 vs GPT-4\"| \u2705 \"Second, we compare GPT-4-instruction-tuned LLaMA\nmodels against the teacher model GPT-4. The observations are quite consistent\nover the three criteria: GPT-4-instruction-tuned LLaMA performs similarly to\nthe original GPT-4.\"| \u2705 \"LLaMA-GPT4 is a closer proxy to GPT-4 than Alpaca.\nclosely follow the behavior of GPT-4. When the sequence length is short, both\nLLaMA-GPT4 and GPT-4 can generate responses that contains the simple ground\ntruth answers, but add extra words to make the response more chat-like.\"  \n\u2705 \"Second, we compare GPT-4-instruction-tuned LLaMA models against the teacher\nmodel GPT-4. The observations are quite consistent over the three criteria:\nGPT-4-instruction-tuned LLaMA performs similarly to the original GPT-4.\"| \u2705\n\"Instruction tuning of LLaMA with GPT-4 often achieves higher performance than\ntuning with text-davinci-003 (i.e. Alpaca) and no tuning (i.e. LLaMA): The 7B\nLLaMA GPT4 outperforms the 13B Alpaca and LLaMA.| \u2705 \"We compare LLaMA-GPT4\nwith GPT-4 and Alpaca unnatural instructions. For ROUGE-L scores, Alpaca\noutperforms the other models. We note that LLaMA-GPT4 and GPT4 gradually\nperform better when the ground truth response length increases, eventually\nshowing higher performance when the length is longer than 4.\"  \n\u2705 \"We compare LLaMA-GPT4 with GPT-4 and Alpaca unnatural instructions. For\nROUGE-L scores, Alpaca outperforms the other models. We note that LLaMA-GPT4\nand GPT4 gradually perform better when the ground truth response length\nincreases, eventually showing higher performance when the length is longer\nthan 4.\"| \u2705 \"LLaMA-GPT4 is a closer proxy to GPT-4 than Alpaca. closely follow\nthe behavior of GPT-4. When the sequence length is short, both LLaMA-GPT4 and\nGPT-4 can generate responses that contains the simple ground truth answers,\nbut add extra words to make the response more chat-like.\"| \u2705 \"31.39%LLaMA-GPT4\n25.99% Tie 42.61% HonestyAlpaca 25.43%LLaMA-GPT4 16.48% Tie 58.10%\nHarmlessness(a) LLaMA-GPT4 vs Alpaca ( i.e.,LLaMA-GPT3 ) GPT4 44.11% LLaMA-\nGPT4 42.78% Tie 13.11% Helpfulness GPT4 37.48% LLaMA-GPT4 37.88% Tie 24.64%\nHonesty GPT4 35.36% LLaMA-GPT4 31.66% Tie 32.98% Harmlessness (b) LLaMA-GPT4\nvs GPT-4\"  \n  \nThe results when asking for a comparison between GPT-4 and Llama are good from\neach model. The primary difference is that Ada 002 and E5 both return a\nplaintext table response \u2014 that is harder for us to read, but most LLMs would\nlikely get some good information from that. Cohere returns a set of three\nvaluable text-only responses.\n\nUsing what we have learned here, we have a good overview of the different\ntypes of embedding models and the qualities we might be most interested in\nwhen assessing which of those we should use. Naturally, a big part of that\nassessment should consist of us focusing on evaluation, which we did a little\n\u2014 qualitatively \u2014 here.\n\nNew models are being added to the MTEB leaderboards almost daily \u2014 many of\nthose showing promising state-of-the-art results. So, there is no shortage of\nhigh-quality embedding models we can use in retrieval.\n\nShare via:\n\nPreviousRerankers for RAG\n\nRetrieval Augmented Generation\n\nChapters\n\n  1. Rerankers for RAG\n  2. Embedding Models\n\n     * MTEB Leaderboards\n     * Downloading Test Data\n\n  3. Agent Evaluation\n\nProduct\n\nOverviewDocumentationIntegrationsTrust and SecurityWhat is a Vector Database?\n\nSolutions\n\nCustomersRAGSemantic SearchMulti-Modal SearchCandidate\nGenerationClassification\n\nResources\n\nLearning CenterCommunityPinecone BlogSupport CenterSystem Status\n\nCompany\n\nAboutPartnersCareersNewsroomContact\n\nLegal\n\nTermsPrivacyCookiesCookie Preferences\n\n\u00a9 Pinecone Systems, Inc. | San Francisco, CA\n\nPinecone is a registered trademark of Pinecone Systems, Inc.\n\n", "frontpage": false}
