{"aid": "40048023", "title": "The AI That Could Heal a Divided Internet", "url": "https://time.com/6966990/ai-google-jigsaw-social-media-division/", "domain": "time.com", "votes": 2, "user": "kjhughes", "posted_at": "2024-04-16 03:12:43", "comments": 0, "source_title": "The AI That Could Heal a Divided Internet", "source_text": "The AI That Could Heal a Divided Internet | TIME\n\nPresented By\n\n  * Tech\n  * Artificial Intelligence\n  * The AI That Could Heal a Divided Internet\n\n# The AI That Could Heal a Divided Internet\n\n13 minute read\n\nIllustration By Dan Page for TIME\n\nBy Billy Perrigo\n\nApril 15, 2024 2:00 PM EDT\n\nIn the 1990s and early 2000s, technologists made the world a grand promise:\nnew communications technologies would strengthen democracy, undermine\nauthoritarianism, and lead to a new era of human flourishing. But today, few\npeople would agree that the internet has lived up to that lofty goal.\n\nToday, on social media platforms, content tends to be ranked by how much\nengagement it receives. Over the last two decades politics, the media, and\nculture have all been reshaped to meet a single, overriding incentive: posts\nthat provoke an emotional response often rise to the top.\n\nEfforts to improve the health of online spaces have long focused on content\nmoderation, the practice of detecting and removing bad content. Tech companies\nhired workers and built AI to identify hate speech, incitement to violence,\nand harassment. That worked imperfectly, but it stopped the worst toxicity\nfrom flooding our feeds.\n\nThere was one problem: while these AIs helped remove the bad, they didn\u2019t\nelevate the good. \u201cDo you see an internet that is working, where we are having\nconversations that are healthy or productive?\u201d asks Yasmin Green, the CEO of\nGoogle\u2019s Jigsaw unit, which was founded in 2010 with a remit to address\nthreats to open societies. \u201cNo. You see an internet that is driving us further\nand further apart.\u201d\n\nWhat if there were another way?\n\nJigsaw believes it has found one. On Monday, the Google subsidiary revealed a\nnew set of AI tools, or classifiers, that can score posts based on the\nlikelihood that they contain good content: Is a post nuanced? Does it contain\nevidence-based reasoning? Does it share a personal story, or foster human\ncompassion? By returning a numerical score (from 0 to 1) representing the\nlikelihood of a post containing each of those virtues and others, these new AI\ntools could allow the designers of online spaces to rank posts in a new way.\nInstead of posts that receive the most likes or comments rising to the top,\nplatforms could\u2014in an effort to foster a better community\u2014choose to put the\nmost nuanced comments, or the most compassionate ones, first.\n\nRead More: How Americans Can Tackle Political Division Together\n\nThe breakthrough was made possible by recent advances in large language models\n(LLMs), the type of AI that underpins chatbots like ChatGPT. In the past, even\ntraining an AI to detect simple forms of toxicity, like whether a post was\nracist, required millions of labeled examples. Those older forms of AI were\noften brittle and ineffectual, not to mention expensive to develop. But the\nnew generation of LLMs can identify even complex linguistic concepts out of\nthe box, and calibrating them to perform specific tasks is far cheaper than it\nused to be. Jigsaw\u2019s new classifiers can identify \u201cattributes\u201d like whether a\npost contains a personal story, curiosity, nuance, compassion, reasoning,\naffinity, or respect. \u201cIt's starting to become feasible to talk about\nsomething like building a classifier for compassion, or curiosity, or nuance,\u201d\nsays Jonathan Stray, a senior scientist at the Berkeley Center for Human-\nCompatible AI. \u201cThese fuzzy, contextual, know-it-when-I-see-it kind of\nconcepts\u2014 we're getting much better at detecting those.\u201d\n\nThis new ability could be a watershed for the internet. Green, and a growing\nchorus of academics who study the effects of social media on public discourse,\nargue that content moderation is \u201cnecessary but not sufficient\u201d to make the\ninternet a better place. Finding a way to boost positive content, they say,\ncould have cascading positive effects both at the personal level\u2014our\nrelationships with each other\u2014but also at the scale of society. \u201cBy changing\nthe way that content is ranked, if you can do it in a broad enough way, you\nmight be able to change the media economics of the entire system,\u201d says Stray,\nwho did not work on the Jigsaw project. \u201cIf enough of the algorithmic\ndistribution channels disfavored divisive rhetoric, it just wouldn\u2019t be worth\nit to produce it any more.\u201d\n\nOne morning in late March, Tin Acosta joins a video call from Jigsaw\u2019s offices\nin New York City. On the conference room wall behind her, there is a large\nphotograph from the 2003 Rose Revolution in Georgia, when peaceful protestors\ntoppled the country\u2019s Soviet-era government. Other rooms have similar photos\nof people in Syria, Iran, Cuba and North Korea \u201cusing tech and their voices to\nsecure their freedom,\u201d Jigsaw\u2019s press officer, who is also in the room, tells\nme. The photos are intended as a reminder of Jigsaw\u2019s mission to use\ntechnology as a force for good, and its duty to serve people in both\ndemocracies and repressive societies.\n\nOn her laptop, Acosta fires up a demonstration of Jigsaw\u2019s new classifiers.\nUsing a database of 380 comments from a recent Reddit thread, the Jigsaw\nsenior product manager begins to demonstrate how ranking the posts using\ndifferent classifiers would change the sorts of comments that rise to the top.\nThe thread\u2019s original poster had asked for life-affirming movie\nrecommendations. Sorted by the default ranking on Reddit\u2014posts that have\nreceived the most upvotes\u2014the top comments are short, and contain little\nbeyond the titles of popular movies. Then Acosta clicks a drop-down menu, and\nselects Jigsaw\u2019s reasoning classifier. The posts reshuffle. Now, the top\ncomments are more detailed. \u201cYou start to see people being really thoughtful\nabout their responses,\u201d Acosta says. \u201cHere\u2019s somebody talking about School of\nRock\u2014not just the content of the plot, but also the ways in which the movie\nhas changed his life and made him fall in love with music.\u201d (TIME agreed not\nto quote directly from the comments, which Jigsaw said were used for\ndemonstrative purposes only and had not been used to train its AI models.)\n\nAcosta chooses another classifier, one of her favorites: whether a post\ncontains a personal story. The top comment is now from a user describing how,\nunder both a heavy blanket and the influence of drugs, they had ugly-cried so\nhard at Ke Huy Quan\u2019s monologue in Everything Everywhere All at Once that\nthey\u2019d had to pause the movie multiple times. Another top comment describes\nhow a movie trailer had inspired them to quit a job they were miserable with.\nAnother tells the story of how a movie reminded them of their sister, who had\ndied 10 years earlier. \u201cThis is a really great way to look through a\nconversation and understand it a little better than [ranking by] engagement or\nrecency,\u201d Acosta says.\n\nFor the classifiers to have an impact on the wider internet, they would\nrequire buy-in from the biggest tech companies, which are all locked in a\nzero-sum competition for our attention. Even though they were developed inside\nGoogle, the tech giant has no plans to start using them to help rank its\nYouTube comments, Green says. Instead, Jigsaw is making the tools freely\navailable for independent developers, in the hopes that smaller online spaces,\nlike message boards and newspaper comment sections, will build up an evidence\nbase that the new forms of ranking are popular with users.\n\nRead More: The Subreddit /r/Collapse Has Become the Doomscrolling Capital of\nthe Internet. Can Its Users Break Free?\n\nThere are some reasons to be skeptical. For all its flaws, ranking by\nengagement is egalitarian. Popular posts get amplified regardless of their\ncontent, and in this way social media has allowed marginalized groups to gain\na voice long denied to them by traditional media. Introducing AI into the mix\ncould threaten this state of affairs. A wide body of research shows that LLMs\nhave plenty of ingrained biases; if applied too hastily, Jigsaw\u2019s classifiers\nmight end up boosting voices that are already prominent online, thus further\nmarginalizing those that aren\u2019t. The classifiers could also exacerbate the\nproblem of AI-generated content flooding the internet, by providing spammers\nwith an easy recipe for AI-generated content that\u2019s likely to get amplified.\nEven if Jigsaw evades those problems, tinkering with online speech has become\na political minefield. Both conservatives and liberals are convinced their\nposts are being censored; meanwhile, tech companies are under fire for making\nunaccountable decisions that affect the global public square. Jigsaw argues\nthat its new tools may allow tech platforms to rely less on the controversial\npractice of content moderation. But there\u2019s no getting away from the fact that\nchanging what kind of speech gets rewarded online will always have political\nopponents.\n\nStill, academics say that given a chance, Jigsaw\u2019s new AI tools could result\nin a paradigm shift for social media. Elevating more desirable forms of online\nspeech could create new incentives for more positive online\u2014and possibly\noffline\u2014social norms. If a platform amplifies toxic comments, \u201cthen people get\nthe signal they should do terrible things,\u201d says Ravi Iyer, a technologist at\nthe University of Southern California who helps run the nonprofit Psychology\nof Technology Research Network. \u201cIf the top comments are informative and\nuseful, then people follow the norm and create more informative and useful\ncomments.\u201d\n\nThe new algorithms have come a long way from Jigsaw\u2019s earlier work. In 2017,\nthe Google unit released Perspective API, an algorithm for detecting toxicity.\nThe free tool was widely used, including by the New York Times, to downrank or\nremove negative comments under articles. But experimenting with the tool,\nwhich is still available online, reveals the ways that AI tools can carry\nhidden biases. \u201cYou\u2019re a f-cking hypocrite\u201d is, according to the classifier,\n96% likely to be a toxic phrase. But many other hateful phrases, according to\nthe tool, are likely to be non-toxic, including the neo-Nazi slogan \u201cJews will\nnot replace us\u201d (41%) and transphobic language like \u201ctrans women are men\u201d\n(36%). The tool breaks when confronted with a slur that is commonly directed\nat South Asians in the U.K. and Canada, returning the error message: \u201cWe don't\nyet support that language, but we're working on it!\u201d\n\nTo be sure, 2017 was a very different era for AI. Jigsaw has made efforts to\nmitigate biases in its new classifiers, which are unlikely to make such basic\nerrors. Its team tested the new classifiers on a set of comments that were\nidentical except for the names of different identity groups, and said it found\nno hint of bias. Still, the patchy effectiveness of the older Perspective API\nserves as a reminder of the pitfalls of relying on AI to make value judgments\nabout language. Even today\u2019s powerful LLMs are not free from bias, and their\nfluency can often conceal their limitations. They can discriminate against\nAfrican American English; they function poorly in some non-English languages;\nand they can treat equally-capable job candidates differently based on their\nnames alone. More work will be required to ensure Jigsaw\u2019s new AIs don\u2019t have\nless visible forms of bias. \u201cOf course, there are things that you have to\nwatch out for,\u201d says Iyer, who did not work on the Jigsaw project. \u201cHow do we\nmake sure that [each classifier] captures the diversity of ways that people\nexpress these concepts?\u201d\n\nIn a paper published earlier this month, Acosta and her colleagues set out to\ntest how readers would respond to a list of comments ranked using Jigsaw\u2019s new\nclassifiers, compared to comments sorted by recency. They found that readers\npreferred the comments sorted by the classifiers, finding them to be more\ninformative, respectful, trustworthy, and interesting. But they also found\nthat ranking comments by just one classifier on its own, like reasoning, could\nput users off. In its press release launching the classifiers on Monday,\nJigsaw says it intends for its tools to be mixed and matched. That\u2019s possible\nbecause all they do is return scores between zero and one\u2014so it\u2019s possible to\nwrite a formula that combines several scores together into a single number,\nand use that number as a ranking signal. Web developers could choose to rank\ncomments using a carefully-calibrated mixture of compassion, respect, and\ncuriosity, for example. They could also throw engagement into the mix as well\n\u2013 to make sure that posts that receive lots of likes still get boosted too.\n\nJust as removing negative content from the internet has received its fair\nshare of pushback, boosting certain forms of \u201cdesirable\u201d content is likely to\nprompt complaints that tech companies are putting their thumbs on the\npolitical scales. Jigsaw is quick to point out that its classifiers are not\nonly apolitical, but also propose to boost types of content that few people\nwould take issue with. In tests, Jigsaw found the tools did not\ndisproportionately boost comments that were seen by users as unfavorable to\nRepublicans or Democrats. \u201cWe have a track record of delivering a product\nthat\u2019s useful for publishers across the political spectrum,\u201d Green says. \u201cThe\nemphasis is on opening up conversations.\u201d Still, the question of power\nremains: who gets to decide which kinds of content are desirable? Jigsaw\u2019s\nhope is that by releasing the technology publicly, different online spaces can\neach choose what works for them\u2014thus avoiding any one hegemonic platform\ntaking that decision on behalf of the entire internet.\n\nFor Stray, the Berkeley scientist, there is a tantalizing prospect to an\ninternet where positive content gets boosted. Many people, he says, think of\nonline misinformation as leading to polarization. And it can. \u201cBut it also\nworks the other way around,\u201d he says. The demand for low-quality information\narises, at least in part, because people are already polarized. If the tools\nresult in people becoming less polarized, \u201cthen that should actually change\nthe demand-side for certain types of lower quality content.\u201d It\u2019s\nhypothetical, he cautions, but it could lead to a virtuous circle, where\ndeclining demand for misinformation feeds a declining supply.\n\nWhy would platforms agree to implement these changes? Almost by definition,\nranking by engagement is the most effective way to keep users onsite, thus\nkeeping eyeballs on the ads that drive up revenue. For the big platforms, that\nmeans both the continued flow of profits, and the fact that users aren\u2019t\nspending time with a competitor\u2019s app. Replacing engagement-based ranking with\nsomething less engaging seems like a tough ask for companies already battling\nto keep their users\u2019 attention.\n\nThat\u2019s true, Stray says. But, he notes that there are different forms of\nengagement. There\u2019s short-term engagement, which is easy for platforms to\noptimize for: is a tweak to a platform likely to make users spend more time\nscrolling during the next hour? Platforms can and do make changes to boost\ntheir short-term engagement, Stray says\u2014but those kinds of changes often mean\nboosting low-quality, engagement-bait types of content, which tend to put\nusers off in the long term.\n\nThe alternative is long-term engagement. How might a change to a platform\ninfluence a user\u2019s likelihood of spending more time scrolling during the next\nthree months? Long-term engagement is healthier, but far harder to optimize\nfor, because it\u2019s harder to isolate the connection between cause and effect.\nMany different factors are acting upon the user at the same time. Large\nplatforms want users to be returning over the long term, Stray says, and for\nthem to cultivate healthy relationships with their products. But it\u2019s\ndifficult to measure, so optimizing for short-term engagement is often an\neasier choice.\n\nJigsaw\u2019s new algorithms could change that calculus. \u201cThe hope is, if we get\nbetter at building products that people want to use in the long run, that will\noffset the race to the bottom,\u201d Stray says. \u201cAt least somewhat.\u201d\n\n### More From TIME\n\n## More Must-Reads From TIME\n\n  * Dua Lipa Manifested All of This\n  * Exclusive: Google Workers Revolt Over $1.2 Billion Contract With Israel\n  * Stop Looking for Your Forever Home\n  * The Sympathizer Counters 50 Years of Hollywood Vietnam War Narratives\n  * The Bliss of Seeing the Eclipse From Cleveland\n  * Hormonal Birth Control Doesn\u2019t Deserve Its Bad Reputation\n  * The Best TV Shows to Watch on Peacock\n  * Want Weekly Recs on What to Watch, Read, and More? Sign Up for Worth Your Time\n\nWrite to Billy Perrigo at billy.perrigo@time.com\n\n## You May Also Like\n\nIdeasThe Huge Risks From AI In an Election Year\n\nTechExclusive: Google Workers Revolt Over $1.2 Billion Contract With Israel\n\nTechIn the Face of U.S. Ban Threats, TikTok's Parent Company is More\nProfitable Than Ever\n\nWorldWhat to Know About Elon Musk\u2019s Battle With a Brazilian Judge Over Speech\non Social Media\n\nEdit Post\n\n\u00a9 2024 TIME USA, LLC. All Rights Reserved. Use of this site constitutes\nacceptance of our Terms of Service, Privacy Policy (Your Privacy Rights) and .\nTIME may receive compensation for some links to products and services on this\nwebsite. Offers may be subject to change without notice.\n\n", "frontpage": false}
