{"aid": "40127790", "title": "Community Voices \u2013 At Xiaomi, where are we heading with Gravitino", "url": "https://datastrato.ai/blog/gravitino-xiaomi/", "domain": "datastrato.ai", "votes": 1, "user": "justinmclean", "posted_at": "2024-04-23 02:18:34", "comments": 0, "source_title": "Community Voices - At Xiaomi, where are we heading with Gravitino", "source_text": "Community Voices - At Xiaomi, where are we heading with Gravitino | Datastrato\n\nSkip to main content\n\n# Community Voices - At Xiaomi, where are we heading with Gravitino\n\nMarch 28, 2024 \u00b7 10 min read\n\nAuthor: Xing Yong (Github Id: YxAc), Head of computing platform, Xiaomi Inc.\n\nXiaomi Inc. is a consumer electronics and smart manufacturing company with\nsmartphones, smart hardware and electric cars connected by an IoT platform at\nits core. As of 2023, Xiaomi was ranked among the top 3 in the global\nsmartphone market, according to Canalys, and listed as Fortune Global 500 for\nthe 5th consecutive year.\n\nThe Xiaomi Cloud Computing Platform team is dedicated to providing reliable\nand secure data computing and processing capabilities for Xiaomi's business.\nWe actively contribute to many open-source projects, covering storage,\ncomputing, resource scheduling, message queue, data lake, etc. By leveraging\nthese advanced technologies, our team has achieved significant milestones,\nincluding winning the Xiaomi Million-dollar Technology Award Finalist.\n\nThis article focuses on the use of Gravitino in Xiaomi, providing solutions to\nfuture work plans and general guidance. There are, as follows, three key\npoints and we look forward to growing our data-driven business with Gravitino.\n\n  * Unifying our metadata\n\n  * Integrating data and AI asset management\n\n  * Unifying user permission management\n\n### 1\\. Unify our metadata\n\nWith the introduction of multi-regional or multi-cloud deployment, the problem\nof data silos becomes even more pronounced. It becomes challenging to maintain\na unified view of the data across different regions or cloud providers. This\nis really true for Xiaomi. Gravitino provides a solution to such challenges,\nand helps break down the data silos. It aims to solve such problems in data\nmanagement, governance, and analysis in a multi-cloud architecture.\n\n#### Gravitino's position in Xiaomi's data platform\n\nGravitino, highlighted in green and yellow in the diagram below, has the\nfollowing features that we need :\n\n  * Unified Metadata Lake: As a unified data catalog, it supports multiple data sources, computing engines, and data platforms for data development, management, and governance.\n\n  * Real-time and Consistency: Real-time acquisition of metadata to ensure SSOT (Single Source of Truth).\n\n  * Dynamic Registration: Supports adding/altering the data catalog on the fly, no need to restart the service, which makes maintenance and upgrades much easier than before.\n\n  * Multi-Engines Support: not only Data engines, like: Trino, Apache Spark, Apache Flink (WIP), but also AI/ML frameworks, such as Tensorflow (WIP), PyTorch (WIP) and Ray (WIP).\n\n  * Multi-Storage Support: Supports both Data and AI domain-specific storages, including HDFS/Hive, Iceberg, RDBMS, as well as NAS/CPFS, JuiceFS, etc.\n\n  * Eco-friendly: Supports using external Apache Ranger for permission management, external event bus for audit and notification, and external SchemaRegistry for messaging catalog.\n\nFeature is still in active development\n\n#### Unified metadata lake, unified management\n\nAs the type of data sources becomes more and more abundant, computing engines\nlike Trino, Spark and Flink need to maintain a long list of the source\ncatalogs for each of them. That introduces a lot of duplicated and complicated\nmaintenance work.\n\nTo build fabric among multiple data sources and computing engines, it is often\nexpected to manage all kinds of data catalogs in one place, and then use a\nunified service to expose those metadata. Gravitino is extremely useful in\nthis context as it provides a unified metadata lake to standardize the data\ncatalog operations, and unify all metadata management and governance.\n\n#### User Story\n\nUsers can use a three-level coordinate: catalog.schema.entity to describe all\nthe data, and used for data integration, federated queries, etc. What is\nexciting is that engines no longer need to maintain complex and tedious data\ncatalogs, which simplifies the complexity of O(M*N) to O(M+N).\n\n> Note: M represents the number of engines, N represents the number of data\n> sources.\n\nFurthermore, we can use a simple and unified language to make data integration\nand federated queries:\n\n  * Apache Spark: Writing to Apache Doris from Apache Hive (with Gravitino Spark connector).\n\n    \n    \n    INSERT INTO doris_cluster_a.doris_db.doris_table SELECT goods_id, goods_name, price FROM hive_cluster_a.hive_db.hive_table\n\n  * Trino: Making a query between Hive and Apache Iceberg (with Gravitino Trino connector).\n\n    \n    \n    SELECT * FROM hive_cluster_b.hive_db.hive_table a JOIN iceberg_cluster_b.iceberg_db.iceberg_table b ON a.name = b.name\n\n### 2\\. Integrate data and AI asset management\n\nIn the realm of big data, we have made significant progress through data\nlineage, access measurement, and life cycle management. However, in the domain\nof AI, non-tabular data has always been the most challenging aspect of data\nmanagement and governance, encompassing HDFS files, NAS files, and other\nformats.\n\n#### Challenges of AI asset management\n\nIn the realm of machine learning, the process of reading and writing files is\nvery flexible. Users can use various formats, such as Thrift-Sequence, Thrift-\nParquet, Parquet, TFRecord, JSON, text, and more. Additionally, they can\nleverage multiple programming languages, including Scala, SQL, Python, and\nothers. To manage our AI assets, we need to take into account these diverse\nuses and ensure adaptability and compatibility.\n\nSimilar to tabular data management, non-tabular data also needs to adapt to a\nvariety of engines and storages, including frameworks like PyTorch and\nTensorFlow, as well as various storage interfaces like FileSystem for file\nsets, FUSE for instance disk, CSI for container storage.\n\n#### Non-tabular data management architecture\n\nWe aim to establish AI asset management capabilities by leveraging Gravitino,\nwhose core technologies are outlined in the figure below.\n\n  * Non-tabular data catalog management: Achieving the auditing for AI assets, and the assurance of the specification for file paths;\n\n  * File interface support: Ensuring seamless compatibility with various file interfaces:\n\n    * Hadoop File System: Achieving the compatibility with the Hadoop file system through GVFS (Gravitino Virtual File System).\n\n    * CSI Driver: Facilitating the reading and writing of files within container storage.\n\n    * FUSE Driver: Enabling the reading and writing of files directly on the physical machine disk.\n\n  * AI asset lifecycle management: Implementing TTL (Time-To-Live) management for non-tabular data.\n\n#### User story\n\nWe expect that the migration process for users from the original way to the\nnew approach will be straightforward and seamless. In fact, the transition\ninvolves just two steps:\n\n  * 1)Create a fileset catalog with the storage location, and configure the TTL (Time-To-Live) on the Gravitino-based data platform.\n\n  * 2)Replace the original file path with a new way: gvfs://\n\nTo illustrate, let's consider the example of Spark reading HDFS files as\nfollows.\n\n    \n    \n    // 1.Structured data - Parquet val inputPath = \"hdfs://cluster_a/database/table/date=20240309\" val df = spark.read.parquet(inputPath).select()... val outputPath = \"hdfs://cluster_a/database/table/date=20240309/hour=00\" df.write().format(\"parquet\").mode(SaveMode.Overwrite).save(outputPath) // 2.Semi-structured data - Json inputPath = \"hdfs://cluster_a/database/table/date=20240309_${date-7}/xxx.json\" val fileRDD = sc.read.json(inputPath) // 3.Unstructured data - Text val inputPath = \"hdfs://cluster_a/database/table/date=20240309_12\" val fileRDD = sc.read.text(inputPath)\n\nLeveraging Gravitino, we create a fileset called \u201cmyfileset\u201d that is pointing\nto the origin HDFS, then we can replace the original hdfs://xxx with the new\ngvfs://fileset/xxx approach, offering users a seamless and intuitive way to\nupgrade. Users will no longer have to care about the real storage location.\n\n    \n    \n    // 1.Structured data - Parquet val inputPath = \"gvfs://fileset/myfileset/database/table/date=20240309\" val df = spark.read.parquet(inputPath).select()... val outputPath = \"gvfs://fileset/myfileset/database/table/date=20240309/hour=00\" df.write().format(\"parquet\").mode(SaveMode.Overwrite).save(outputPath) // 2.Semi-structured data - Json inputPath = \"gvfs://fileset/myfileset/database/table/date=20240309_${date-7}/xxx.json\" val fileRDD = sc.read.json(inputPath) // 3.Unstructured data - Text val inputPath = \"gvfs://fileset/myfileset/database/table/date=20240309_12\") val fileRDD = sc.read.text(inputPath)\n\nAs previously mentioned, file reading and writing exhibit a lot of\nflexibility, it also adapts to diverse engines. Instead of enumerating\nindividual examples, the overarching principle remains that users should be\nable to manage and govern non-tabular data with minimal modifications.\n\nMany challenges within AI asset management require exploration and development\nwork. It includes specifying the depth and date of file paths, facilitating\ndata sharing, exploring non-tabular data reading and writing solutions based\non the data lake like Iceberg. Those will be our focus in the near future.\n\n### 3\\. Unify user permission management\n\nMetadata and user permission information are so close to each other, and it is\nalways a good idea to manage them together. The metadata service also needs to\nintegrate user permission-related capabilities to authenticate resource\noperations. We expect to achieve this in our data platform by leveraging\nGravitino.\n\n#### Challenges of unified authentication across multi-system\n\nIn order to provide users with a seamless data development experience, the\ndata platform often needs to be integrated with various storage and\ncomputation systems. However, such integrations often lead to the challenge of\nmanaging multiple systems and accounts.\n\nUsers need to authenticate themselves using different accounts in different\nsystems like HDFS (Kerberos), Doris (User/Password), and Talos (AK/SK - Xiaomi\nIAM account). Such fragmented authentication and authorization processes\nsignificantly slow and can even block development.\n\nTo address this issue, a crucial step for a streamlined data development\nplatform is to shield the complexity of different account systems and\nestablish a unified authorization framework to increase the efficiency of data\ndevelopment.\n\n#### Unified user permissions based on workspace\n\nXiaomi's data platform is designed around the concept of Workspace and\nutilizes the RBAC (Role-Based Access Control) permission model. Gravitino\nallows us to generate what we call \"mini-accounts\" (actual resource accounts,\nsuch as HDFS-Kerberos) within the workspace, effectively shielding users from\nthe complexities of Kerberos, User/Password, and IAM/AKSK accounts.\n\nHere are the key components of this setup:\n\n  * Workspace: Workspaces serve as the smallest operational unit within the data platform, containing all associated resources.\n\n  * Role: Identities within the workspace, such as Admin, Developer, and Guest. Each role is granted different permissions for accessing workspace resources.\n\n  * Resource: Resources within the workspace, such as catalog.database.table, are abstracted into three-level coordinates thanks to unified metadata.\n\n  * Permission: Permissions determine the level of control granted to users for operating resources within the workspace, including admin, write, and read.\n\n  * Token: A unique ID used to identify individuals within the workspace.\n\n  * Authentication: API operations are authenticated using tokens, while IAM identities are carried through UI operations after login.\n\n  * Authorization: Authorization is managed through Apache Ranger, granting the necessary permissions to authenticated workspace roles.\n\n  * Mini-account: Each workspace has a dedicated set of proxy accounts to access the resources, such as HDFS (Kerberos) or Apache Doris (User/Password). When the engine accesses the underlying resources, it seamlessly utilizes the corresponding mini-account authentication for each resource. However, the entire process remains transparent to the user, who only needs to focus on managing workspace permissions (which are equivalent to resource permissions by leveraging Gravitino).\n\n#### User story\n\nThe figure below shows a brief process for users to create and access\nresources on our data platform:\n\nAll users are only aware of the workspace identity and workspace permissions.\n\nUpon creating a workspace, a suite of workspace proxy mini-accounts is\nautomatically created. Whenever resources are created or imported within the\nworkspace, the corresponding proxy mini-account is authorized with the\nnecessary resource permissions.\n\nWhen a user attempts to read or write to a resource, the system verifies their\nworkspace permissions. If the workspace permission check is successful, the\nengine utilizes the mini-account to perform the desired read or write\noperation on the resource.\n\n### Summary\n\nIn this blog, we showcase three important scenarios at Xiaomi that we\u2019re using\nGravitino to accomplish - most of the critical work has been done, the rest\nare ongoing with good progress. We're confident in the successful landing of\nall above scenarios in Xiaomi to support our data-driven business in a better\nway, and we are glad to be part of the Gravitino community to co-create the\npotential de-facto standard of the unified metadata lake.\n\n  * 1\\. Unify our metadata\n\n    * Gravitino's position in Xiaomi's data platform\n    * Unified metadata lake, unified management\n    * User Story\n  * 2\\. Integrate data and AI asset management\n\n    * Challenges of AI asset management\n    * Non-tabular data management architecture\n    * User story\n  * 3\\. Unify user permission management\n\n    * Challenges of unified authentication across multi-system\n    * Unified user permissions based on workspace\n    * User story\n  * Summary\n\nDatastrato is next-gen data and AI platform. Unify all your data, analytics\nand AI in one place.\n\nSubscribe to our newsletter.\n\nStar us on GitHub\n\nProduct\n\n  * Open Source\n\nLearn\n\n  * Discourse Community\n  * Slack Community\n\nSocial\n\n  * Twitter\n  * GitHub\n  * Linkedin\n  * YouTube\n  * Slack\n\nCompany\n\n  * About us\n  * Careers\n\nCopyright \u00a9 2024 DatastratoPrivacy\n\n", "frontpage": false}
