{"aid": "40094890", "title": "Common Weakness Scoring System (CWSS)", "url": "https://cwe.mitre.org/cwss/cwss_v1.0.1.html", "domain": "mitre.org", "votes": 1, "user": "keepamovin", "posted_at": "2024-04-20 04:45:15", "comments": 0, "source_title": "CWE - Common Weakness Scoring System (CWSS)", "source_text": "CWE - Common Weakness Scoring System (CWSS)\n\nOpens in a new window Opens an external website Opens an external website in a\nnew window\n\nThis website utilizes technologies such as cookies to enable essential site\nfunctionality, as well as for analytics, personalization, and targeted\nadvertising purposes. You may change your settings at any time or accept the\ndefault settings. You may close this banner to continue with only essential\ncookies. Privacy Policy\n\nStorage Preferences\n\n# Common Weakness Enumeration\n\nA community-developed list of SW & HW weaknesses that can become\nvulnerabilities\n\nHome > CWSS > Common Weakness Scoring System (CWSS)  \n---  \n  \n  * Home\n  * \u25bc\n\nAbout New to CWE User Stories History Documents FAQs Glossary Compatibility\n\n  * \u25bc\n\nLatest Version Downloads Reports Visualizations Archive\n\n  * \u25bc\n\nRoot Cause Mapping Guidance Root Cause Mapping Quick Tips Root Cause Mapping\nExamples\n\n  * \u25bc\n\nTop 25 Software Top Hardware Top 10 KEV Weaknesses\n\n  * \u25bc\n\nCommunity Working Groups & Special Interest Groups Board Board Meeting Minutes\nCWE Discussion List CWE Discussion Archives Contribute Weakness Content to CWE\n\n  * \u25bc\n\nCurrent News X-Twitter Mastodon LinkedIn YouTube Podcast Medium News Archive\n\n  * Search\n\n| CWE Glossary Definition  \n---  \n  \n## Common Weakness Scoring System (CWSSTM)\n\nThe MITRE Corporation\n\nCopyright \u00a9 2014\n\nhttps://cwe.mitre.org/cwss/\n\nCWSS version: 1.0.1| Document version: 1.0.1  \n---|---  \nRevision Date: September 5, 2014  \n  \nSection Contents  \n---  \nCWSS  \nCWRAF  \nVignettes  \nChange Log  \nArchive  \nFAQs  \nContact Us  \n  \nSection Contents  \n---  \nCWSS  \nCWRAF  \nVignettes  \nChange Log  \nFAQs  \nContact Us  \n  \nTable of Contents\n\n  * 1 Introduction\n\n    * 1.1 What is CWSS?\n    * 1.2 Other weakness scoring systems\n    * 1.3 How does CWSS work?\n    * 1.4 Who performs the scoring?\n    * 1.5 Who owns CWSS?\n    * 1.6 Who is using CWSS?\n  * 2 Metric Groups\n\n    * 2.1 Metric Group Factors\n    * 2.2 Values for Uncertainty and Flexibility\n    * 2.3 Base Finding Metric Group\n      * 2.3.1 Technical Impact (TI)\n      * 2.3.2 Acquired Privilege (AP)\n      * 2.3.3 Acquired Privilege Layer (AL)\n      * 2.3.4 Internal Control Effectiveness (IC)\n      * 2.3.5 Finding Confidence (FC)\n    * 2.4 Attack Surface Metric Group\n\n      * 2.4.1 Required Privilege (RP)\n      * 2.4.2 Required Privilege Layer (RL)\n      * 2.4.3 Access Vector (AV)\n      * 2.4.4 Authentication Strength (AS)\n      * 2.4.5 Level of Interaction (IN)\n      * 2.4.6 Deployment Scope (SC)\n    * 2.5 Environmental Metric Group\n\n      * 2.5.1 Business Impact (BI)\n      * 2.5.2 Likelihood of Discovery (DI)\n      * 2.5.3 Likelihood of Exploit (EX)\n      * 2.5.4 External Control Effectiveness (EC)\n      * 2.5.5 Prevalence (P)\n  * 3 CWSS Score Formula\n\n    * 3.1 Base Finding Subscore\n    * 3.2 Attack Surface Subscore\n    * 3.3 Environmental Subscore\n    * 3.4 Additional Features of the Formula\n  * 4 Vectors, Scoring Examples, and Portability\n\n    * 4.1 Example: Business-critical application\n    * 4.2 Example: Wiki with limited business criticality\n    * 4.3 Other Approaches to Score Portability\n  * 5 Considerations for CWSS beyond 1.0\n\n    * 5.1 Current Limitations of the Scoring Method\n    * 5.2 Community Review and Validation of Factors\n    * 5.3 Impact of CWSS and CWE Version Changes to Scoring\n  * 6 Future Activities0\n  * 7 Community Participation in CWSS\n  * A Appendix A: CVSS Comparison\n\n    * A.1 CVSS Overview\n    * A.2 Usage Scenarios for CWSS versus CVSS\n    * A.3 Comparison of CVSSv2 Factors with CWSS Factors\n    * A.4 Other Differences between CVSS and CWSS\n  * B Appendix B: Other Scoring Methods\n\n    * B.1 2008 CWSS Kickoff Meeting\n    * B.2 2010 SANS/CWE Top 25\n    * B.3 2010 OWASP Top Ten\n    * B.4 Other Models\n  * C Appendix C: Design Considerations\n  * D Appendix D: Generalized Scoring Approaches\n  * E Appendix E: Aggregated Scoring Methods\n  * Change Log\n\n(1) Introduction\n\nThe Common Weakness Scoring System (CWSS) provides a mechanism for\nprioritizing software weaknesses in a consistent, flexible, open manner. It is\na collaborative, community-based effort that is addressing the needs of its\nstakeholders across government, academia, and industry.\n\nSoftware developers often face hundreds or thousands of individual bug reports\nfor weaknesses that are discovered in their code. In certain circumstances, a\nsoftware weakness can even lead to an exploitable vulnerability. Due to this\nhigh volume of reported weaknesses, stakeholders are often forced to\nprioritize which issues they should investigate and fix first, often using\nincomplete information. In short, people need to be able to reason and\ncommunicate about the relative importance of different weaknesses. While\nvarious scoring methods are used today, they are either ad hoc or\ninappropriate for application to the still-imprecise evaluation of software\nsecurity.\n\nSoftware developers, managers, testers, security vendors and service\nsuppliers, buyers, application vendors, and researchers must identify and\nassess weaknesses in software that could manifest as vulnerabilities when the\nsoftware is used. They then need to be able to prioritize these weaknesses and\ndetermine which to remediate based on which of them pose the greatest risk.\nWhen there are so many weaknesses to fix, with each being scored using\ndifferent scales, and often operating with incomplete information, the various\ncommunity members, managers, testers, buyers, and developers are left to their\nown methodologies to find some way of comparing disparate weaknesses and\ntranslating them into actionable information.\n\nBecause CWSS standardizes the approach for characterizing weaknesses, users of\nCWSS can invoke attack surface and environmental metrics to apply contextual\ninformation that more accurately reflects the risk to the software capability,\ngiven the unique business context it will function within and the unique\nbusiness capability it is meant to provide. This allows stakeholders to make\nmore informed decisions when trying to mitigate risks posed by weaknesses.\n\nCWSS is distinct from - but not a competitor to - the Common Vulnerability\nScoring System (CVSS). These efforts have different roles, and they can be\nleveraged together.\n\nCWSS offers:\n\n  * Quantitative Measurements: CWSS provides a quantitative measurement of the unfixed weaknesses that are present within a software application.\n  * Common Framework: CWSS provides a common framework for prioritizing security errors (\"weaknesses\") that are discovered in software applications.\n  * Customized Prioritization: in conjunction with the Common Weakness Risk Analysis Framework (CWRAF), CWSS can be used by consumers to identify the most important types of weaknesses for their business domains, in order to inform their acquisition and protection activities as one part of the larger process of achieving software assurance.\n\n### 1.1 What is CWSS?\n\nCWSS is organized into three metric groups: Base Finding, Attack Surface, and\nEnvironmental. Each group contains multiple metrics - also known as factors -\nthat are used to compute a CWSS score for a weakness.\n\n  * Base Finding metric group: captures the inherent risk of the weakness, confidence in the accuracy of the finding, and strength of controls.\n  * Attack Surface metric group: the barriers that an attacker must overcome in order to exploit the weakness.\n  * Environmental metric group: characteristics of the weakness that are specific to a particular environment or operational context.\n\nFigure 1: CWSS Metric Groups\n\n(A larger picture is available.)\n\n### 1.2 Other weakness scoring systems\n\nVarious weakness scoring systems have been used or proposed over the years.\nAutomated tools such as source code scanners typically perform their own\ncustom scoring; as a result, multiple tools can produce inconsistent scores\nfor the same weakness.\n\nThe Common Vulnerability Scoring System (CVSS) is perhaps the most similar\nscoring system. However, it has some important limitations that make it\ndifficult to adapt to software security assessment. A more detailed comparison\nis in Appendix A.\n\nMore details on other scoring systems, including adaptations to CVSS, are\nprovided in Appendix B.\n\n### 1.3 How does CWSS work?\n\n#### 1.3.1 Score Calculation\n\nEach factor in the Base Finding metric group is assigned a value. These values\nare converted to associated weights, and a Base Finding subscore is\ncalculated. The Base Finding subscore can range between 0 and 100. The same\nmethod is applied to the Attack Surface and Environmental metric group; their\nsubscores can range between 0 and 1. Finally, the three subscores are\nmultiplied together, which produces a CWSS score between 0 and 100.\n\nFigure 2: CWSS Scoring\n\n(A larger picture is available.)\n\n#### 1.3.2 Scoring Methods within CWSS\n\nThe stakeholder community is collaborating with MITRE to investigate several\ndifferent scoring methods that might need to be supported within the CWSS\nframework.\n\nMethod| Notes  \n---|---  \nTargeted| Score individual weaknesses that are discovered in the design or\nimplementation of a specific (\"targeted\") software package, e.g. a buffer\noverflow in the username of an authentication routine in line 1234 of server.c\nin an FTP server package.Automated tools and software security consultants use\ntargeted methods when evaluating the security of a software package in terms\nof the weaknesses that are contained within the package.  \nGeneralized| Score classes of weaknesses independent of any particular\nsoftware package, in order to prioritize them relative to each other (e.g.\n\"buffer overflows are higher priority than memory leaks\"). This approach is\nused by the CWE/SANS Top 25, OWASP Top Ten, and similar efforts, but also by\nsome automated code scanners.The generalized scores could vary significantly\nfrom the targeted scores that would result from a full analysis of the\nindividual occurrences of the weakness class within a specific software\npackage. For example, while the class of buffer overflows remains very\nimportant to many developers, individual buffer overflow bugs might be\nconsidered less important if they cannot be directly triggered by an attacker\nand their impact is reduced due to OS-level protection mechanisms such as\nASLR.  \nContext-adjusted| Modify scores in accordance with the needs of a specific\nanalytical context that may integrate business/mission priorities, threat\nenvironments, risk tolerance, etc. These needs are captured using vignettes\nthat link inherent characteristics of weaknesses with higher-level business\nconsiderations. This method could be applied to both targeted and generalized\nscoring.  \nAggregated| Combine the results of multiple, lower-level weakness scores to\nproduce a single, overall score (or \"grade\"). While aggregation might be most\napplicable to the targeted method, it could also be used in generalized\nscoring, as occurred in the 2010 CWE/SANS Top 25.  \n  \nNote that the current focus for CWSS is on the Targeted scoring method and a\nframework for context-adjusted scoring. Methods for aggregated scoring will\nfollow. Generalized scoring is being developed separately, primarily as part\nof the 2011 Top 25 and CWRAF.\n\n### 1.4 Who performs the scoring?\n\nCWSS scores can be automatically calculated, e.g. by a code analysis tool, or\nthey can be manually calculated by a software security consultant or\ndeveloper. Since automated analysis is not likely to have certain information\navailable - such as the application's operating environment - CWSS scoring\ncould possibly be conducted in multiple rounds: a tool first automatically\ncalculates CWSS scores, then a human analyst manually adds additional details\nand recalculates the scores.\n\n### 1.5 Who owns CWSS?\n\nCWSS is a part of the Common Weakness Enumeration (CWE) project, co-sponsored\nby the Software Assurance program in the office of Cybersecurity and\nCommunications of the U.S. Department of Homeland Security (DHS).\n\n### 1.6 Who is using CWSS?\n\nTo be most effective, CWSS supports multiple usage scenarios by different\nstakeholders who all have an interest in a consistent scoring system for\nprioritizing software weaknesses that could introduce risks to products,\nsystems, networks and services. Some of the primary stakeholders are listed\nbelow.\n\nStakeholder| Description  \n---|---  \nSoftware developers| Developers often operate within limited time frames, due\nto release cycles and limited resources. As a result, they are unable to\ninvestigate and fix every reported weakness. They may choose to concentrate on\nthe worst problems, the easiest-to-fix. In the case of automatic weakness\nfindings, they might choose to focus on the findings that are least likely to\nbe false positives.  \nSoftware development managers| Development managers create strategies for\nprioritizing and removing entire classes of weaknesses from the entire code\nbase, or at least the portion that is deemed to be most at risk, possibly by\ndefining custom \"Top-N\" lists. They must understand the security implications\nof integrating third-party software, which may contain its own weaknesses.\nThey may need to support distinct security requirements and prioritization for\neach product line.  \nSoftware acquirers| Customers, including acquisition personnel, want to obtain\nthird-party software with a reasonable level of assurance that the software\nprovider has performed due diligence in removing or avoiding weaknesses that\nare most critical to the acquirer's business and mission. Related stakeholders\ninclude CIOs, CSOs, system administrators, and end users of the software.  \nEnterprise security managers| Enterprise security managers seek to minimize\nrisk within their enterprise, both for well-known vulnerabilities in third-\nparty products, as well as vulnerabilities (or weaknesses) in their own in-\nhouse software. They may wish to use a scoring mechanism that can be\nintegrated with other security management processes, such as combining third-\nparty vulnerability scanning results (for known third-party vulnerabilities)\nwith custom application analysis (for in-house software) to help assess the\noverall risk to an asset.  \nCode analysis vendors and consultants| Vendors and consultants often have\ntheir own custom scoring techniques, but they want to provide a consistent,\ncommunity-vetted scoring mechanism for different customers.  \nEvaluators of code analysis capabilities| Evaluators analyze and measure the\ncapabilities of code analysis techniques (e.g., NIST SAMATE). They could use a\nconsistent weakness scoring mechanism to support sampling of reported\nfindings, as well as understanding the severity of these findings without\ndepending on ad hoc scoring methods that may vary widely by tool/technique.  \nOther stakeholders| Other stakeholders may include vulnerability researchers,\nadvocates of secure development, and compliance-based analysts (e.g., PCI\nDSS).  \n  \nAs of July 2014 (when CWSS 0.8 was active), there are several real-world\nimplementations of CWSS. The primary users have been code analysis vendors and\nsoftware security consultants.\n\nBack to top\n\n(2) Metric Groups\n\n### 2.1 Metric Group Factors\n\nCWSS contains the following factors, organized based on their metric group:\n\nGroup| Name| Summary  \n---|---|---  \nBase Finding| Technical Impact (TI)| The potential result that can be produced\nby the weakness, assuming that the weakness can be successfully reached and\nexploited.  \nBase Finding| Acquired Privilege (AP)| The type of privileges that are\nobtained by an attacker who can successfully exploit the weakness.  \nBase Finding| Acquired Privilege Layer (AL)| The operational layer to which\nthe attacker gains privileges by successfully exploiting the weakness.  \nBase Finding| Internal Control Effectiveness (IC)| the ability of the control\nto render the weakness unable to be exploited by an attacker.  \nBase Finding| Finding Confidence (FC)| the confidence that the reported issue\nis a weakness that can be utilized by an attacker  \nAttack Surface| Required Privilege (RP)| The type of privileges that an\nattacker must already have in order to reach the code/functionality that\ncontains the weakness.  \nAttack Surface| Required Privilege Layer (RL)| The operational layer to which\nthe attacker must have privileges in order to attempt to attack the weakness.  \nAttack Surface| Access Vector (AV)| The channel through which an attacker must\ncommunicate to reach the code or functionality that contains the weakness.  \nAttack Surface| Authentication Strength (AS)| The strength of the\nauthentication routine that protects the code/functionality that contains the\nweakness.  \nAttack Surface| Level of Interaction (IN)| the actions that are required by\nthe human victim(s) to enable a successful attack to take place.  \nAttack Surface| Deployment Scope (SC)| Whether the weakness is present in all\ndeployable instances of the software, or if it is limited to a subset of\nplatforms and/or configurations.  \nEnvironmental| Business Impact (BI)| The potential impact to the business or\nmission if the weakness can be successfully exploited.  \nEnvironmental| Likelihood of Discovery (DI)| The likelihood that an attacker\ncan discover the weakness  \nEnvironmental| Likelihood of Exploit (EX)| the likelihood that, if the\nweakness is discovered, an attacker with the required\nprivileges/authentication/access would be able to successfully exploit it.  \nEnvironmental| External Control Effectiveness (EC)| the capability of controls\nor mitigations outside of the software that may render the weakness more\ndifficult for an attacker to reach and/or trigger.  \nEnvironmental| Prevalence (P)| How frequently this type of weakness appears in\nsoftware.  \n  \nEach factor is described in more detail in subsequent sections.\n\n### 2.2 Values for Uncertainty and Flexibility\n\nCWSS can be used in cases where there is little information at first, but the\nquality of information can improve over time. It is anticipated that in many\nuse-cases, the CWSS score for an individual weakness finding may change\nfrequently, as more information is discovered. Different entities may\ndetermine separate factors at different points in time.\n\nAs such, every CWSS factor effectively has \"environmental\" or \"temporal\"\ncharacteristics, so it is not particularly useful to adopt the same types of\nmetric groups as are used in CVSS.\n\nMost factors have these four values in common:\n\nValue| Usage  \n---|---  \nUnknown| The entity calculating the score does not have enough information to\nprovide a value for the factor. This can be a signal for further\ninvestigation. For example, an automated code scanner might be able to find\ncertain weaknesses, but be unable to detect whether any authentication\nmechanisms are in place.The use of \"Unknown\" emphasizes that the score is\nincomplete or estimated, and further analysis may be necessary. This makes it\neasier to model incomplete information, and for the Business Value Context to\ninfluence final scores that were generated using incomplete information.The\nweight for this value is 0.5 for all factors, which generally produces a lower\nscore; the addition of new information (i.e., changing some factors from\n\"Unknown\" to another value) will then adjust the score upward or downward\nbased on the new information.  \nNot Applicable| The factor is being explicitly ignored in the score\ncalculation. This effectively allows the Business Value Context to dictate\nwhether a factor is relevant to the final score. For example, a customer-\nfocused CWSS scoring method might ignore the remediation effort, and a high-\nassurance environment might require investigation of all reported findings,\neven if there is low confidence in their accuracy.For a set of weakness\nfindings for an individual software package, it is expected that all findings\nwould have the same \"Not Applicable\" value for the factor that is being\nignored.  \nQuantified| The factor can be weighted using a quantified, continuous range of\n0.0 through 1.0, instead of the factor's defined set of discrete values. Not\nall factors are quantifiable in this way, but it allows for additional\ncustomization of the metric.  \nDefault| The factor's weight can be set to a default value. Labeling the\nfactor as a default allows for investigation and possible modification at a\nlater time.  \n  \n### 2.3 Base Finding Metric Group\n\nThe Base Finding metric group consists of the following factors:\n\n  * Technical Impact (TI)\n  * Acquired Privilege (AP)\n  * Acquired Privilege Layer (AL)\n  * Internal Control Effectiveness (IC)\n  * Finding Confidence (FC)\n\nThe combination of values from Technical Impact, Acquired Privilege, and\nAcquired Privilege Layer gives the user some expressive power. For example,\nthe user can characterize \"High\" Technical Impact with \"Administrator\"\nprivilege at the \"Application\" layer.\n\n#### 2.3.1 Technical Impact (TI)\n\nTechnical Impact is the potential result that can be produced by the weakness,\nassuming that the weakness can be successfully reached and exploited. This is\nexpressed in terms that are more fine-grained than confidentiality, integrity,\nand availability.\n\nThe Technical Impact should be evaluated relative to the Acquired Privilege\n(AP) and Acquired Privilege Layer (AL).\n\nValue| Code| Weight| Description  \n---|---|---|---  \nCritical| C| 1.0| Complete control over the software being analyzed, to the\npoint where operations cannot take place.  \nHigh| H| 0.9| Significant control over the software being analyzed, or access\nto critical information can be obtained.  \nMedium| M| 0.6| Moderate control over the software being analyzed, or access\nto moderately important information can be obtained.  \nLow| L| 0.3| Minimal control over the software being analyzed, or only access\nto relatively unimportant information can be obtained.  \nNone| N| 0.0| There is no technical impact to the software being analyzed at\nall. In other words, this does not lead to a vulnerability.  \nDefault| D| 0.6| The Default weight is the median of the weights for Critical,\nHigh, Medium, Low, and None.  \nUnknown| UK| 0.5| There is not enough information to provide a value for this\nfactor. Further analysis may be necessary. In the future, a different value\nmight be chosen, which could affect the score.  \nNot Applicable| NA| 1.0| This factor is being intentionally ignored in the\nscore calculation because it is not relevant to how the scorer prioritizes\nweaknesses. This factor might not be applicable in an environment with high\nassurance requirements; the user might want to investigate every weakness\nfinding of interest, regardless of confidence.  \nQuantified| Q| This factor could be quantified with custom weights.  \n  \nIf this set of values is not precise enough, CWSS users can use their own\nQuantified methods to derive a subscore. One such method involves using the\nCommon Weakness Risk Analysis Framework (CWRAF) to define a vignette and a\nTechnical Impact Scorecard. The Impact weight is calculated using vignette-\nspecific Importance ratings for different technical impacts that could arise\nfrom exploitation of the weakness, such as modification of sensitive data,\ngaining privileges, resource consumption, etc.\n\n#### 2.3.2 Acquired Privilege (AP)\n\nThe Acquired Privilege identifies the type of privileges that are obtained by\nan attacker who can successfully exploit the weakness.\n\nNotice that the values are the same as those for Required Privilege, but the\nweights are different.\n\nIn some cases, the value for Acquired Privileges may be the same as for\nRequired Privileges, which implies either (1) \"horizontal\" privilege\nescalation (e.g. from one unprivileged user to another) or (2) privilege\nescalation within a sandbox, such as an FTP-only user who can escape to the\nshell.\n\nValue| Code*| Weight| Description  \n---|---|---|---  \nAdministrator| A| 1.0| The attacker gains access to an entity with\nadministrator, root, SYSTEM, or equivalent privileges that imply full control\nover the software under analysis; or, the attacker can raise their own (lower)\nprivileges to an administrator.  \nPartially-Privileged User| P| 0.9| The attacker gains access to an entity with\nsome special privileges, but not enough privileges that are equivalent to an\nadministrator; or, the attacker can raise their own (lower) privileges to a\npartially-privileged user. For example, a user might have privileges to make\nbackups, but not to modify the software's configuration or install updates.  \nRegular User| RU| 0.7| The attacker gains access to an entity that is a\nregular user who has no special privileges; or, the attacker can raise their\nown (lower) privileges to that of a regular user.  \nLimited / Guest| L| 0.6| The attacker gains access to an entity with limited\nor \"guest\" privileges that can significantly restrict allowable activities;\nor, the attacker can raise their own (lower) privileges to a guest. Note: this\nvalue does not refer to the \"guest operating system\" concept in virtualized\nhosts.  \nNone| N| 0.1| The attacker cannot gain access to any extra privileges beyond\nthose that are already available to the attacker. (Note that this value can be\nuseful in limited circumstances in which the attacker can escape a sandbox or\nother restrictive environment but still cannot gain extra privileges, or\nobtain access as other users.)  \nDefault| D| 0.7| Median of the weights for None, Guest, Regular User,\nPartially-Privileged User, and Administrator.  \nUnknown| UK| 0.5| There is not enough information to provide a value for this\nfactor. Further analysis may be necessary. In the future, a different value\nmight be chosen, which could affect the score.  \nNot Applicable| NA| 1.0| This factor is being intentionally ignored in the\nscore calculation because it is not relevant to how the scorer prioritizes\nweaknesses. This factor might not be applicable in an environment with high\nassurance requirements that wants strict enforcement of privilege separation,\neven between already-privileged users.  \nQuantified| Q| This factor could be quantified with custom weights. Note that\nQuantified values are supported for completeness; however, since privileges\nand users are discrete entities, there might be limited circumstances in which\na quantified model would be useful.  \n  \n* A mnemonic for the main values in this factor is \"RUNLAP\" (Regular User, None, Limited, Admin, Partially-Privileged).\n\n#### 2.3.3 Acquired Privilege Layer (AL)\n\nThe Acquired Privilege Layer identifies the operational layer to which the\nattacker gains privileges by successfully exploiting the weakness.\n\nValue| Code*| Weight| Description  \n---|---|---|---  \nApplication| A| 1.0| The attacker acquires privileges that are supported\nwithin the software under analysis itself. (If the software under analysis is\nan essential part of the underlying system, such as an operating system\nkernel, then the System value may be more appropriate.)  \nSystem| S| 0.9| The attacker acquires privileges to the underlying system or\nphysical host that is being used to run the software under analysis.  \nNetwork| N| 0.7| The attacker acquires privileges to access the network.  \nEnterprise Infrastructure| E| 1.0| The attacker acquires privileges to a\ncritical piece of enterprise infrastructure, such as a router, switch, DNS,\ndomain controller, firewall, identity server, etc.  \nDefault| D| 0.9| Median of the weights for Application, System, Network, and\nEnterprise Infrastructure.  \nUnknown| UK| 0.5| There is not enough information to provide a value for this\nfactor. Further analysis may be necessary. In the future, a different value\nmight be chosen, which could affect the score.  \nNot Applicable| NA| 1.0| This factor is being intentionally ignored in the\nscore calculation because it is not relevant to how the scorer prioritizes\nweaknesses.This factor might not be applicable in an environment with high\nassurance requirements that wants strict enforcement of privilege separation,\neven between already-privileged users.  \nQuantified| Q| This factor could be quantified with custom weights. Note that\nQuantified values are supported for completeness; however, since privilege\nlayers are discrete entities, there might be limited circumstances in which a\nquantified model would be useful.  \n  \n* A mnemonic for the main values in this factor is \"SANE\" (System, Application, Network, Enterprise Infrastructure).\n\n#### 2.3.4 Internal Control Effectiveness (IC)\n\nAn Internal Control is a control, protection mechanism, or mitigation that has\nbeen explicitly built into the software (whether through architecture, design,\nor implementation). Internal Control Effectiveness measures the ability of the\ncontrol to render the weakness unable to be exploited by an attacker. For\nexample, an input validation routine that restricts input length to 15\ncharacters might be moderately effective against XSS attacks by reducing the\nsize of the XSS exploit that can be attempted.\n\nWhen there are multiple internal controls, or multiple code paths that can\nreach the same weakness, then the following guidance applies:\n\n  * For each code path, analyze each internal control that exists along the code path, and choose the Value with the lowest weight (i.e., the strongest internal control along the code path). This is called the Code Path Value.\n  * Collect all Code Path Values.\n  * Select the Code Path Value that has the highest weight (i.e., is the weakest control).\n\nThis method evaluates each code path in terms of the code path's strongest\ncontrol (since an attacker would have to bypass that control), then selects\nthe weakest code path (i.e., the easiest route for the attacker to take).\n\nValue| Code| Weight| Description  \n---|---|---|---  \nNone| N| 1.0| No controls exist.  \nLimited| L| 0.9| There are simplistic methods or accidental restrictions that\nmight prevent a casual attacker from exploiting the issue.  \nModerate| M| 0.7| The protection mechanism is commonly used but has known\nlimitations that might be bypassed with some effort by a knowledgeable\nattacker. For example, the use of HTML entity encoding to prevent XSS attacks\nmay be bypassed when the output is placed into another context such as a\nCascading Style Sheet or HTML tag attribute.  \nIndirect (Defense-in-Depth)| I| 0.5| The control does not specifically protect\nagainst exploitation of the weakness, but it indirectly reduces the impact\nwhen a successful attack is launched, or otherwise makes it more difficult to\nconstruct a functional exploit. For example, a validation routine might\nindirectly limit the size of an input, which might make it difficult for an\nattacker to construct a payload for an XSS or SQL injection attack.  \nBest-Available| B| 0.3| The control follows best current practices, although\nit may have some limitations that can be overcome by a skilled, determined\nattacker, possibly requiring the presence of other weaknesses. For example,\nthe double-submit method for CSRF protection is considered one of the\nstrongest available, but it can be defeated in conjunction with behaviors of\ncertain functionality that can read raw HTTP headers.  \nComplete| C| 0.0| The control is completely effective against the weakness,\ni.e., there is no bug or vulnerability, and no adverse consequence of\nexploiting the issue. For example, a buffer copy operation that ensures that\nthe destination buffer is always larger than the source (plus any indirect\nexpansion of the original source size) will not cause an overflow.  \nDefault| D| 0.6| Median of the weights for Complete, Best-Available, Indirect,\nModerate, Limited, and None.  \nUnknown| UK| 0.5| There is not enough information to provide a value for this\nfactor. Further analysis may be necessary. In the future, a different value\nmight be chosen, which could affect the score.  \nNot Applicable| NA| 1.0| This factor is being intentionally ignored in the\nscore calculation because it is not relevant to how the scorer prioritizes\nweaknesses.  \nQuantified| Q| This factor could be quantified with custom weights.  \n  \n#### 2.3.5 Finding Confidence (FC)\n\nFinding Confidence is the confidence that the reported issue:\n\n  1. is a weakness, and\n  2. can be triggered or utilized by an attacker\n\nValue| Code| Weight| Description  \n---|---|---|---  \nProven True| T| 1.0| The weakness is reachable by the attacker.  \nProven Locally True| LT| 0.8| The weakness occurs within an individual\nfunction or component whose design relies on safe invocation of that function,\nbut attacker reachability to that function is unknown or not present. For\nexample, a utility function might construct a database query without encoding\nits inputs, but if it is only called with constant strings, the finding is\nlocally true.  \nProven False| F| 0.0| The finding is erroneous (i.e. the finding is a false\npositive and there is no weakness), and/or there is no possible attacker role.  \nDefault| D| 0.8| Median of the weights for Proven True, Proven Locally True,\nand Proven False.  \nUnknown| UK| 0.5| There is not enough information to provide a value for this\nfactor. Further analysis may be necessary. In the future, a different value\nmight be chosen, which could affect the score.  \nNot Applicable| NA| 1.0| This factor is being intentionally ignored in the\nscore calculation because it is not relevant to how the scorer prioritizes\nweaknesses.This factor might not be applicable in an environment with high\nassurance requirements; the user might want to investigate every weakness\nfinding of interest, regardless of confidence.  \nQuantified| Q| This factor could be quantified with custom weights. Some code\nanalysis tools have precise measurements of the accuracy of specific detection\npatterns.  \n  \n### 2.4 Attack Surface Metric Group\n\nThe Attack Surface metric group consists of the following factors:\n\n  * Required Privilege (RP)\n  * Required Privilege Layer (RL)\n  * Access Vector (AV)\n  * Authentication Strength (AS)\n  * Level of Interaction (IN)\n  * Deployment Scope (SC)\n\n#### 2.4.1 Required Privilege (RP)\n\nThe Required Privilege identifies the type of privileges that an attacker must\nalready have in order to reach the code/functionality that contains the\nweakness.\n\nValue| Code*| Weight| Description  \n---|---|---|---  \nNone| N| 1.0| No privileges are required. For example, a web-based search\nengine may not require any privileges for an entity to enter a search term and\nview results.  \nLimited / Guest| L| 0.9| The entity has limited or \"guest\" privileges that can\nsignificantly restrict allowed activities; the entity might be able to\nregister or create a new account without any special requirements or proof of\nidentity. For example, a web blog might allow participants to create a user\nname and submit a valid email address before entering comments. Note: this\nvalue does not refer to the \"guest operating system\" concept in virtualized\nhosts.  \nRegular User| RU| 0.7| The entity is a regular user who has no special\nprivileges.  \nPartially-Privileged User| P| 0.6| The entity is a valid user with some\nspecial privileges, but not enough privileges that are equivalent to an\nadministrator. For example, a user might have privileges to make backups, but\nnot to modify the software's configuration or install updates.  \nAdministrator| A| 0.1| The entity has administrator, root, SYSTEM, or\nequivalent privileges that imply full control over the software or the\nunderlying OS.  \nDefault| D| 0.7| Median of the weights for None, Limited, Regular User,\nPartially-Privileged User, and Administrator.  \nUnknown| UK| 0.5| There is not enough information to provide a value for this\nfactor. Further analysis may be necessary. In the future, a different value\nmight be chosen, which could affect the score.  \nNot Applicable| NA| 1.0| This factor is being intentionally ignored in the\nscore calculation because it is not relevant to how the scorer prioritizes\nweaknesses.This factor might not be applicable in an environment with high\nassurance requirements that wants strict enforcement of privilege separation,\neven between already-privileged users.  \nQuantified| Q| This factor could be quantified with custom weights. Note that\nQuantified values are supported for completeness; however, since privileges\nand users are discrete entities, there might be limited circumstances in which\na quantified model would be useful.  \n  \n* A mnemonic for the main values in this factor is \"RUNLAP\" (Regular User, None, Limited, Admin, Partially-Privileged).\n\n#### 2.4.2 Required Privilege Layer (RL)\n\nThe Required Privilege Layer identifies the operational layer to which the\nattacker must have privileges in order to attempt to attack the weakness.\n\nValue| Code*| Weight| Description  \n---|---|---|---  \nApplication| A| 1.0| The attacker must have privileges that are supported\nwithin the software under analysis itself. (If the software under analysis is\nan essential part of the underlying system, such as an operating system\nkernel, then the System value may be more appropriate.)  \nSystem| S| 0.9| The attacker must have privileges to the underlying system or\nphysical host that is being used to run the software under analysis.  \nNetwork| N| 0.7| The attacker must have privileges to access the network.  \nEnterprise Infrastructure| E| 1.0| The attacker must have privileges on a\ncritical piece of enterprise infrastructure, such as a router, switch, DNS,\ndomain controller, firewall, identity server, etc.  \nDefault| D| 0.9| Median of the weights for Application, System, Network, and\nEnterprise Infrastructure.  \nUnknown| UK| 0.5| There is not enough information to provide a value for this\nfactor. Further analysis may be necessary. In the future, a different value\nmight be chosen, which could affect the score.  \nNot Applicable| NA| 1.0| This factor is being intentionally ignored in the\nscore calculation because it is not relevant to how the scorer prioritizes\nweaknesses.This factor might not be applicable in an environment with high\nassurance requirements that wants strict enforcement of privilege separation,\neven between already-privileged users.  \nQuantified| Q| This factor could be quantified with custom weights. Note that\nQuantified values are supported for completeness; however, since privilege\nlayers are discrete entities, there might be limited circumstances in which a\nquantified model would be useful.  \n  \n* A mnemonic for the main values in this factor is \"SANE\" (System, Application, Network, Enterprise Infrastructure).\n\n#### 2.4.3 Access Vector (AV)\n\nThe Access Vector identifies the channel through which an attacker must\ncommunicate to reach the code or functionality that contains the weakness.\nNote that these values are very similar to the ones used in CVSS, except CWSS\ndistinguishes between physical access and local (shell/account) access.\n\nWhile there is a close relationship between Access Vector and Required\nPrivilege Layer, the two are distinct. For example, an attacker with\n\"physical\" access to a router might be able to affect the Network or\nEnterprise layer.\n\nValue| Code| Weight| Description  \n---|---|---|---  \nInternet| I| 1.0| An attacker must have access to the Internet to reach the\nweakness.  \nIntranet| R| 0.8| An attacker must have access to an enterprise intranet that\nis shielded from direct access from the Internet, e.g. by using a firewall,\nbut otherwise the intranet is accessible to most members of the enterprise.  \nPrivate Network| V| 0.8| An attacker must have access to a private network\nthat is only accessible to a narrowly-defined set of trusted parties.  \nAdjacent Network| A| 0.7| An attacker must have access to a physical interface\nto the network, such as the broadcast or collision domain of the vulnerable\nsoftware. Examples of local networks include local IP subnet, Bluetooth, IEEE\n802.11, and local Ethernet segment.  \nLocal| L| 0.5| The attacker must have an interactive, local (shell) account\nthat interfaces directly with the underlying operating system.  \nPhysical| P| 0.2| The attacker must have physical access to the system that\nthe software runs on, or otherwise able to interact with the system via\ninterfaces such as USB, CD, keyboard, mouse, etc.  \nDefault| D| 0.75| Median of weights for relevant values.  \nUnknown| U| 0.5  \nNot Applicable| NA| 1.0| This factor is being intentionally ignored in the\nscore calculation because it is not relevant to how the scorer prioritizes\nweaknesses.  \nQuantified| Q| This factor could be quantified with custom weights. Note that\nQuantified values are supported for completeness; however, since access\nvectors are discrete entities, there might be limited circumstances in which a\nquantified model would be useful.  \n  \n#### 2.4.4 Authentication Strength (AS)\n\nThe Authentication Strength covers the strength of the authentication routine\nthat protects the code/functionality that contains the weakness.\n\nWhen there are multiple authentication routine, or multiple code paths that\ncan reach the same weakness, then the following guidance applies:\n\n  * For each code path, analyze each authentication routine that exists along the code path, and choose the Value with the lowest weight (i.e., the strongest authentication routine along the code path). This is called the Code Path Value.\n  * Collect all Code Path Values.\n  * Select the Code Path Value that has the highest weight (i.e., contains the weakest routine).\n\nThis method evaluates each code path in terms of the code path's strongest\nauthentication routine (since an attacker would have to bypass that control),\nthen selects the weakest code path (i.e., the easiest route for the attacker\nto take).\n\nValue| Code| Weight| Description  \n---|---|---|---  \nStrong| S| 0.7| The weakness requires strongest-available methods to tie the\nentity to a real-world identity, such as hardware-based tokens, and/or multi-\nfactor authentication.  \nModerate| M| 0.8| The weakness requires authentication using moderately strong\nmethods, such as the use of certificates from untrusted authorities,\nknowledge-based authentication, or one-time passwords.  \nWeak| W| 0.9| The weakness requires a simple, weak authentication method that\nis easily compromised using spoofing, dictionary, or replay attacks, such as a\nstatic password.  \nNone| N| 1.0| The weakness does not require any authentication at all.  \nDefault| D| 0.85| Median of values for Strong, Moderate, Weak, and None.  \nUnknown| UK| 0.5| There is not enough information to provide a value for this\nfactor. Further analysis may be necessary. In the future, a different value\nmight be chosen, which could affect the score.  \nNot Applicable| NA| 1.0| This factor is being intentionally ignored in the\nscore calculation because it is not relevant to how the scorer prioritizes\nweaknesses.This might not be applicable in an environment with high assurance\nrequirements that seek to eliminate all weaknesses.  \nQuantified| Q| This factor could be quantified with custom weights.  \n  \n#### 2.4.5 Level of Interaction (IN)\n\nThe Level of Interaction covers the actions that are required by the human\nvictim(s) to enable a successful attack to take place.\n\nValue| Code| Weight| Description  \n---|---|---|---  \nAutomated| A| 1.0| No human interaction is required.  \nTypical/Limited| T| 0.9| The attacker must convince the user to perform an\naction that is common or regarded as \"normal\" within typical product\noperation. For example, clicking on a link in a web page, or previewing the\nbody of an email, is common behavior.  \nModerate| M| 0.8| The attacker must convince the user to perform an action\nthat might appear suspicious to a cautious, knowledgeable user. For example:\nthe user has to accept a warning that suggests the attacker's payload might\ncontain dangerous content.  \nOpportunistic| O| 0.3| The attacker cannot directly control or influence the\nvictim, and can only passively capitalize on mistakes or actions of others.  \nHigh| H| 0.1| A large amount of social engineering is required, possibly\nincluding ignorance or negligence on the part of the victim.  \nNo interaction| NI| 0.0| There is no interaction possible, not even\nopportunistically; this typically would render the weakness as a \"bug\" instead\nof leading to a vulnerability. Since CWSS is for security, the weight is 0.  \nDefault| D| 0.55| Median of values for Automated, Limited, Moderate,\nOpportunistic, High, and No interaction.  \nUnknown| UK| 0.5| There is not enough information to provide a value for this\nfactor. Further analysis may be necessary. In the future, a different value\nmight be chosen, which could affect the score.  \nNot Applicable| NA| 1.0| This factor is being intentionally ignored in the\nscore calculation because it is not relevant to how the scorer prioritizes\nweaknesses.This might not be applicable in an environment with high assurance\nrequirements, or an environment that has high concerns about insider attacks\nbetween people with an established trust relationship.  \nQuantified| Q| This factor could be quantified with custom weights.  \n  \n#### 2.4.6 Deployment Scope (SC)\n\nDeployment Scope identifies whether the weakness is present in all deployable\ninstances of the software, or if it is limited to a subset of platforms and/or\nconfigurations. For example, a numeric calculation error might only be\napplicable for software that is running under a particular OS and a 64-bit\narchitecture, or a path traversal issue might only affect operating systems\nfor which \"\\\" is treated as a directory separator.\n\nValue| Code*| Weight| Description  \n---|---|---|---  \nAll| A| 1.0| Present in all platforms or configurations  \nModerate| M| 0.9| Present in common platforms or configurations  \nRare| R| 0.5| Only present in rare platforms or configurations  \nPotentially Reachable| P| 0.1| Potentially reachable**, but all code paths are\ncurrently safe, and/or the weakness is in dead code  \nDefault| D| 0.7| The median of weights for RAMP values  \nUnknown| UK| 0.5| There is not enough information to provide a value for this\nfactor. Further analysis may be necessary. In the future, a different value\nmight be chosen, which could affect the score.  \nNot Applicable| NA| 1.0| This factor is being intentionally ignored in the\nscore calculation because it is not relevant to how the scorer prioritizes\nweaknesses.  \nQuantified| Q| This factor could be quantified with custom weights. The user\nmay know what percentage of shipped (or supported) software contains this bug.  \n  \n* A mnemonic for the main values in this factor is \"RAMP\" (Rare, All, Moderate, Potentially Reachable).\n\n** \"Potentially Reachable\" has some overlap with \"Locally True\" in the Finding\nConfidence (FC) factor.\n\n### 2.5 Environmental Metric Group\n\nThe Environmental metric group consists of the following factors:\n\n  * Business Impact (BI)\n  * Likelihood of Discovery (DI)\n  * Likelihood of Exploit (EX)\n  * External Control Effectiveness (EC)\n  * Prevalence (P)\n\n#### 2.5.1 Business Impact (BI)\n\nBusiness Impact describes the potential impact to the business or mission if\nthe weakness can be successfully exploited.*\n\nValue| Code| Weight| Description  \n---|---|---|---  \nCritical| C| 1.0| The business/mission could fail.  \nHigh| H| 0.9| The operations of the business/mission would be significantly\naffected.  \nMedium| M| 0.6| The business/mission would be affected, but without extensive\ndamage to regular operations.  \nLow| L| 0.3| Minimal impact on the business/mission.  \nNone| N| 0.0| No impact.  \nDefault| D| 0.6| The median of weights for Critical, High, Medium, Low, and\nNone.  \nUnknown| UK| 0.5| There is not enough information to provide a value for this\nfactor. Further analysis may be necessary. In the future, a different value\nmight be chosen, which could affect the score.  \nNot Applicable| NA| 1.0| This factor is being intentionally ignored in the\nscore calculation because it is not relevant to how the scorer prioritizes\nweaknesses.This factor might not be applicable in contexts in which the\nbusiness impact is irrelevant, or if the impact is being assessed and\nconsidered in analytical processes that are outside of the CWSS score itself.  \nQuantified| Q| This factor could be quantified with custom weights. Some\norganizations might have specific measurements for the business value of the\nasset, for example, which could be integrated into this measurement.  \n  \n* Since business concerns vary widely across organizations, CWSS 1.0 does not attempt to provide a more precise breakdown, e.g. in terms of financial, reputational, physical, legal, or other types of damage. This factor can be quantified to support any externally-defined models.\n\n#### 2.5.2 Likelihood of Discovery (DI)\n\nLikelihood of Discovery* is the likelihood that an attacker can discover the\nweakness.\n\nValue| Code| Weight| Description  \n---|---|---|---  \nHigh| H| 1.0| It is very likely that an attacker can discover the weakness\nquickly and with little effort using simple techniques, without access to\nsource code or other artifacts that simplify weakness detection.  \nMedium| M| 0.6| An attacker might be able to discover the weakness, but would\nrequire certain skills to do so, possibly requiring source code access or\nreverse engineering knowledge. It may require some time investment to find the\nissue.  \nLow| L| 0.2| An attacker is unlikely to discover the weakness without highly\nspecialized skills, access to source code (or its equivalent), and a large\ntime investment.  \nDefault| D| 0.6| The median of the High, Medium, and Low values.  \nUnknown| UK| 0.5| There is not enough information to provide a value for this\nfactor. Further analysis may be necessary. In the future, a different value\nmight be chosen, which could affect the score.  \nNot Applicable| NA| 1.0| This factor is being intentionally ignored in the\nscore calculation because it is not relevant to how the scorer prioritizes\nweaknesses.This might not be applicable when the scorer assumes that all\nweaknesses will be discovered by an attacker.  \nQuantified| Q| This factor could be quantified with custom weights.  \n  \n* This factor was considered for removal in CWSS 1.0, since it can be difficult to measure and can be influenced by other factors such as Acquired Privilege, Technical Impact, and Prevalence. However, it has been preserved to reflect that some developers will use likelihood of discovery to help prioritize how quickly an issue should be fixed.\n\n#### 2.5.3 Likelihood of Exploit (EX)\n\nLikelihood of Exploit is the likelihood that, if the weakness is discovered,\nan attacker with the required privileges/authentication/access would be able\nto successfully exploit it.\n\nValue| Code| Weight| Description  \n---|---|---|---  \nHigh| H| 1.0| It is highly likely that an attacker would target this weakness\nsuccessfully, with a reliable exploit that is easy to develop.  \nMedium| M| 0.6| An attacker would probably target this weakness successfully,\nbut the chances of success might vary, or require multiple attempts to\nsucceed.  \nLow| L| 0.2| An attacker probably would not target this weakness, or could\nhave very limited chances of success.  \nNone| N| 0.0| An attacker has no chance of success; i.e., the issue is a \"bug\"\nbecause there is no attacker role, and no benefit to the attacker.  \nDefault| D| 0.6| Median of the High, Medium, and Low values. The \"None\" value\nis ignored with the expectation that few weakness findings would be scored\nusing the value, and including it in the median calculation would reduce the\nweight to a non-intuitive level.  \nUnknown| UK| 0.5| There is not enough information to provide a value for this\nfactor. Further analysis may be necessary. In the future, a different value\nmight be chosen, which could affect the score.  \nNot Applicable| NA| 1.0| This factor is being intentionally ignored in the\nscore calculation because it is not relevant to how the scorer prioritizes\nweaknesses.For example, the scorer might want to assume that attackers could\nexploit any weakness they can find, or be willing to invest significant\nresources to work around any possible barriers to exploit success.  \nQuantified| Q| This factor could be quantified with custom weights.  \n  \nNote that this factor is influenced by the Impact of a weakness, since\nattackers often target weaknesses that have the most severe impacts.\nAlternately, they may target weaknesses that are easy to trigger. It is also\ninfluenced by other factors such as the effectiveness of internal and external\ncontrols.\n\nIt might seem that the prevalence is also an influence, but prevalence is more\nclosely related to Likelihood of Discovery.\n\n#### 2.5.4 External Control Effectiveness (EC)\n\nExternal Control Effectiveness is the capability of controls or mitigations\noutside of the software that may render the weakness more difficult for an\nattacker to reach and/or trigger. For example, Address Space Layout\nRandomization (ASLR) and similar technologies reduce, but do not eliminate,\nthe chances of success for a buffer overflow attack. However, ASLR is not\ndirectly instantiated within the software itself.\n\nWhen there are multiple external controls, or multiple code paths that can\nreach the same weakness, then the following guidance applies:\n\n  * For each code path, analyze each external control that exists along the code path, and choose the Value with the lowest weight (i.e., the strongest external control along the code path). This is called the Code Path Value.\n  * Collect all Code Path Values.\n  * Select the Code Path Value that has the highest weight (i.e., is the weakest control).\n\nThis method evaluates each code path in terms of the code path's strongest\ncontrol (since an attacker would have to bypass that control), then selects\nthe weakest code path (i.e., the easiest route for the attacker to take).\n\nValue| Code| Weight| Description  \n---|---|---|---  \nNone| N| 1.0| No controls exist.  \nLimited| L| 0.9| There are simplistic methods or accidental restrictions that\nmight prevent a casual attacker from exploiting the issue.  \nModerate| M| 0.7| The protection mechanism is commonly used but has known\nlimitations that might be bypassed with some effort by a knowledgeable\nattacker.  \nIndirect (Defense-in-Depth)| I| 0.5| The control does not specifically protect\nagainst exploitation of the weakness, but it indirectly reduces the impact\nwhen a successful attack is launched, or otherwise makes it more difficult to\nconstruct a functional exploit.For example, Address Space Layout Randomization\n(ASLR) and similar technologies reduce, but do not eliminate, the chances of\nsuccess in a buffer overflow attack. Since the response is typically to exit\nthe process, the result is still a denial of service.  \nBest-Available| B| 0.3| The control follows best current practices, although\nit may have some limitations that can be overcome by a skilled, determined\nattacker, possibly requiring the presence of other weaknesses. For example,\nTransport Layer Security (TLS) / SSL 3 are in operation throughout much of the\nWeb, and stronger methods generally are not available due to compatibility\nissues.  \nComplete| C| 0.1| The control is completely effective against the weakness,\ni.e., there is no bug or vulnerability, and no adverse consequence of\nexploiting the issue. For example, a sandbox environment might restrict file\naccess operations to a single working directory, which would protect against\nexploitation of path traversal.A non-zero weight is used to reflect the\npossibility that the external control could be accidentally removed in the\nfuture, e.g. if the software's environment changes.  \nDefault| D| 0.6| The median of Complete, Best-Available, Indirect, Moderate,\nLimited, and None.  \nUnknown| UK| 0.5| There is not enough information to provide a value for this\nfactor. Further analysis may be necessary. In the future, a different value\nmight be chosen, which could affect the score.  \nNot Applicable| NA| 1.0| This factor is being intentionally ignored in the\nscore calculation because it is not relevant to how the scorer prioritizes\nweaknesses.  \nQuantified| Q| This factor could be quantified with custom weights.  \n  \n#### 2.5.5 Prevalence (P)\n\nThe Prevalence* of a finding identifies how frequently this type of weakness\nappears in software.\n\nThis factor is intended for use in Generalized scoring of classes of\nweaknesses, such as the development of custom Top-N weakness lists. When\nscoring an individual weakness finding in an automated-scanning context, this\nfactor is likely to use a \"Not Applicable\" value.\n\nValue| Code| Weight**| Description  \n---|---|---|---  \nWidespread| W| 1.0| The weakness is found in most or all software in the\nassociated environment, and may occur multiple times within the same software\npackage.  \nHigh| H| 0.9| The weakness is encountered very often, but it is not\nwidespread.  \nCommon| C| 0.8| The weakness is encountered periodically.  \nLimited| L| 0.7| The weakness is encountered rarely, or never.  \nDefault| D| 0.85| The median of Limited, Common, High, and Widespread.  \nUnknown| UK| 0.5| There is not enough information to provide a value for this\nfactor. Further analysis may be necessary. In the future, a different value\nmight be chosen, which could affect the score.  \nNot Applicable| NA| 1.0| This factor is being intentionally ignored in the\nscore calculation because it is not relevant to how the scorer prioritizes\nweaknesses.When performing targeted scoring against specific weakness findings\nin an application, Prevalence is normally expected to be irrelevant, since the\nindividual application and the analytical techniques determine how frequently\nthe weakness occurs, and many aggregated scoring methods will generate larger\nscores if there are more weaknesses.  \nQuantified| Q| This factor could be quantified with custom weights. Precise\nprevalence data may be available within limited use cases, provided the user\nis tracking weakness data at a low level of granularity. For example, a\ndeveloper may be tracking weaknesses across a suite of products, or a code-\nauditing vendor could measure prevalence from the software analyzed across the\nentire customer base. In a previous version of CWSS, prevalence was calculated\nbased on from raw voting data that was collected for the 2010 Top 25, which\nused discrete values (range 1 to 4) which were then adjusted to a 1-to-10\nrange.  \n  \n* Note that this factor might be considered for removal in future versions. However, it is too closely tied to Generalized scoring methods and CWRAF to be removed within CWSS 1.0.\n\n** Since software can be successfully attacked even in the presence of a\nsingle weakness, the selected weights do not provide significant distinction\nbetween each other.\n\nBack to top\n\n(3) CWSS Score Formula\n\nA CWSS 1.0 score can range between 0 and 100. It is calculated as follows:\n\nBaseFindingSubscore * AttackSurfaceSubscore * EnvironmentSubscore\n\nThe BaseFindingSubscore supports values between 0 and 100. Both the\nAttackSurfaceSubscore and EnvironmentSubscore support values between 0 and 1.\n\n### 3.1 Base Finding Subscore\n\nThe Base Finding subscore (BaseFindingSubscore) is calculated as follows:\n\nBase = [ (10 * TechnicalImpact + 5*(AcquiredPrivilege +\nAcquiredPrivilegeLayer) + 5*FindingConfidence) * f(TechnicalImpact) *\nInternalControlEffectiveness ] * 4.0\n\nf(TechnicalImpact) = 0 if TechnicalImpact = 0; otherwise f(TechnicalImpact) =\n1.\n\nThe maximum potential BaseFindingSubscore is 100.\n\nThe definition of f(TechnicalImpact) has an equivalent in CVSS. It is used to\nensure that if the Technical Impact is 0, that the other added factors do not\ninadvertently generate a non-zero score.\n\nTechnicalImpact and the AcquiredPrivilege/AcquiredPrivilegeLayer combination\nare given equal weight, each accounting for 40% of the BaseFindingSubscore.\n(Each generate a sub-value with a maximum of 10). There is some adjustment for\nFinding Confidence, which accounts for 20% of the Base (maximum of 5). The\nInternalControlEffectiveness can adjust the score downward, perhaps to 0,\ndepending on the strength of any internal controls that have been applied to\nthe issue. After application of InternalControlEffectiveness, the possible\nrange of results is between 0 and 25, so the 4.0 coefficient is used to adjust\nthe BaseFindingSubscore to a range between 0 and 100.\n\n### 3.2 Attack Surface Subscore\n\nThe AttackSurfaceSubscore is calculated as:\n\n[ 20*(RequiredPrivilege + RequiredPrivilegeLayer + AccessVector) +\n20*DeploymentScope + 15*LevelOfInteraction + 5*AuthenticationStrength ] /\n100.0\n\nThe combination of required privileges / access makes up 60% of the Attack\nSurface subscore; deployment scope, another 20%; interaction, 15%; and\nauthentication, 5%. The authentication requirements are not given much focus,\nunder the assumption that strong proof of identity will not significantly\ndeter an attacker from attempting to exploit the vulnerability.\n\nThis generates a range of values between 0 and 100, which are then divided by\n100.\n\n### 3.3 Environmental Subscore\n\nThe EnvironmentalSubscore is calculated as:\n\n[ (10*BusinessImpact + 3*LikelihoodOfDiscovery + 4*LikelihoodOfExploit +\n3*Prevalence) * f(BusinessImpact) * ExternalControlEffectiveness ] / 20.0\n\nf(BusinessImpact) = 0 if BusinessImpact == 0; otherwise f(BusinessImpact) = 1\n\nBusinessImpact accounts for 50% of the environmental score, and it can move\nthe final score to 0. ExternalControlEffectiveness is always non-zero (to\naccount for the risk that it can be inadvertently removed if the environment\nchanges), but otherwise it can have major impact on the final score. The\ncombination of LikelihoodOfDiscovery and LikelihoodOfExploit accounts for 35%\nof the score, and Prevalence at 15%.\n\n### 3.4 Additional Features of the Formula\n\nThere is significant diversity in the kinds of scores that can be represented,\nalthough the use of multiplication of many different factors, combined with\nmultiple weights with small values, means that the range of potential scores\nis somewhat skewed towards lower values.\n\nSince \"Not Applicable\" values have a weight of 1, the formula always has a\npotential maximum score of 100.0. In extremely rare cases in which certain\nfactors are treated as Not Applicable (e.g., Technical Impact, Business\nImpact, and Internal Control Effectiveness), then the minimum possible score\ncould be non-zero.\n\nWhen default values are used for a large number of factors for a single score,\nusing the median weights as defined in CWSS 1.0, the scores will skew heavily\nto the low side. The median weight for a factor does not necessarily reflect\nthe most likely value that could be used, so the selection of Default weights\nmay be changed in future versions. Ideally, the formula would have a property\nin which the use of many default values produces a score that is relatively\nclose to 50; the selection of non-default values would adjust the final score\nupward or downward, thereby increasing precision.\n\nThe use of \"Unknown\" values also generally produces scores that skew to the\nlow side. This might be a useful feature, since scores will be higher if they\nhave more specific information.\n\nBack to top\n\n(4) CWSS Vectors, Scoring Examples, and Score Portability\n\nUsing the Codes as specified for each factor, a CWSS score can be stored in a\ncompact, machine-parsable, human-readable format that provides the details for\nhow the score was generated. This is very similar to how CVSS vectors are\nconstructed.\n\nUnlike CVSS, not all CWSS factors can be described symbolically with discrete\nvalues. Any factor can be quantified with continuous weights that override the\noriginally-defined default discrete values, using the \"Q\" value. When\ncalculated using CWRAF, the Impact factor is effectively an expression of 32\nseparate Technical Impacts and layers, many of which would not be applicable\nto a particular weakness. Treating each impact as a separate factor would\nroughly double the number of factors required to calculate a CWSS score. In\naddition, CWRAF's use of Business Value Context (BVC) to adjust scores for\nbusiness-specific concerns also means that a CWSS score and its vector may\nappear to be inconsistent if they are \"transported\" to other domains or\nvignettes.\n\nWith this concern in mind, a CWSS 1.0 vector should explicitly list the\nweights for each factor, even though it increases the size of the vector\nrepresentation.\n\nThe format of a single factor in a CWSS vector is:\n\nFactorName:Value,Weight\n\nFor example, \"P:NA,1.0\" specifies a \"Not Applicable\" value for Prevalence with\na weight of 1.0. A specifier of \"AV:P,0.2\" indicates the \"Physical\" value for\nAccess Vector with a weight of 0.2.\n\nFactors are separated by forward slash characters, such as:\n\nAV:I,1.0/RP:L,0.9/AS:N,1.0\n\nwhich lists values and weights for \"AV\" (Access Vector), \"RP\" (Required\nPrivilege Level), and \"AS\" (Authentication Strength).\n\nIf a CWSS vector is provided that does not list the actual weights for a\nvalue, then an implementation should report a possible error or inconsistency,\ntry to infer the CWSS version based on the vector's factors and values, re-\ncalculate the CWSS score based on the inferred version, and compare this to\nthe original score. If the scores are inconsistent, the implementation should\nreport a possible error or inconsistency.\n\n### 4.1 Example: Business-critical application\n\nConsider a reported weakness in which an application is the primary source of\nincome for a company, thus has critical business value. The application allows\narbitrary Internet users to sign up for an account using only an email\naddress. A user can then exploit the weakness to obtain administrator\nprivileges for the application, but the attack cannot succeed until the\nadministrator views a report of recent user activities - a common occurrence.\nThe attacker cannot take complete control over the application, but can delete\nits users and data. Suppose further that there are no controls to prevent the\nweakness, but the fix for the issue is simple, and limited to a few lines of\ncode.\n\nThis situation could be captured in the following CWSS vector:\n\n(TI:H,0.9/AP:A,1.0/AL:A,1.0/IC:N,1.0/FC:T,1.0/\n\nRP:L,0.9/RL:A,1.0/AV:I,1.0/AS:N,1.0/IN:T,0.9/SC:A,1.0/\n\nBI:C,0.9/DI:H,1.0/EX:H,1.0/EC:N,1.0/P:NA,1.0)\n\nThe vector has been split into multiple lines for readability. Each line\nrepresents a metric group.\n\nThe factors and values are as follows:\n\nFactor| Value  \n---|---  \nTechnical Impact| High  \nAcquired Privilege| Administrator  \nAcquired Privilege Layer| Application  \nInternal Control Effectiveness| None  \nFinding Confidence| Proven True  \nRequired Privilege| Guest  \nRequired Privilege Layer| Application  \nAccess Vector| Internet  \nAuthentication Strength| None  \nLevel of Interaction| Typical/Limited  \nDeployment Scope| All  \nBusiness Impact| Critical  \nLikelihood of Discovery| High  \nLikelihood of Exploit| High  \nExternal Control Effectiveness| None  \nPrevalence| Not Applicable  \n  \nThe CWSS score for this vector is 92.6, derived as follows:\n\n  * BaseSubscore:\n    * [ (10 * TI + 5*(AP + AL) + 5*FC) * f(TI) * IC ] * 4.0\n    * f(TI) = 1\n    * = [ (10 * 0.9 + 5*(1.0 + 1.0) + 5*1.0) * 1 * 1.0 ] * 4.0\n    * = [ (9.0 + 10.0 + 5.0) * 1.0 ] * 4.0\n    * = 24.0 * 4.0\n    * = 96.0\n  * AttackSurfaceSubscore:\n    * [ 20*(RP + RL + AV) + 20*SC + 15*IN + 5*AS ] / 100.0\n    * = [ 20*(0.9 + 1.0 + 1.0) + 20*1.0 + 15*0.9 + 5*1.0 ] / 100.0\n    * = [ 58.0 + 20.0 + 13.5 + 5.0 ] / 100.0\n    * = 96.5 / 100.0\n    * = 0.965\n  * EnvironmentSubscore:\n    * [ (10*BI + 3*DI + 4*EX + 3*P) * f(BI) * EC ] / 20.0\n    * f(BI) = 1\n    * = [ (10*1.0 + 3*1.0 + 4*1.0 + 3*1.0) * 1 * 1.0 ] / 20.0\n    * = [ (10.0 + 3.0 + 4.0 + 3.0) * 1.0 ] / 20.0\n    * = 20.0 / 20.0\n    * = 1.0\n\nThe final score is:\n\n96.0 * 0.965 * 1.0 = 92.64 == 92.6\n\n### 4.2 Example: Wiki with limited business criticality\n\nConsider this CWSS vector. Suppose the software is a wiki that is used for\ntracking social events for a mid-size business. Some of the most important\ncharacteristics are that there is medium technical impact to an application\nadministrator from a regular user of the application, but the application is\nnot business-critical, so the overall business impact is low. Also note that\nmost of the environment factors are set to \"Not Applicable.\"\n\n(TI:M,0.6/AP:A,1.0/AL:A,1.0/IC:N,1.0/FC:T,1.0/\n\nRP:RU,0.7/RL:A,1.0/AV:I,1.0/AS:W,0.9/IN:A,1.0/SC:NA,1.0/\n\nBI:L,0.3/DI:NA,1.0/EX:NA,1.0/EC:N,1.0/RE:NA,1.0/P:NA,1.0)\n\nThe vector has been split into multiple lines for readability. Each line\nrepresents a metric group.\n\nThe factors and values are as follows:\n\nFactor| Value  \n---|---  \nTechnical Impact| Medium  \nAcquired Privilege| Administrator  \nAcquired Privilege Layer| Application  \nInternal Control Effectiveness| None  \nFinding Confidence| Proven True  \nRequired Privilege| Regular User  \nRequired Privilege Layer| Application  \nAccess Vector| Internet  \nAuthentication Strength| Weak  \nLevel of Interaction| Automated  \nDeployment Scope| Not Applicable  \nBusiness Impact| Low  \nLikelihood of Discovery| Not Applicable  \nLikelihood of Exploit| Not Applicable  \nExternal Control Effectiveness| None  \nPrevalence| Not Applicable  \n  \nThe CWSS score for this vector is 51.1, derived as follows:\n\n  * BaseSubscore:\n    * [ (10 * TI + 5*(AP + AL) + 5*FC) * f(TI) * IC ] * 4.0\n    * f(TI) = 1\n    * = [ (10 * 0.6 + 5*(1 + 1) + 5*1) * f(TI) * 1 ] * 4.0\n    * = 84.0\n  * AttackSurfaceSubscore:\n    * [ 20*(RP + RL + AV) + 20*SC + 15*IN + 5*AS ] / 100.0\n    * = [ 20*(0.7 + 1 + 1) + 20*1.0 + 15*1.0 + 5*0.9 ] / 100.0\n    * = [ 54.0 + 20.0 + 15.0 + 4.5 ] / 100.0\n    * = 93.5 / 100.0\n    * = 0.94 (0.935)\n  * EnvironmentSubscore:\n    * [ (10*BI + 3*DI + 4*EX + 3*P) * f(BI) * EC ] / 20.0\n    * f(BI) = 1\n    * = [ (10*0.3 + 3*1.0 + 4*1.0 + 3*1.0) * f(BI) * 1 ] / 20.0\n    * = [ (3.0 + 3.0 + 4.0 + 3.0) * 1.0 * 1.0 ] / 20.0\n    * = [ 13.0 * 1.0 ] / 20.0\n    * = 0.65\n\nThe final score is:\n\n84.0 * 0.935 * 0.65 = 51.051 == 51.1\n\n### 4.3 Other Approaches to CWSS Score Portability\n\nInstead of recording each individual weight within a CWSS vector, several\nother methods could be adopted.\n\nOne possibility is to extend the CWSS vectors to record additional metadata\nthat does not affect the score but reflects the version or other important\ninformation. The metadata portion would not necessarily need to capture\nweights, per se. For example, the CWSS version could be recorded by using a\n\"factor\" name such as \"V\" along with a value that represents the CWSS version,\ne.g. \"V:1.1\". This would add approximately 4 bytes to each CWSS vector.\nHowever, if the version is encoded within a vector, then the assigned weights\nwould no longer need to be recorded (except for Quantified values), so the\nresulting vectors could be much shorter.\n\nA different approach would be to attach metadata to a set of generated CWSS\nscores (such as the Technical Impact Scorecard if CWRAF is used), but it could\nbe too easy for this metadata to become detached from the scores/vectors.\nQuantified factors would still need to be represented within a vector, since\nthey could vary for each weakness finding.\n\nAnother approach is that when CWSS scores are transferred from one party to\nthe other, then the receiving party could re-calculate the scores from the\ngiven CWSS vectors, then compare the re-calculated scores with the original\nscores. A difference in scores would suggest that different mechanisms are in\nuse between the provider and receiverd, possibly a different CWSS version.\n\nBack to top\n\n(5) Considerations for CWSS beyond 1.0\n\nFor future versions, the following should be considered.\n\n### 5.1 Current Limitations of the Scoring Method\n\n#### 5.1.1 Score Weight and Distribution\n\nThe formula still needs to be refined to ensure that the range of potential\nscores is more evenly distibuted. There are probably unexpected interactions\nbetween factors that must be identified and resolved. CVSS scoring contains\nbuilt-in adjustments that prevent many factors from affecting the score too\nmuch, while also giving some preference to impact over exploitability; similar\nbuilt-in adjustments may need to be performed for CWSS.\n\n#### 5.1.2 Raising the Priority of Design or Architectural Flaws\n\nCWSS 1.0 does not provide any clear mechanism to give higher priority to\ndesign or architecture flaws versus implementation vulnerabilities. This can\nbe an important distinction, because compromise of a design flaw could\npotentially compromise the entire software package, but a single CWSS score\nfor such a design flaw could become \"buried\" if there are many different\nimplementation bugs. For example, a single report of the lack of an input\nvalidation framework might be expected to carry more weight than multiple\nindividual XSS and SQL injection bugs. This issue can be exacerbated if\naggregated scores are used.\n\nIt might be possible to use the Business Impact factor, but this factor might\nalready be in use for its originally-intended purpose.\n\nA \"Weakness Scope\" factor could be used to cover the following scenario.\nWithin CWSS 1.0, design and architecture flaws receive the same relative\npriority as implementation issues, even though they may lead to a complete\ncompromise of the software. It may be reasonable to use a separate factor in\norder to give design/architecture flaws a larger weight, e.g. so that the lack\nof an input validation framework (one \"finding\") can be given higher priority\nthan hundreds of individual findings for XSS or SQL injection.*\n\n#### 5.1.3 Compound Elements (Chains, Etc.)\n\nThere are also some challenges for scoring findings that combine multiple\nweaknesses. By their nature, compound elements such as chains and composites\ninvolve interactions between multiple reported weaknesses. With some detection\ntechniques such as automated code scanning, multiple CWE entries might be\nreported as separate findings for a single chain or composite. If individual\nscores within each link of the chain are counted separately, this could wind\nup artificially inflating any aggregate scores. However, sometimes the\ncompound element is more than the sum of its parts, and the combination of\nmultiple weaknesses has a higher impact than the maximum impact of any\nindividual weakness. This is not well-handled in the current CWSS scheme;\nhowever, it should be noted that CVSSv3 is likely to include guidance for\nscoring chains as their own independent entities, so future versions of CWSS\nmight follow suit.\n\n#### 5.1.4 Other Types of Software Assessments\n\nIt is anticipated that CWSS may be considered for use in other types of\nsoftware assessments, such as safety, reliability, and quality. Weaknesses or\nother issues related to code quality might receive higher prioritization\nwithin a CWRAF vignette-oriented scheme, since safety, compliance, or\nmaintainability might be important. This usage is not explicitly supported\nwith CWSS 1.0. However, such quality-related issues could be scored in which\nthe Required Privilege is the same as Acquired Privilege, and the Required\nPrivilege Layer is the same as the Acquired Privilege Layer; the Business\nImpact could also be used.\n\n### 5.2 Community Review and Validation of Factors\n\nPending community review, future versions of CWSS might modify, remove, or add\nfactors to the methodology.\n\nAs long as there are enough stakeholders or use-cases for whom a factor is\nimportant, then it is a strong argument for keeping the factor within CWSS,\neven if it is not essential for everyone. Others could use the \"Not\nApplicable\" value when the factor is not relevant to their own environment.\nHowever, the more factors in the metric, the more complexity.\n\n### 5.3 Impact of CWSS and CWE Version Changes to Scoring\n\nThe values for the factors involved in scoring could change frequently based\non industry trends. For example, the likelihood of discovery of a particular\nweakness may change - rising if detection techniques improve (or if there is a\nshift in focus because of increases in attacks), or falling if enough\ndevelopers have methods for avoiding the weakness, and/or if automatic\nprotection mechanisms reach wide-scale adoption.\n\nIn the future, default values for some factors might be directly obtained from\nCWE data. However, new CWE versions are released on a regular basis,\napproximately 4 or 5 times a year. If a CWE entry is modified in a way that\naffects CWSS-related factors, then the resulting CWSS score for a weakness\nmight differ depending on which version of CWE is used. Theoretically,\nhowever, this could be automatically detected by observing inconsistencies in\nthe weights used for \"Default\" values in CWSS vectors.\n\nBecause of these underlying changes, there is a significant risk that CWSS\nscores will not be comparable across organizations or assessments if they were\ncalculated using different versions of CWSS, vignettes, or CWE.\n\nIn anticipation of such changes, CWSS design should consider including CWE\nand/or CWSS version numbers in the CWSS vector representation (or associated\nmetadata).\n\nFinally, these changes should not occur too frequently, since each change\ncould cause CWSS scores to change unpredictably, causing confusion and\npossibly interfering with strategic efforts to fix weaknesses whose importance\nhas suddenly been reduced.* Although this may be inevitable for CWSS as a\nnatural result of growth, the community should attempt to prevent this from\nhappening where possible.\n\nWhile scores may change as CWSS and CWE evolve, there is not necessarily a\nrequirement for an organization to re-score whenever a new version is\nreleased, especially if the organization is using CWSS for internal purposes\nonly. The variability of scores is largely a problem for sharing scores\nbetween organizations. For example, a software developer may have its own\ninternally-defined vignettes and BVC, so the developer may not have a need (or\nwillingness) to share CWSS scores outside the organization.\n\n*CVSS encountered these problems when changing from version 1 to version 2, and there were significant labor costs to re-score vulnerabilities, which numbered in the tens of thousands. As a result of this, there has been significant reluctance by the CVSS SIG to make any substantive changes beyond version 2.\n\nBack to top\n\n(6) Future Activities\n\nAfter the release of CWSS 1.0, the schedule for future development is\nuncertain. However, future plans might include:\n\n  * Continue to obtain stakeholder validation or feedback for the existing factors, values, and weights. Now that CWSS is seeing some real-world use, these stakeholders will have valuable input to improve the metric.\n  * Continue to modify the scoring method and formula so that there is less bias towards low scores, and incomplete or non-applicable data does not adversely affect the scores.\n  * Refine and evaluate aggregated scoring techniques.\n  * Define a data exchange representation for CWSS scores and vectors, e.g. XML/XSD.\n  * The values for Authentication Strength (AS) might need to be defined more clearly. It might be reasonable to adopt approaches such as the four levels outlined in NIST Special Publication 800-63 (\"Electronic Authentication Guideline\") and OMB Memo 04-04. However, since the strength of an authentication mechanism may degrade over time due to advances in attack techniques or computing power, it might be useful to select values that are effectively \"future-proof.\" (On the other hand, this might make it difficult to compare CWSS scores if they were assigned at different times.)\n  * It is not clear whether the SANE model of privilege layers is sufficiently expressive or useful. Once CVSSv3 is released and independently validated, CVSS' model should be revisited.\n\nBack to top\n\n(7) Community Participation in CWSS\n\nCurrently, members of the software assurance community can participate in the\ndevelopment of CWSS in the following ways:\n\n  * Provide feedback on this document.\n  * Review the factors that are currently defined; suggest modifications to the current factors, and any additional factors that would be useful.\n  * Evaluate the scoring formula and the relative importance of factors within that formula.\n  * Define specific use cases for CWSS.\n\nAppendix A: CVSS Comparison\n\n### A.1 CVSS Overview\n\nThe Common Vulnerability Scoring System (CVSS) is commonly used when ranking\nvulnerabilities as they appear in deployed software. CVSS provides a common\nframework for consistently scoring vulnerabilities.\n\nConceptually, CVSS and CWSS are very similar. There are some important\nstrengths and limitations with CVSS, however.\n\nOne of CVSS' strengths lies in its simplicity. CVSS divides the overall score\ninto 14 separate characteristics within three metric groups: Base, Temporal,\nand Environmental. Each characteristic is decomposed into two or more distinct\nvalues. For example, the Access Vector reflects the location from which an\nattacker must exploit a vulnerability, with possible values of Local\n(authenticated to the local system), Remote (across the network), or Network\nAdjacent (on the same physical or logical network). Typically, in addition to\nthe CVSS score, a vector is provided that identifies the selected values for\neach characteristic.\n\nWith the associated documentation, CVSS scoring is fairly repeatable, i.e.,\ndifferent analysts will typically generate the same score for a vulnerability.\nHowever, different scores can be generated when information is incomplete, and\nsignificant variation is possible if an analyst does not closely follow\ndocumentation. While the simplified Confidentiality/Integrity/Availability\nmodel does not provide the depth and flexibility desired by some security\nexperts, CVSS does provide the consistency that is useful for non-expert\nsystem and network administrators for prioritizing vulnerabilities.\n\nCVSS has been widely adopted, especially the use of base scores from the Base\nmetric group. Some organizations use the Temporal and Environmental portions\nof CVSS, but this is relatively rare, so these metric groups may not have been\nsufficiently vetted in the real world.\n\n### A.2 Usage Scenarios for CWSS versus CVSS\n\nThe usage scenarios for CVSS differ from CWSS in the following ways:\n\n  * CVSS assumes that a vulnerability has already been discovered and verified; CWSS can be applied earlier in the process, before any vulnerabilities have been proven. CVSS is not scalable to the security assessment of a single software package. A detailed assessment, such as an automated code scan, may report thousands of weakness findings. Because of the high volume, these findings often need to be scored and prioritized before they can be more closely examined to determine if they lead to vulnerabilities. CWSS explicitly supports \"unknown\" values when there is incomplete information.\n  * CVSS scoring does not account for incomplete information, but CWSS scoring has built-in support for incomplete information. Within CWSS, scoring may be necessary before the weakness is even known to contribute to a vulnerability. For example, the scoring entity - whether a human or machine - might not know the expected operating environment or authentication requirements during initial CWSS scoring. In CVSS, incomplete information is sometimes a problem because many vulnerability reports do not contain all the relevant details needed for scoring. When information is missing, a conservative approach is to select values that will generate the largest CVSS score. This approach is used by the National Vulnerability Database and others, but it can artificially inflate scores. Conservative scoring is viable, as long as most vulnerability reports contain sufficient information. Within a weakness-scoring context, a high percentage of weakness findings will be missing a critical piece of information, since some detection techniques will not be able to reliably determine if a weakness can be exploited by an attacker without further analysis. As a result, CWSS provides a way to explicitly record when information is unavailable.\n  * CVSSv2 scoring has a large bias towards the impact on the physical system; CWSS has a small bias in favor of the application containing the weakness. In some contexts, users may strongly prefer to score issues based on their impact to business-critical data or functionality, which might have limited implications for the impact to the overall physical system. For example, the maximum possible score for CVSS is often 7.0 for Oracle products, since these products typically run with limited privileges. CVSSv3 will remove this bias toward the system, but its model still differs from the model that CWSS uses.\n\nEarly CWSS development sought to preserve the strengths of CVSS while also\nattempting to avoid some of the associated limitations.\n\n### A.3 Comparison of CVSSv2 Factors with CWSS Factors\n\nNote that in CVSS, the Access Complexity (AC) value combines multiple\ncharacteristics that are split into distinct factors within CWSS, such as\nRequired Privilege Level and Level of Interaction.\n\nCVSS| CWSS| Notes  \n---|---|---  \nConfidentiality Impact (C), Integrity Impact (I), Availability Impact (A),\nSecurity Requirements (CR, IR, AR), Collateral Damage Potential (CDP)|\nTechnical Impact| CWSS attempts to use a more fine-grained \"Technical Impact\"\nmodel than confidentiality, integrity, and availability. Business Value\nContext adjustments effectively encode the security requirements from the\nEnvironmental portion of CVSS. The CDP is indirectly covered within the BVC's\nlinkage between business concerns and technical impacts.  \nAccess Complexity (AC), Target Distribution (TD)| Deployment Scope| Deployment\nScope is indirectly covered by CVSS' Access Complexity, which combines\nmultiple distinct factors into a single item. It also has an indirect\nassociation with Target Distribution (TD).  \nAccess Vector (AV)| Access Vector| The values are similar, but CWSS\ndistinguishes between physical access and local (shell/account) access.  \nAccess Complexity (AC)| Required Privilege Level| Required Privilege Level is\nindirectly covered by CVSS' Access Complexity, which combines multiple\ndistinct factors into a single item.  \nN/A| Authentication Strength| This is not directly specified within CVSS, but\nscorers might consider the authentication strength when evaluating Access\nComplexity (AC).  \nAuthentication (Au)| Authentication Instances  \nN/A| Likelihood of Discovery| Within many CVSS use-cases, the vulnerability\nhas already been discovered and disclosed by another party when CVSS scoring\ntakes place. So there is no need to track the likelihood of discovery, as the\nlikelihood is (effectively) 1.0. However, within some CWSS use-cases, the\nissue is only known to the developer at the time of scoring, and the developer\nmay choose to increase the priority of issues that are most likely to be\ndiscovered.  \nN/A| Likelihood of Exploit| This is not covered in CVSS.  \nAccess Complexity (AC)| Interaction Requirements  \nAccess Complexity (AC), Remediation Level (RL)| Internal Control Effectiveness\n(IC)| The presence (or absence) of controls/mitigations may affect the CVSS\nAccess Complexity.  \nAccess Complexity (AC)| External Control Effectiveness (EC)| The presence (or\nabsence) of controls/mitigations may affect the CVSS Access Complexity.\nHowever, a single CVE vulnerability could have different CVSS scores based on\nvendor-specific configurations.  \nReport Confidence (RC)| Finding Confidence  \nN/A| Remediation Effort (RE)  \nExploitability (E)| N/A  \nTarget Distribution (TD)| N/A| There is no direct connection in CWSS 1.0 for\ntarget distribution; there is no consideration of how many installations may\nbe using the software.  \n  \n### A.4 Other Differences between CVSS and CWSS\n\nCWSS scores and CVSS scores are not necessarily comparable. Even if CWSS\nscores (with a maximum of 100) are \"normalized\" to a CVSS range by dividing by\n10 (which would produce CVSS-equivalent scores within the range of 0 to 10),\nthis does not mean that a CWSS score of 7 is equivalent to a CVSS 7. This\nmight be a desirable feature by some consumers, but since CWSS is often\nmeasuring completely different characteristics than CVSS does, scoring\nequivalence might not be feasible. In addition, in practice, CVSS scores do\nnot follow a regular distribution, generally with a skew towards high scores;\nit is possible that CWSS might have better distribution.\n\nSome organizations attempted to modify CVSS in order to address some of the\nunique requirements of software security analysis; these modifications are\nmentioned in Appendix B.\n\nThe metric groups in CWSS are different than those in CVSS, by design. Some\nreviewers of early CWSS versions suggested that CWSS adopt the same set of\nmetric groups that are used by CVSS - Base, Temporal, and Environmental.\nHowever, since CWSS scores can be calculated in early, low-information\nscenarios, many factors are \"temporal\" in nature, regardless of which group\nthey are in; also, these scores are likely to change as further analysis\nyields more information about the weakness. CWSS supports the use of values\nsuch as \"Unknown\" or \"Default\", which can be filled in at a later time.\n\nOne aspect of CVSS that is not explicitly modeled in CWSS is the notion of\n\"partial\" impacts. However, the acquired privileges, privilege layer,\ntechnical impact, and business impact are roughly equivalent, with more\nexpressive power.\n\nAppendix B: Other Scoring Methods\n\n### B.1 2008 CWSS Kickoff Meeting\n\nIn October 2008, a single-day kickoff meeting for CWSS was held. Several\nparticipants described their scoring approaches:\n\nVeracode reported an adaptation of CVSS to evaluate detected\nweaknesses/vulnerabilities. Each issue is given weights for Confidentiality,\nIntegrity, and Availability, based on its associated CWE entry. The weighting\nconsiders the average likely severity to occur. For example, a buffer overflow\ncould allow an attacker to cause a crash, but it is not always exploitable for\ncode execution.\n\nFor aggregated scores, Veracode has several \"VERAFIED Security Marks\" that are\nbased on a calculated Security Quality Score (SQS), which ranges from 0 to\n100. The \"VerAfied\" security mark is used to indicate software that Veracode\nhas assessed to be free of \"very high,\" \"high,\" or \"medium\" severity\nvulnerabilities, and free of automatically-detectable vulnerabilities from the\nCWE/SANS Top 25 or OWASP Top Ten. Two \"High Assurance\" variations of the mark\ninclude a manual assessment step that covers the remainder of the CWE/SANS Top\n25 or OWASP Top Ten that could not be identified by automatic detection.\n\nThe Veracode Rating System uses a three-letter rating system (with grades of\n\"A\", \"B\", \"C\", \"D\", and \"F\"). The first letter is used for the results from\nbinary analysis, the second for automated dynamic analysis, and the third for\nhuman testing.\n\nCigital described a feasibility study of CVSSv2 with respect to weaknesses.\nSome attributes such as \"Target Distribution\" did not fit well. Other\nattributes were extended to add more granularity. A polynomial scoring method\nwas recommended. It was also regarded as important to model the distinction\nbetween the likelihood and the impact.\n\nCenzic provided details of the Hailstorm Application Risk Metric (HARM). It is\na quantitative score that is utilized by black box analysis of web\napplications. The goal of the metric was to provide a scalable approach to\nfocus remediation efforts. The metric was split into 4 impact areas relevant\nto web application security: the browser, the session, the web application,\nand the server. The benefit to this approach was that it was easily\nconsumable.\n\nCERT/SEI presented its approach to scoring the C Secure Coding Rules. The\nFMECA metric, an ISO standard, was used. It characterizes items in terms of\nSeverity, Likelihood (of leading to a vulnerability), and Remediation Cost.\n\n### B.2 2010 SANS/CWE Top 25\n\nThe 2010 SANS/CWE Top 25 Most Dangerous Software Errors list attempted to\nperform quantitative prioritization of CWE entries using a combination of\nPrevalence and Importance, which became the basis of CWSS 0.1 later in the\nyear. A survey approach was taken in which respondents performed their own\nindividual evaluation of Prevalence and Importance for 41 candidate\nweaknesses, from which the final scores were determined. To reflect the\ndiverse opinions and use cases of the respondents for the general Top 25 list,\nthe Importance factor was used instead of Impact. In an attempt to force\nconsensus, respondents were restricted to 4 selections of the highest value\nfor Importance (\"Critical\") and Prevalence (\"Widespread\"), although this\nforced choice was not popular; it will probably be abandoned in future\nversions of the Top 25. Many respondents used high-level rollup data, or a\nrough consensus of opinion with the organization, sometimes covering multiple\nteams or functions. Very few respondents had real-world data at the low level\nof granularity used by the Top 25 (typically the \"Base\" level of abstraction\nfor CWE). An evaluation by PlexLogic later found that the two variables were\nnot entirely independent. This discovery makes some sense, because the\nvulnerability research community tends to focus on vulnerabilities/weaknesses\nwith the highest impact. When reliable attack techniques are devised for a\nparticular weakness/vulnerability, it becomes easier for more researchers to\nfind them, which can lead to widespread exploitation. Consequently, this\nraises the relative Importance of a weakness.\n\nThe 2010 Top 25 was structured in a way to support multiple points of view\nthat could reflect different prioritizations of the weaknesses. The creation\nof separate focus profiles stemmed from some critiques of the original 2009\nTop 25, in which a generalized Top 25 list would not necessarily be useful to\nall audiences, and that a customized prioritization would be ideal. Eight\nfocus profiles were provided with the 2010 Top 25. For example, the\nEducational Emphasis focus profile evaluated weaknesses that are regarded as\nimportant from an educational perspective within a school or university\ncontext. It emphasized the CWE entries that graduating students should know,\nincluding weaknesses that were historically important or increased the breadth\nof coverage. A separate focus profile ranked weaknesses based solely on their\nevaluated Importance, which would be useful to software customers who want the\nmost serious issues removed, without consideration for how frequently they\noccur or how resource-intensive it is to fix. These ranking-oriented focus\nprofiles made the Top 25 more useful to certain audiences, and their\nconstruction and management have served as a useful predecessor to CWSS and\nvignettes.\n\nWhile the 2009 Top 25 did not rank items, several factors were presented that\nwere thought to be relevant to an audience: attack frequency, impact or\nconsequences, prevalence, and ease of detection. Other considerations included\nremediation cost, amount of public knowledge, and the likelihood that the\nweakness discovery would increase in the future.\n\n### B.3 2010 OWASP Top Ten\n\nIn contrast to previous versions, the 2010 OWASP Top Ten shifted focus from\nweaknesses/vulnerabilities to risks, which typically caused each OWASP Top Ten\nentry to cover multiple related weakness types that posed the same risk.\nFactors for prioritization included Ease of Exploit, Prevalence,\nDetectability, and Technical Impact. Input from contributors was solicited to\ndetermine the values for these factors, but the final decision for each factor\nwas made by the Top Ten editorial staff based on trend information from\nseveral real-world data sources. A metric was developed that used these\nfactors to prioritize the final Top Ten list.\n\n### B.4 Other Models\n\nMicrosoft's STRIDE model characterizes issues in terms of Spoofing Identity,\nTampering with Data, Repudiation, Information Disclosure, Denial of Service,\nand Elevation of Privilege. The DREAD scheme evaluates issues based on Damage\nPotential, Reproducibility of the issue, Exploitability, Affected Users, and\nDiscoverability. Many of these attributes have equivalent factors in CWSS.\n\nAppendix C: Design Considerations\n\nFor CWSS to be most effective to its stakeholders, several aspects of the\nproblem area were considered when designing the framework and metrics.\n\n  * Flexibility: CWSS should be automatable and flexible wherever possible, but support human input as well.\n  * Selected Automation: It is assumed that portions of CWSS scores can be automatically generated. For example, some factors may be dependent on the type of weakness being scored; potentially, the resulting subscores could be derived from CWE data. As another example, a web script might only be accessible by an administrator, so all weaknesses may be interpreted in light of this required privilege.\n  * Scalability: Some usage scenarios may require the scoring of thousands of weaknesses, such as defect reports from an automated code scanning tool. When such a high volume is encountered, there are too many issues to analyze manually. As a result, automated scoring must be supported.\n  * Stakeholder Analysis: The potential CWSS stakeholders, their needs, and associated use cases should be analyzed to understand their individual requirements. This might require support for multiple scoring techniques or methods.\n  * Usability vs. Completeness: Associated metrics must balance usability with completeness, i.e., they cannot be too complex.\n  * Prioritization Flexibility: Environmental conditions and business/mission priorities should impact how scores are generated and interpreted.\n\nAppendix D: Generalized Scoring Approaches\n\nWhile CWSS 1.0 is focused on targeted scoring, it could be further adapted for\nscoring weaknesses in a general fashion, e.g. to develop a relative\nprioritization of issues such as buffer overflows, XSS, and SQL injection,\nindependent of any specific software package.\n\nA generalized scoring approach could account for:\n\n  * Prevalence: how often does this appear at least once within a software package?\n  * Frequency: in a software package in which this weakness occurs, how often does it occur? (perhaps summarized as \"diffusion\")\n  * Likelihood of Discovery\n  * Likelihood of Exploit\n  * Technical Impact\n\nIn the earlier CWSS 0.1, the formula was:\n\n> Prevalence x Importance\n\nThis formula was a characterization of the metric used for the 2010 CWE/SANS\nTop 25. Importance was derived from the vignette-specific subscores for\nTechnical Impacts of the CWE entry. Prevalence could be obtained from general\ninformation (derived from CWE content, or from other sources), with the\npossibility of vignette-specific specifications of prevalence. For example,\nXSS or SQL injection might occur more frequently in a web-based retail context\nthan in embedded software.\n\n### D.1 Prevalence Assessment\n\nIn the earlier CWSS version 0.1, prevalence scores for the 2010 Top 25 were\nobtained by re-using raw voting data from the 2010 Top 25 participants. The\noriginal 1-4 scale (with discrete values) was extended to use values between 1\nand 10. When using real-world prevalence data, this artificial normalization\nmight not be necessary.\n\nThe following table summarizes the prevalence scores for some of the Top 25\nentries. Notice the high prevalence value for XSS; this reflects the fact that\nnearly all of the voting members scored XSS as \"Widespread.\" Complete details\nare available on a separate page.\n\nTop 25 Rank| CWE| Name| Prevalence (1-10)  \n---|---|---|---  \n[1]| CWE-79| XSS| 9.46  \n[2]| CWE-89| SQL Injection| 7.43  \n[3]| CWE-120| Classic Buffer Overflow| 6.04  \n[4]| CWE-352| Cross-site Request Forgery| 7.75  \n[16]| CWE-209| Information Exposure Through an Error Message| 7.11  \n  \nAppendix E: Aggregated Scoring Methods: Measuring Weakness Surface\n\nFor years, software consumers have wanted clear guidance on how secure a\nsoftware package is, but the only available methods have been proprietary,\ncrude, or indirect, such as:\n\n  * Crude methods, such as counting the number and/or severity of publicly reported vulnerabilities\n  * Proprietary methods developed by consultants or tool developers\n  * Indirect methods, such as the attack surface metric, which likely has a strong association with overall software security, although this has not necessarily been empirically proven.\n\nA software package could be evaluated in light of the number and importance of\nweaknesses that have been detected, whether from automated or manual\ntechniques. The results from these individual weaknesses could be aggregated\nto develop a single score, tentatively called the \"Weakness Surface.\" This\ncould move the software assurance community one step closer to consumer-\nfriendly software security indicators such as the Software Facts Label\nconcept, as originally proposed by Jeff Williams (Aspect Security) and Paul\nBlack (NIST).\n\nWhen there is a set of targeted weaknesses for a single software package,\nthere are several possible aggregated scoring methods, including but not\nnecessarily limited to:\n\n  * Compute the sum of all individual weakness scores\n  * Choose the highest of all individual weakness scores\n  * Select a subset of individual weakness scores exceeding a stated minimum, and add the scores together\n  * Compute the sum of all individual weakness scores, then normalize these scores according to KLOC or other metrics that reflect code size, i.e., \"defect density.\"\n  * Normalize the results on a per-executable basis.\n  * Normalize the results to a point scale between 0 (no assurance) and 100 (high assurance).\n\nEarly CWSS implementations have typically aggregated based on either the sum\nof all scores, or by choosing the highest of all scores.\n\nSome methods from the 2008 CWSS kickoff workshop may be adaptable or\napplicable; see Appendix B. In addition, some SCAP users have begun creating\naggregated metrics for a host system by aggregating CVSS scores for the\nindividual CVE vulnerabilities that are detected on the host. These users may\nhave some useful guidance for the CWSS approach to aggregate scoring.\n\nChange Log\n\nDate| Document Version| Notes  \n---|---|---  \nSeptember 5, 2014| 1.0.1| Changed 4.2 example (\"Wiki with limited business\ncriticality\") vector weights to use 1.0 instead of 1, for consistenc with the\n4.1 example. Credit: an external contributor.Changed 4.2 example (\"Wiki with\nlimited business criticality\") vector values \"AS:L\" to \"AS:W\" and \"AV:N\" to\n\"AV:I\". The example was using values that were valid for 0.8 but changed\n(i.e., made obsolete) in 1.0. Credit: an external contributor.Changed 4.2\nexample (\"Wiki with limited business criticality\") to include table listing\nvector values.Added description for Default value in Prevalence (P) table.  \nJuly 17, 2014| 1.0| Removed Authentication Instances (AI). The CVSSv3 team\nfound that in this factor, the Multiple value was \"rarely, if ever, used\"\nwithin several real-world implementations of CVSSv2, and the CVSS SIG will\nremove this factor in CVSSv3. In addition, within CVSSv2, there was sometimes\nuser confusion between this factor and the Access Vector's \"Local\" value that\ncould cause inconsistent values to be chosen.Removed Remediation Effort (RE).\nThis factor does not directly contribute to the inherent risk or severity of a\nweakness, so it is outside the scope of CWSS scoring. Remediation effort could\nbe better handled using separate processes within the context of a remediation\nplan. For example, bug databases and external maintenance/release processes\nmight be more appropriate for recording this information.Changed scoring\nformula to handle removal of AuthenticationInstances and RemediationEffort.\nWithin the Attack Surface Subscore, the multiplier for LevelOfInteraction was\nadjusted from 10 to 15. Within the Environmental Subscore, the multiplier for\nLikelihoodOfExploit was increased from 3 to 4, which reflects current trends\nin vulnerability management that place greater emphasis on likelihood of\nexploit than raw CVSS scores.Modified introduction.Clarified distinction\nbetween CWSS and CVSS.Created new images for metric groups and scoring,\nomitting factors that were removed for 1.0.Improve display of tables,\nformulas, etc.Give numbers to sections.Acquired Privilege (AP) - Default\nweight changed from 1.0 to 0.5 (typo).Changed \"Unk\" code to \"UK\" everywhere.\nListed \"Unknown\" description in each factor.Clarified and fixed values /\ndefinitions for AP, AL, RP, and RL.Level of Interaction (IN) - changed codes\nto use only 1 to 2 uppercase letters.Deployment Scope (SC) - changed codes to\nuse only 1 to 2 uppercase letters.  \nJune 27, 2011| 0.8| Bumped up version number to synchronize with CWRAF.  \nJune 23, 2011| 0.6| Major changes to the formula to better reflect relative\npriorities of the factors.Modified Access Vector (AV) to include Internet,\nIntranet, and Private Network values.Renamed Remediation Cost (RC) to\nRemediation Effort (RE), and changed the available values.Changed External\nControl Effectiveness (EC) weight for \"Complete\" to 0.1, to reflect the\npossibility of accidental removal of the control if the environment\nchanges.Modified values for Authentication Strength (AS) and added notes for\npotential enhancements.Modified weights for Prevalence (P) so the range of\nvariation is more narrow.All weights for Unknown values were changed to 0.5 so\nthat lack of information does not over-emphasize scores; additional\ninformation can move scores up or down, accordingly.Changed Defense-in-\nDepth/\"D\" value to Indirect/\"I\" for internal and external control\neffectiveness to avoid conflict with the Default/\"D\" value.Removed most\nreferences to vignettes, technical impact scorecards, business value context,\netc. - now covered in CWRAF.Skipped version 0.5 to reflect maturity and for\nalignment with other efforts.  \nApril 26, 2011| 0.4| Removed content that became part of Common Weakness Risk\nAnalysis Framework (CWRAF).Reorganized metric groups.Defined new factors -\nBusiness Impact, Acquired Privilege, Acquired Privilege Layer.Defined a new\nformula.Added \"Default\" values to each factor.  \nMarch 7, 2011| 0.3| Created overview images and shortened the introduction.\nDefined technology groups, added more business domains, added more vignettes.\nAnnotated each factor that could be quantified. Updated stakeholders section.\nIntegrated CVSS descriptions into a single section. Added more details on the\nscoring method, including CWSS vectors.  \nFebruary 11, 2011| 0.2| Added business domains, archetypes, and Business Value\nContext; identified detailed factors; emphasized use of CWSS for targeted\nscoring; reorganized sections; made other modifications based on community\nfeedback.  \nDecember 2, 2010| 0.1| Initial version for review by limited audience  \n  \nBack to top\n\nMore information is available \u2014 Please edit the custom filter or select a\ndifferent filter.  \n  \nPage Last Updated: June 06, 2023\n\nSite Map | Terms of Use | Manage Cookies | Cookie Notice | Privacy Policy | Contact Us |Use of the Common Weakness Enumeration (CWETM) and the associated references from this website are subject to the Terms of Use. CWE is sponsored by the U.S. Department of Homeland Security (DHS) Cybersecurity and Infrastructure Security Agency (CISA) and managed by the Homeland Security Systems Engineering and Development Institute (HSSEDI) which is operated by The MITRE Corporation (MITRE). Copyright \u00a9 2006\u20132024, The MITRE Corporation. CWE, CWSS, CWRAF, and the CWE logo are trademarks of The MITRE Corporation.  \n---\n\n", "frontpage": false}
