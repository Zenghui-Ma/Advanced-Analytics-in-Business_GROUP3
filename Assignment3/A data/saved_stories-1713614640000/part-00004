{"aid": "40094729", "title": "Israel uses facial recognition systems in Gaza and beyond", "url": "https://www.theguardian.com/technology/2024/apr/19/idf-facial-recognition-surveillance-palestinians", "domain": "theguardian.com", "votes": 5, "user": "racional", "posted_at": "2024-04-20 04:08:36", "comments": 0, "source_title": "How Israel uses facial recognition systems in Gaza and beyond", "source_text": "How Israel uses facial recognition systems in Gaza and beyond | Facial recognition | The Guardian\n\nSkip to main contentSkip to navigation\n\nSkip to navigation\n\nPrint subscriptions\n\nSign in\n\nSearch jobs\n\nSearch\n\n  * Europe edition\n\n  * UK edition\n\n  * US edition\n\n  * Australia edition\n\n  * International edition\n\nThe Guardian - Back to homeThe Guardian\n\n  * World\n  * UK\n  * Climate crisis\n  * Ukraine\n  * Environment\n  * Science\n  * Global development\n  * Football\n  * Tech\n  * Business\n  * Obituaries\n\nPalestinians search for usable items among the rubble in Deir al-Balah, Gaza,\non Thursday. Photograph: Anadolu/Getty Images\n\nPalestinians search for usable items among the rubble in Deir al-Balah, Gaza,\non Thursday. Photograph: Anadolu/Getty Images\n\nFacial recognition\n\nInterview\n\n# How Israel uses facial recognition systems in Gaza and beyond\n\nNick Robins-Early\n\nAmnesty International researcher Matt Mahmoudi discusses the IDF\u2019s use of the\ntechonology as a tool of mass surveillance\n\nFri 19 Apr 2024 19.16 CEST\n\nShare\n\nGovernments around the world have increasingly turned to facial recognition\nsystems in recent years to target suspected criminals and crack down on\ndissent. The recent boom in artificial intelligence has accelerated the\ntechnology\u2019s capabilities and proliferation, much to the concern of human\nrights groups and privacy advocates who see it as a tool with immense\npotential for harm.\n\nFew countries have experimented with the technology as extensively as Israel,\nwhich the New York Times recently reported has developed new facial\nrecognition systems and expanded its surveillance of Palestinians since the\nstart of the Gaza war. Israeli authorities deploy the system at checkpoints in\nGaza, scanning the faces of Palestinians passing through and detaining anyone\nwith suspected ties to Hamas. The technology has also falsely tagged civilians\nas militants, one Israeli officer told the Times. The country\u2019s use of facial\nrecognition is one of the new ways that artificial intelligence is being\ndeployed in conflict, with rights groups warning this marks an escalation in\nIsrael\u2019s already pervasive targeting of Palestinians via technology.\n\n\u2018The machine did it coldly\u2019: Israel used AI to identify 37,000 Hamas targets\n\nRead more\n\nIn an Amnesty International report on Israel\u2019s use of facial recognition last\nyear, the rights group documented security forces\u2019 extensive gathering of\nPalestinian biometric data without their consent. Israeli authorities have\nused facial recognition to build a huge database of Palestinians that is then\nused to restrict freedom of movement and carry out mass surveillance,\naccording to the report.\n\nThe Israeli ministry of defense did not return a request for comment on the\nfindings of Amnesty\u2019s report or the New York Times article on its facial\nrecognition programs.\n\nThe Guardian spoke with Matt Mahmoudi, an adviser on AI and human rights at\nAmnesty and lead researcher on the report, about how Israel deploys facial\nrecognition systems and how their use has expanded during the war in Gaza.\n\n## One thing that stands out from your report is that there\u2019s not just one\nsystem of facial recognition but several different apps and tools. What are\nthe ways that Israeli authorities collect facial data?\n\nThere\u2019s a slew of facial recognition tools that the state of Israel has\nexperimented with in the occupied Palestinian territories for the better part\nof the last decade. We\u2019re looking at tools by the names of Red Wolf, Blue Wolf\nand Wolf Pack. These are systems that have been tested in the West Bank, and\nin particular in Hebron. All of these systems are effectively facial\nrecognition tools.\n\nIn Hebron, for a long time, there was a reliance on something known as the\nWolf Pack system \u2013 a database of information pertaining to just Palestinians.\nThey would effectively hold a detained person in front of the CCTV camera, and\nthen the operations room would pull information from the Wolf Pack system.\nThat\u2019s an old system that requires this linkup between the operations room and\nthe soldier on the ground, and it\u2019s since been upgraded.\n\nOne of the first upgrades that we saw was the Blue Wolf system, which was\nfirst reported on by Elizabeth Dwoskin in the Washington Post back in 2021.\nThat system is effectively an attempt at collecting as many faces of\nPalestinians as possible, in a way that\u2019s akin to a game. The idea is that the\nsystem would eventually learn the faces of Palestinians, and soldiers would\nonly have to whip out the Blue Wolf app, hold it in front of someone\u2019s face,\nand it would pull all the information that existed on them.\n\n## You mentioned that there\u2019s a gamification of that system. How do incentives\nwork for soldiers to collect as much biometric data as possible?\n\nThere\u2019s a leaderboard on the Blue Wolf app, which effectively tracks the\nmilitary units that are using the tool and capturing the faces of\nPalestinians. It gives you a weekly score based on the most amount of pictures\ntaken. Military units that captured the most faces of Palestinians on a weekly\nbasis would be provided rewards such as paid time away.\n\nSo you\u2019re constantly put into the terrain of no longer treating Palestinians\nas individual human beings with human dignity. You\u2019re operating by a gamified\nlogic, in which you will do everything in your power to map as many\nPalestinian faces as possible.\n\n## And you said there are other systems as well?\n\nThe latest we\u2019ve seen happen in Hebron has been the additional introduction of\nthe Red Wolf system, which is deployed at checkpoints and interfaces with the\nother systems. The way that it works is that individuals passing through\ncheckpoints are held within the turnstile, cameras scan their faces and a\npicture is put up on the screen. The soldier operating it will be given a\nlight indicator \u2013 green, yellow, red. If it\u2019s red, the Palestinian individual\nis not able to cross.\n\nThe system is based only on images of Palestinians, and I can\u2019t stress enough\nthat these checkpoints are intended for Palestinian residents only. That they\nhave to use these checkpoints in the first place in order to be able to access\nvery basic rights, and are now subject to these arbitrary restrictions by way\nof an algorithm, is deeply problematic.\n\nWhat\u2019s been particularly chilling about the system has been hearing the\nstories about individuals who haven\u2019t been able to even come back into their\nown communities as a result of not being recognized by the algorithm. Also\nhearing soldiers speaking about the fact that now they were doubting whether\nthey should let a person that they know very well pass through a checkpoint,\nbecause the computer was telling them not to. They were finding that\nincreasingly they had a tendency of thinking of Palestinians as numbers that\nhad either green, yellow or red lights associated with them on a computer\nscreen.\n\n## These facial recognition systems operate in a very opaque way and it\u2019s\noften hard to know why they are making certain judgments. I was wondering how\nthat affects the way Israeli authorities who are using them make decisions.\n\nAll the research that we have on human-computer interaction to date suggests\nthat people are more likely to defer agency to an algorithmic indicator in\nespecially pressing circumstances. What we\u2019ve seen in testimonies that we\nreviewed has been that soldiers time and time again defer to the system rather\nthan to their own judgment. That simply comes down to a fear of being wrong\nabout particular individuals. What if their judgment is not correct,\nirrespective of the fact that they might know the person in front of them?\nWhat if the computer knows more than they do?\n\nEven if you know that these systems are incredibly inaccurate, the fact that\nyour livelihood might depend on following a strict algorithmic prescription\nmeans that you\u2019re more likely to follow it. It has tremendously problematic\noutcomes and means that there is a void in terms of accountability.\n\n## What has the Israeli government or military said publicly about the use of\nthese technologies?\n\nThe only public acknowledgment that the Israeli inistry of defense has made \u2013\nas far as the Red Wolf, Blue Wolf and Wolf Pack systems \u2013 is simply saying\nthat they of course have to take measures to guarantee security. They say some\nof these measures include innovative tech solutions, but they\u2019re not at\nliberty to discuss the particularities. That\u2019s kind of the messaging that\ncomes again and again whenever they\u2019re hit with a report that specifically\ntakes to task their systems for human rights violations.\n\nWhat\u2019s also interesting is the way in which tech companies, together with\nvarious parts of the ministry of defense, have boasted about their\ntechnological prowess when it comes to AI systems. I don\u2019t think it\u2019s a secret\nthat Israeli authorities rely on a heavy dose of PR that touts their military\ncapabilities in AI as being quite sophisticated, while also not being super\ndetailed about exactly how these systems function.\n\n## There\u2019s been past statements from the Israeli government, I\u2019m thinking\nspecifically of a statement on the use of autonomous weapons, that argue these\nare sophisticated tools that could actually be good for human rights. That\nthey could remove the potential for harm or accidents. What is your take on\nthat argument?\n\nIf anything we have seen time and time again how semi-autonomous weapons\nsystems end up dehumanizing people and leading to deaths that are later on\ndescribed as \u201cregrettable\u201d. They don\u2019t have meaningful control and they\neffectively turn people into numbers crunched by an algorithm, as opposed to a\nmoral, ethical and legal judgment that has been made by an individual. It has\nthis sort of consequence of saying, \u201cWell look, identifying what counts as a\nmilitary target is not up to us. It\u2019s up to the algorithm.\u201d\n\n## Since your report came out, the New York Times reported that similar facial\nrecognition tech has been developed and deployed to surveil Palestinians in\nGaza. What have you been seeing in terms of the expansion of this technology?\n\nWe know that facial recognition systems were being used for visitors coming in\nand out of Gaza, but following the New York Times reporting it\u2019s the first\ntime that we\u2019ve heard it being used in the way that it has been \u2013 particularly\nagainst Palestinians in Gaza who are fleeing from the north to the south. What\nI\u2019ve been able to observe is largely based on open-source intelligence, but\nwhat we see is footage that shows what look like cameras mounted on tripods\nthat are situated outside of makeshift checkpoints. People move slowly from\nnorth to south through these checkpoints, and then people are pulled out of\nthe crowd and detained on the basis of what we suspect is a facial recognition\ntool.\n\n> We have seen time and time again how semi-autonomous weapons systems end up\n> dehumanizing people\n\nThe issue there is that people are already being moved around chaotically from\nA to B under the auspices of being brought to further safety, only to be\nslowed down and asked to effectively be biometrically checked before they\u2019re\nallowed to do so. We also hear about individuals that have been detained and\nbeaten and questioned after having been biometrically identified, but later\nhave it been established that was a mistake.\n\n## The Israeli government\u2019s justifications for its use of checkpoints and\ndetentions is that it\u2019s based on national security fears. When it comes to\nfacial recognition, how does that argument hold up from a human rights\nperspective?\n\nYour rights aren\u2019t less viable or active just because there\u2019s a national\nsecurity threat. There is, however, a three-part test that you deploy when it\ncomes to figuring out in what particularly unique kinds of circumstances a\nstate would violate certain rights in order to uphold others. That three-part\ntest tries to assess the necessity, proportionality and legitimacy of a\nparticular intervention.\n\nAt Amnesty, under international human rights law, we don\u2019t believe that there\nis necessity, proportionality or legitimacy. This facial recognition\ntechnology isn\u2019t compatible with the right to privacy, the right to non-\ndiscrimination, the right to peaceful assembly or freedom of movement. All\nthese rights are severely compromised under a system that is, by design,\neffectively a system of mass surveillance and therefore unlawful.\n\n## You\u2019ve talked about the lack of accountability and accuracy with these\ntools. If the Israeli government knows these are not accurate, then why\ncontinue to use them?\n\nI think AI-washing is a significant part of how governments posture that\nthey\u2019re doing something about a problem that they want to seem like they\u2019re\nbeing proactive on. It\u2019s been clear since the 2010s that governments around\nthe globe have relied on tech solutions. Now, since the explosion of\ngenerative AI in particular, there\u2019s this idea that AI is going to solve some\nof the most complex social, economic and political issues.\n\nI think all it does is absolve states of the responsibilities that they have\nto their citizens \u2013 the obligations that they have under international law to\nuphold the rights of those whom they subject to their power by basically\nsaying \u201cthe system will take care of it\u201d or \u201cthe system was at fault\u201d. It\ncreates these neat grounds for states to be able to seem like they\u2019re doing\nsomething, without being held to account on whatever they\u2019re actually doing.\nThere\u2019s a technical system that is mediating both accountability and\nresponsibility.\n\nThis interview has been edited and condensed for clarity\n\nExplore more on these topics\n\n  * Facial recognition\n  * Israel\n  * Gaza\n  * Palestinian territories\n  * interviews\n\nShare\n\nReuse this content\n\n## Most viewed\n\n## Most viewed\n\n  * World\n  * UK\n  * Climate crisis\n  * Ukraine\n  * Environment\n  * Science\n  * Global development\n  * Football\n  * Tech\n  * Business\n  * Obituaries\n\n  * News\n  * Opinion\n  * Sport\n  * Culture\n  * Lifestyle\n\nOriginal reporting and incisive analysis, direct from the Guardian every\nmorning\n\nSign up for our email\n\n  * Help\n  * Complaints & corrections\n  * SecureDrop\n  * Work for us\n  * Privacy policy\n  * Cookie policy\n  * Terms & conditions\n  * Contact us\n\n  * All topics\n  * All writers\n  * Digital newspaper archive\n  * Facebook\n  * YouTube\n  * Instagram\n  * LinkedIn\n  * Twitter\n  * Newsletters\n\n  * Advertise with us\n  * Search UK jobs\n\nBack to top\n\n\u00a9 2024 Guardian News & Media Limited or its affiliated companies. All rights\nreserved. (dcr)\n\n", "frontpage": true}
